import json
import nltk
import spacy
import re
import torch
from collections import defaultdict
from sentence_transformers import SentenceTransformer, util
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import warnings
warnings.filterwarnings('ignore')
import requests
import fitz
from io import BytesIO
import nest_asyncio
import asyncio
import aiohttp
import pickle
import os
import hashlib 

def compute_md5(text):
    import hashlib
    return hashlib.md5(text.encode("utf-8")).hexdigest()

async def initialize_chatbot(json_file_path):
    """Initialize chatbot with cached embeddings and merged PDFs."""
    chatbot = UniversityChatbot(json_file_path)

    # --- JSON embeddings caching ---
    json_cache_file = "json_embeddings.pkl"
    json_hash_file = "json_hash.pkl"

    with open(json_file_path, "r", encoding="utf-8") as f:
        json_text = f.read()
    json_hash = compute_md5(json_text)

    json_cache_valid = os.path.exists(json_cache_file) and os.path.exists(json_hash_file)
    if json_cache_valid:
        with open(json_hash_file, "rb") as f:
            cached_hash = pickle.load(f)
        json_cache_valid = (cached_hash == json_hash)

    if json_cache_valid:
        with open(json_cache_file, "rb") as f:
            chatbot.corpus_embeddings = pickle.load(f)
        with open(json_hash_file, "rb") as f:
            pickle.load(f)
        print("Loaded JSON embeddings from cache")
    else:
        chatbot.prepare_semantic_corpus()
        with open(json_cache_file, "wb") as f:
            pickle.dump(chatbot.corpus_embeddings, f)
        with open(json_hash_file, "wb") as f:
            pickle.dump(json_hash, f)
        print("Created and cached JSON embeddings")

    # --- PDF embeddings caching ---
    pdf_cache_file = "pdf_embeddings.pkl"
    if os.path.exists(pdf_cache_file):
        with open(pdf_cache_file, "rb") as f:
            chatbot.pdf_docs = pickle.load(f)
        print("Loaded PDF embeddings from cache")
    else:
        # If async is desired, wrap this in asyncio.run(chatbot.load_all_pdfs()) outside
        await chatbot.load_all_pdfs()
        with open(pdf_cache_file, "wb") as f:
            pickle.dump(chatbot.pdf_docs, f)
        print("Loaded and cached PDFs")

    # --- Merge PDFs into main corpus ---
    for doc_name, doc_info in chatbot.pdf_docs.items():
        if doc_info.get("text") and (doc_info.get("chunks") is None or doc_info.get("embeddings") is None):
            chunks = [doc_info["text"][i:i+1000] for i in range(0, len(doc_info["text"]), 1000)]
            doc_info["chunks"] = chunks
            doc_info["embeddings"] = chatbot.sentence_model.encode(chunks, convert_to_tensor=True)

        for i, chunk in enumerate(doc_info.get("chunks", [])):
            chatbot.corpus.append(chunk)
            chatbot.corpus_metadata.append({
                "path": f"PDF:{doc_name}_chunk_{i}",
                "parent_key": doc_name,
                "content": chunk,
                "source": doc_info.get("url")
            })
        if doc_info.get("embeddings") is not None:
            if chatbot.corpus_embeddings is not None:
                chatbot.corpus_embeddings = torch.cat([chatbot.corpus_embeddings, doc_info["embeddings"]])
            else:
                chatbot.corpus_embeddings = doc_info["embeddings"]

    print(f"Total corpus segments: {len(chatbot.corpus)}")
    pdf_count = sum(1 for m in chatbot.corpus_metadata if "PDF:" in m["path"])
    print(f"Number of PDF segments in corpus: {pdf_count}")
    print("Chatbot initialized with JSON + PDF embeddings")

    return chatbot

class UniversityChatbot():

    # Initializing the university chatbot with the data and intent
    def __init__(self, json_file_path):
        """Initialize the chatbot with university data"""
        
        # Download required NLTK data
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt')
        
        try:
            nltk.data.find('corpora/stopwords')
        except LookupError:
            nltk.download('stopwords')
        
        # Load spaCy model
        try:
            self.nlp = spacy.load("xx_ent_wiki_sm")
        except OSError:
            print("âš ï¸  spaCy Multilingual model not found. Install with: python -m spacy download xx_ent_wiki_sm")
            self.nlp = None
        
        # Initialize Sentence Transformer for semantic similarity
        try:
            self.sentence_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        except Exception as e:
            print(f"âš ï¸ Error loading sentence transformer: {e}")
            self.sentence_model = None

        # Initialize local LLM for Arabic response generation
        try:
            self.generator = pipeline(
                "text-generation", 
                model="aubmindlab/aragpt2-base", 
                tokenizer="aubmindlab/aragpt2-base",
                max_length=100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=50256
            )
        except Exception as e:
            print(f"âš ï¸ Could not load Arabic language model: {e}")
            self.generator = None
        
        # Load university data
        self.json_file_path = json_file_path
        self.data = self.load_data(json_file_path)
        
        
        # Enhanced intent patterns with semantic variations
        self.intent_patterns = {
            'Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ ÙÙŠ Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„Ùƒ Ø³Ø¹ÙˆØ¯': [
                'Ø´Ø±ÙˆØ· Ø§Ù„Ù‚Ø¨ÙˆÙ„', 'ÙƒÙŠÙÙŠØ© Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…', 'Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…', 'Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„',
                'Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù‚Ø¨ÙˆÙ„', 'Ø´Ø±ÙˆØ· Ø§Ù„Ø§Ù„ØªØ­Ø§Ù‚', 'Ø§Ù„Ø£Ù‡Ù„ÙŠØ© Ù„Ù„ØªÙ‚Ø¯ÙŠÙ…',
                'Ù…Ø§ Ø§Ù„Ø°ÙŠ Ø£Ø­ØªØ§Ø¬Ù‡ Ù„Ù„ØªÙ‚Ø¯ÙŠÙ…', 'Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…', 'ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ù„ØªØ­Ø§Ù‚ Ø¨Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©'
            ],
            'Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ': [
                'Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ', 'Ø¬Ø¯ÙˆÙ„ Ø§Ù„ÙØµÙ„ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ', 'ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ', 'ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„ÙØµÙ„',
                'Ù…ØªÙ‰ ÙŠØ¨Ø¯Ø£ Ø§Ù„ÙØµÙ„ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ', 'Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø§Ù…ØªØ­Ø§Ù†Ø§Øª', 'ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„ØªØ³Ø¬ÙŠÙ„',
                'Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ', 'Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù„ÙØµÙ„ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ', 'Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø±Ø±Ø§Øª', 'Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø­ØµØµ',
                'Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„Ù…Ù‡Ù…Ø©', 'Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©', 'Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¹Ø·Ù„'
            ],
            'Ø§Ù„ÙƒÙ„ÙŠØ§Øª': [
                'Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ©', 'Ø§Ù„ØªØ®ØµØµØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©', 'Ø§Ù„Ù…Ù‚Ø±Ø±Ø§Øª Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ©', 'Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©',
                'Ù…Ø§ Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø¯Ø±Ø§Ø³ØªÙ‡', 'Ø§Ù„ÙƒÙ„ÙŠØ§Øª ÙˆØ§Ù„ØªØ®ØµØµØ§Øª', 'Ø§Ù„Ø®Ø·Ø© Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©',
                'Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø¨ÙƒØ§Ù„ÙˆØ±ÙŠÙˆØ³', 'Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ù…Ø§Ø¬Ø³ØªÙŠØ±', 'Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø§Ù‡', 'Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠØ©'
            ],
            'Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³': [
                'Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³', 'Ø£Ø¹Ø¶Ø§Ø¡ Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³', 'Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø³Ø§ØªØ°Ø©', 'Ø§Ù„ÙƒØ§Ø¯Ø± Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ',
                'Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§ØªØ°Ø©', 'Ø¬Ù‡Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³ÙŠØ©', 'Ø·Ø§Ù‚Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ³'
            ],
            'contact_info': [
                'Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„', 'Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù‡ÙˆØ§ØªÙ', 'Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ', 'Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…ÙƒØ§ØªØ¨',
                'ÙƒÙŠÙÙŠØ© Ø§Ù„ØªÙˆØ§ØµÙ„', 'ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø§ØªØµØ§Ù„', 'Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ù…', 'Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØµØ§Ù„'
            ],
            'Ø§Ø³ÙƒØ§Ù† Ø¬Ø§Ù…Ø¹Ø©': [
                'Ø§Ù„Ø³ÙƒÙ† Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠ', 'Ù…Ø³Ø§ÙƒÙ† Ø§Ù„Ø·Ù„Ø§Ø¨', 'Ø§Ù„Ø¥Ù‚Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©', 'Ø§Ù„Ø·Ù„Ø§Ø¨',
                'Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø³ÙƒÙ†ÙŠØ©', 'Ø£Ù…Ø§ÙƒÙ† Ø§Ù„Ø³ÙƒÙ†', 'Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¥Ù‚Ø§Ù…Ø©'
            ],
            'Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª': [
                'Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©', 'Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ÙƒØªØ¨Ø©', 'Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø¯Ø±Ø§Ø³Ø©', 'Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ù…ÙƒØªØ¨Ø©',
                'Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¨Ø­Ø«ÙŠØ©', 'Ø£Ù…Ø§ÙƒÙ† Ø§Ù„Ø¯Ø±Ø§Ø³Ø©', 'ÙƒØªØ§Ù„ÙˆØ¬ Ø§Ù„Ù…ÙƒØªØ¨Ø©'
            ],
            'Ù†Ø¸Ø§Ù… Ø§Ù„Ø¯Ø±Ø¬Ø§Øª': [
                'Ù†Ø¸Ø§Ù… Ø§Ù„Ø¯Ø±Ø¬Ø§Øª', 'Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„ØªÙ‚Ø¯ÙŠØ±', 'ÙƒÙŠÙÙŠØ© Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¹Ø¯Ù„', 'Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ',
                'Ø®Ø·Ø© Ø§Ù„Ø¯Ø±Ø¬Ø§Øª', 'ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª', 'ÙƒØ´Ù Ø§Ù„Ø¯Ø±Ø¬Ø§Øª'
            ],
            'Ø§Ù„Ø§Ù†ØªØ­Ø§Ù„ Ø§Ù„Ø¹Ù„Ù…ÙŠ': [
                'Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø§Ù†ØªØ­Ø§Ù„', 'Ø§Ù„Ø£Ù…Ø§Ù†Ø© Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©', 'Ø³ÙŠØ§Ø³Ø© Ø§Ù„ØºØ´', 'Ù…ÙƒØ§ÙØ­Ø© Ø§Ù„Ø§Ù†ØªØ­Ø§Ù„',
                'Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ÙŠØ§Øª Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©', 'Ù…Ø®Ø§Ù„ÙØ§Øª Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©', 'Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù†Ø²Ø§Ù‡Ø©'
            ],
            'attendance': [
                'Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø­Ø¶ÙˆØ±', 'Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø­Ø¶ÙˆØ±', 'Ø§Ù„ØºÙŠØ§Ø¨', 'Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø­Ø¶ÙˆØ±', 'Ø§Ù„ØªØºÙŠØ¨ Ø¹Ù† Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø§Øª'
            ],
            'Ø¨Ø­Ø«': [
                'Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ù„Ù…ÙŠ', 'ÙØ±Øµ Ø§Ù„Ø¨Ø­Ø«', 'Ø§Ù„Ù…Ø®ØªØ¨Ø±Ø§Øª Ø§Ù„Ø¨Ø­Ø«ÙŠØ©', 'Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø£Ø¨Ø­Ø§Ø«',
                'Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø¨Ø­Ø«', 'Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ø¨Ø­Ø«ÙŠØ©'
            ],
            'it_support': [
                'Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ', 'Ù…Ø³Ø§Ø¹Ø¯Ø© ØªÙ‚Ù†ÙŠØ©', 'Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±', 'Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…',
                'Ù…ÙƒØªØ¨ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©', 'Ù…Ø´ÙƒÙ„Ø© ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„', 'Ù…Ø´ÙƒÙ„Ø© ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±',
                'Ù…Ø´ÙƒÙ„Ø© Ø§Ù„ÙˆØ§ÙŠ ÙØ§ÙŠ', 'Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬', 'Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø£Ø¬Ù‡Ø²Ø©'
            ],
            'Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©': [
                'Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©', 'Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©', 'Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª', 'Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª',
                'Ø´Ø±ÙˆØ· Ø§Ù„Ù‚Ø¨ÙˆÙ„', 'Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…', 'Ø§Ù„Ø£Ù‡Ù„ÙŠØ©', 'Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© ÙÙŠ Ø§Ù„Ù‚Ø¨ÙˆÙ„',
                'Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±', 'Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ø¹Ø§Ù…', 'Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ­ÙˆÙŠÙ„', 'Ø§Ù„ØªØ®Ø±Ø¬', 'Ø§Ù„ØªÙ‚Ø¯ÙŠÙ… Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¯Ø±Ø§Ø³Ø©'
            ]
        }

        
        # Create intent embeddings for semantic matching
        self.intent_embeddings = None
        if self.sentence_model:
            self.create_intent_embeddings()
        
        # Conversation context
        self.user_context = {'last_intent': None, 'entities': {}}
        self.conversation_state = None
        self.user_type = None

        # PDFs
        self.pdf_docs = {
            "Student Rights Protection Unit": {"url": "...", "text": "", "embeddings": None, "chunks": None},
            "Engineering Regulations": {"url": "...", "text": "", "embeddings": None, "chunks": None}
        }
        
        # Corpus placeholders (JSON + PDFs)
        self.corpus = []
        self.corpus_metadata = []
        self.corpus_embeddings = None
        
    async def load_pdf_async(self, doc_name, doc_info):
        """Download PDF, extract text with fitz, split into chunks, compute embeddings asynchronously.
        Skip if embeddings already exist."""
        try:
            # Skip if embeddings are already present
            if doc_info.get("embeddings") is not None and doc_info.get("text"):
                print(f"Skipping {doc_name}, embeddings already loaded.")
                return

            async with aiohttp.ClientSession() as session:
                async with session.get(doc_info["url"]) as response:
                    pdf_bytes = await response.read()
            
            pdf_file = BytesIO(pdf_bytes)
            text = ""
            with fitz.open(stream=pdf_file, filetype="pdf") as pdf:
                for page in pdf:
                    page_text = page.get_text()
                    if page_text:
                        text += page_text + "\n"
            
            self.pdf_docs[doc_name]["text"] = text
            # Split text into chunks
            chunks = [text[i:i+1000] for i in range(0, len(text), 1000)]
            self.pdf_docs[doc_name]["chunks"] = chunks

            # Compute embeddings asynchronously only if sentence_model is available
            if self.sentence_model:
                loop = asyncio.get_event_loop()
                embeddings = await loop.run_in_executor(
                    None, lambda: self.sentence_model.encode(
                        chunks, show_progress_bar=False, convert_to_tensor=True
                    )
                )
                self.pdf_docs[doc_name]["embeddings"] = embeddings
                print(f"Loaded and embedded PDF: {doc_name}")
            else:
                print(f"Sentence model not loaded, skipping embeddings for {doc_name}")

        except Exception as e:
            print(f"Error loading {doc_name}: {e}")

    async def load_all_pdfs(self):
        """Load all PDFs concurrently and update corpus embeddings"""
        tasks = [
            self.load_pdf_async(doc_name, doc_info)
            for doc_name, doc_info in self.pdf_docs.items()
        ]
        await asyncio.gather(*tasks)

        # Recompute embeddings for the entire corpus (JSON + PDFs)
        if self.corpus and self.sentence_model:
            print(f"Creating embeddings for {len(self.corpus)} text segments (JSON + PDFs)...")
            self.corpus_embeddings = self.sentence_model.encode(self.corpus, convert_to_tensor=True)
            print("Semantic corpus ready")
        
    # Loading the data    
    def load_data(self, json_file_path):
        """Load JSON safely."""
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"Loaded university data from {json_file_path}")
                return data
        except Exception as e:
            print(f"Error loading JSON: {e}")
            return {}
    
    # Add the semantic embeddings onto the text corpus
    def prepare_semantic_corpus(self):
        """Prepare text corpus with semantic embeddings"""
        self.corpus = []
        self.corpus_metadata = []
        self.corpus_embeddings = None
        
        def extract_text_recursive(data, path="", parent_key="", parent_obj=None):
            if isinstance(data, dict):
                for key, value in data.items():
                    current_path = f"{path}/{key}" if path else key
                    extract_text_recursive(value, current_path, key, data)
            elif isinstance(data, list):
                for i, item in enumerate(data):
                    current_path = f"{path}[{i}]"
                    extract_text_recursive(item, current_path, parent_key, data)
            elif isinstance(data, str) and len(data.strip()) > 10:
                # Extract additional fields from parent object
                additional_fields = {}
                if parent_obj and isinstance(parent_obj, dict):
                    # Look for common additional fields like 'info', 'url', 'link', etc.
                    for field in ['info', 'url', 'link', 'source', 'reference']:
                        if field in parent_obj and parent_obj[field]:
                            additional_fields[field] = parent_obj[field]
                
                self.corpus.append(data.strip())
                self.corpus_metadata.append({
                    'path': path,
                    'parent_key': parent_key,
                    'content': data.strip(),
                    **additional_fields  # Include additional fields from parent object
                })
        
        extract_text_recursive(self.data)
        
        if self.corpus and self.sentence_model:
            print(f"Creating embeddings for {len(self.corpus)} text segments...")
            self.corpus_embeddings = self.sentence_model.encode(self.corpus, convert_to_tensor=True)
            print("Semantic corpus ready")
    
    # Creating and adding embeddings for intent patterns and for better semantic matching
    def create_intent_embeddings(self):
        """Create embeddings for intent patterns for better semantic matching"""
        self.intent_embeddings = {}
        
        for intent, patterns in self.intent_patterns.items():
            # Create embeddings for all patterns of this intent
            pattern_embeddings = self.sentence_model.encode(patterns, convert_to_tensor=True, batch_size=16)
            # Use mean embedding as the intent representation
            self.intent_embeddings[intent] = torch.mean(pattern_embeddings, dim=0)
    
    # Identifying user intent using semantic similarity
    def identify_intent_semantic(self, user_input):
        """Identify user intent using semantic similarity"""
        if not self.sentence_model or not hasattr(self, 'intent_embeddings'):
            return self.identify_intent_fallback(user_input)
        
        user_embedding = self.sentence_model.encode([user_input], convert_to_tensor=True)
        
        best_intent = 'general'
        best_score = 0.0
        
        for intent, intent_embedding in self.intent_embeddings.items():
            # Calculate cosine similarity
            similarity = torch.cosine_similarity(user_embedding, intent_embedding.unsqueeze(0))
            score = similarity.item()
            
            if score > best_score and score > 0.3:  # Threshold for intent confidence
                best_score = score
                best_intent = intent
        
        return best_intent, best_score
    
    # Falling back using keyword matching 
    def identify_intent_fallback(self, user_input):
        """Fallback intent identification using keyword matching"""
        user_input_lower = user_input.lower()
        intent_scores = defaultdict(float)
        
        # Keyword matching
        for intent, patterns in self.intent_patterns.items():
            for pattern in patterns:
                if pattern.lower() in user_input_lower:
                    intent_scores[intent] += 1
        
        if intent_scores:
            best_intent = max(intent_scores.items(), key=lambda x: x[1])[0]
            best_score = intent_scores[best_intent] / len(self.intent_patterns[best_intent])
            return best_intent, best_score
        
        return 'general', 0.0
    
    # Extradting entities and adding similar names to recognize them 
    def extract_entities_enhanced(self, user_input):
        """Enhanced entity extraction with better NLP"""
        entities = {
            'college': None,
            'department': None,
            'level': None,
            'user_type': None,
            'specific_info': []
        }
        
        user_input_lower = user_input.lower()
        
        # Enhanced education level detection
        level_patterns = {
            'arabic_degree_keywords': {
                'undergraduate': ['Ø¨ÙƒØ§Ù„ÙˆØ±ÙŠÙˆØ³', 'Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¨ÙƒØ§Ù„ÙˆØ±ÙŠÙˆØ³', 'Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰', 'Ø§Ù„Ù„ÙŠØ³Ø§Ù†Ø³', 'Ø§Ù„Ø¨ÙƒØ§Ù„ÙˆØ±ÙŠÙˆØ³'],
                'masters': ['Ù…Ø§Ø¬Ø³ØªÙŠØ±', 'Ø¯Ø±Ø¬Ø© Ø§Ù„Ù…Ø§Ø¬Ø³ØªÙŠØ±', 'Ø§Ù„Ø¯Ø±Ø§Ø³Ø§Øª Ø§Ù„Ø¹Ù„ÙŠØ§', 'Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠ Ø§Ù„Ø«Ø§Ù†ÙŠ', 'Ù…Ø§Ø³ØªØ±', 'Ø¯Ø±Ø¬Ø© Ø§Ù„Ù…Ø§Ø¬Ø³ØªÙŠØ± Ø§Ù„Ù…Ù‡Ù†ÙŠØ©'],
                'phd': ['Ø¯ÙƒØªÙˆØ±Ø§Ù‡', 'Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø§Ù‡', 'Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø§Ù‡ Ø§Ù„ÙÙ„Ø³ÙÙŠØ©', 'Ø¯ÙƒØªÙˆØ±', 'Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø§Ù‡']
            }
        }
        
        for level, patterns in level_patterns.items():
            if any(pattern in user_input_lower for pattern in patterns):
                entities['level'] = level
                break
        
        # Enhanced user type detection
        user_type_patterns = {
            'student': ['Ø·Ø§Ù„Ø¨', 'Ø·Ù„Ø§Ø¨', 'ØªÙ„Ù…ÙŠØ°', 'Ù…ØªØ¹Ù„Ù…ÙŠÙ†', 'Ø¯Ø§Ø±Ø³', 'Ø¯Ø§Ø±Ø³Ø©', 'Ø§Ù„Ø·Ø§Ù„Ø¨', 'Ø§Ù„Ø·Ø§Ù„Ø¨Ø©', 'Ø§Ù„Ø·Ù„Ø§Ø¨', 'Ø§Ù„Ø¯Ø§Ø±Ø³ÙˆÙ†'],
            'staff': ['Ù…ÙˆØ¸Ù', 'Ø§Ù„Ù…ÙˆØ¸Ù', 'Ù…ÙˆØ¸ÙÙŠÙ†', 'Ø¹Ù…Ø§Ù„', 'Ø§Ù„Ø¹Ø§Ù…Ù„ÙŠÙ†', 'Ø¥Ø¯Ø§Ø±ÙŠ', 'Ø¥Ø¯Ø§Ø±ÙŠÙŠÙ†', 'Ø§Ù„Ø¹Ø§Ù…Ù„', 'Ø§Ù„Ù…ÙˆØ¸ÙÙŠÙ†', 'Ø¥Ø¯Ø§Ø±Ø©', 'Ø¥Ø¯Ø§Ø±ÙŠ/Ø¥Ø¯Ø§Ø±ÙŠØ©'],
            'faculty': ['Ø£Ø³ØªØ§Ø°', 'Ø£Ø³ØªØ§Ø°Ø©', 'Ø§Ù„Ø£Ø³Ø§ØªØ°Ø©', 'Ø£Ø¹Ø¶Ø§Ø¡ Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³', 'Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³', 'Ù…Ø¯Ø±Ø³', 'Ù…Ø¹Ù„Ù…', 'Ù…Ø­Ø§Ø¶Ø±', 'Ù…Ø­Ø§Ø¶Ø±Ø©', 'Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ', 'Ø§Ù„Ø·Ø§Ù‚Ù… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ']
        }
        
        for user_type, patterns in user_type_patterns.items():
            if any(pattern in user_input_lower for pattern in patterns):
                entities['user_type'] = user_type
                break
        
        # Use spaCy for named entity recognition
        if self.nlp:
            doc = self.nlp(user_input)
            for ent in doc.ents:
                if ent.label_ == "ORG":
                    entities['college'] = ent.text
                elif ent.label_ in ["PERSON", "GPE"]:
                    entities['specific_info'].append(ent.text)
        
        return entities
    
    # Find most relevant content using semantic similarity which includes fallback check, encoding query, comparing similarities, best matching and filtering based on the similarity threshold
    def find_relevant_content_semantic(self, query, top_k=5, threshold=0.25):
        """Find most relevant content using semantic similarity"""
        if not self.sentence_model or self.corpus_embeddings is None or len(self.corpus) == 0:
            return self.find_relevant_content_fallback(query, top_k)
        
        query_embedding = self.sentence_model.encode([query], convert_to_tensor=True)
        
        # Calculate cosine similarities
        similarities = torch.cosine_similarity(query_embedding, self.corpus_embeddings)
        
        # Get top-k most similar documents
        top_indices = torch.topk(similarities, min(top_k, len(similarities))).indices
        
        results = []
        for idx in top_indices:
            score = similarities[idx].item()
            if score > threshold:
                results.append({
                    'content': self.corpus[idx],
                    'metadata': self.corpus_metadata[idx],
                    'similarity': score
                })
        
        return results
    
    # Fallback method for the previous one 
    def find_relevant_content_fallback(self, query, top_k=5):
        """Fallback content search using simple text matching"""
        query_lower = query.lower()
        results = []
        
        for i, content in enumerate(self.corpus):
            content_lower = content.lower()
            # Simple relevance scoring based on keyword overlap
            query_words = set(query_lower.split())
            content_words = set(content_lower.split())
            overlap = len(query_words.intersection(content_words))
            
            if overlap > 0:
                score = overlap / len(query_words)
                results.append({
                    'content': content,
                    'metadata': self.corpus_metadata[i],
                    'similarity': score
                })
        
        # Sort by similarity and return top k
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]
    
    # Extracting links from content
    def extract_links_from_content(self, content):
        """Extract URLs from content"""
        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        links = re.findall(url_pattern, str(content))
        return links
    
    def format_table_from_html(self, html_content):
        """Convert HTML table to readable format"""
        if '<table' in str(html_content).lower():
            return f"Table Data:\n{str(html_content)}\n"
        return str(html_content)
    
    # Help getter function to search for departments and colleges
    def get_colleges_and_departments(self):
        """Extract list of colleges and departments from data"""
        colleges = {}
        
        def search_recursive(data, path=""):
            if isinstance(data, dict):
                for key, value in data.items():
                    current_path = f"{path}/{key}" if path else key
                    
                    # Check if this looks like a college or department
                    if any(term in key for term in ['Ø§Ù„ÙƒÙ„ÙŠØ§Øª', 'Ù‚Ø³Ù…', 'Ø£Ø¹Ø¶Ø§Ø¡ Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³ ']):
                        colleges[key] = []
                        
                        # Look for departments within this college
                        if isinstance(value, dict):
                            for sub_key in value.keys():
                                if any(term in sub_key.lower() for term in ['Ù‚Ø³Ù…', 'Ø£Ù‚Ø³Ø§Ù…', 'Ø´Ø¹Ø¨Ø©', 'Ø¨Ø±Ù†Ø§Ù…Ø¬', 'Ø¨Ø±Ø§Ù…Ø¬', 'Ø§Ù„Ø®Ø·Ø© Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ©', 'Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ', 'Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ']):
                                    colleges[key].append(sub_key)
                    
                    search_recursive(value, current_path)
        
        search_recursive(self.data)
        
        # If no colleges found, try semantic search
        if not colleges and self.sentence_model:
            college_query = "ÙƒÙ„ÙŠØ© Ù…Ø¯Ø±Ø³Ø© Ù‚Ø³Ù… Ø´Ø¹Ø¨Ø© Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©"
            relevant_content = self.find_relevant_content_semantic(college_query, top_k=10)
            
            for content in relevant_content:
                path = content['metadata']['path']
                if any(term in path.lower() for term in ['ÙƒÙ„ÙŠØ©', 'Ù…Ø¯Ø±Ø³Ø©', 'Ù‚Ø³Ù…']):
                    parts = path.split('/')
                    for part in parts:
                        if any(term in part.lower() for term in ['ÙƒÙ„ÙŠØ©', 'Ù…Ø¯Ø±Ø³Ø©']):
                            if part not in colleges:
                                colleges[part] = []
        
        return colleges

    # This function helps get admission requirements of undergraduate (found in FAQs), masters and PhD from links found in Admission Requirements section from the json file
    def handle_admission_requirements(self, entities, user_input):
        """Admission requirements handler with JSON navigation and follow-up support"""

        if not hasattr(self, 'conversation_state'):
            self.conversation_state = None

        # Handle follow-up input if waiting for level
        if self.conversation_state == "awaiting_admission_level":
            # Re-extract entities from the follow-up input
            follow_up_entities = self.extract_entities_enhanced(user_input)
            if follow_up_entities['level']:
                entities['level'] = follow_up_entities['level']
            
            self.conversation_state = None

        # If still no level detected, prompt user
        if not entities['level']:
            self.conversation_state = "awaiting_admission_level"
            return (
                "Ù…Ø±Ø­Ø¨Ù‹Ø§! ÙŠØ³Ø¹Ø¯Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø°ÙŠ ØªÙ‡ØªÙ… Ø¨Ù‡ØŸ\n"
                "Ø¨ÙƒØ§Ù„ÙˆØ±ÙŠÙˆØ³ (Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n"
                "Ù…Ø§Ø¬Ø³ØªÙŠØ± (Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠØ© Ø§Ù„Ø¹Ù„ÙŠØ§)\n"
                "Ø¯ÙƒØªÙˆØ±Ø§Ù‡ (Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¹Ù„Ù…ÙŠØ©)\n\n"
                "ÙÙ‚Ø· Ø£Ø¬Ø¨ Ø¨Ø£Ø­Ø¯ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø£Ø¹Ù„Ø§Ù‡!"
            )

        self.conversation_state = None  # Clear state

        # Handle UNDERGRADUATE requests
        if entities['level'] == 'undergraduate':
            # Look for FAQs section
            for section in self.data:
                if section.get("title", "") == "Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©":
                    # Look for the specific FAQ about high school certificate requirements
                    for faq in section.get("children", []):
                        title = faq.get("title", "").lower()
                        if "Ø§Ù„Ø­Ø§ØµÙ„ÙŠÙ† Ø¹Ù„Ù‰ Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© Ù…Ù† Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø£Ùˆ Ø®Ø§Ø±Ø¬Ù‡Ø§ Ù…Ø·Ø§Ù„Ø¨ÙŠÙ† Ø¨Ø£Ø¯Ø§Ø¡ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„ÙˆØ·Ù†ÙŠ" in title:
                            answer = faq.get("answer", "Ù„Ù… ÙŠØªÙ… ØªÙ‚Ø¯ÙŠÙ… Ø£ÙŠ Ø¥Ø¬Ø§Ø¨Ø©")
                            url = section.get("url", "")
                            
                            return (
                                "Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ ÙÙŠ Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø¨ÙƒØ§Ù„ÙˆØ±ÙŠÙˆØ³: \n\n"
                                f"{answer}\n\n"
                                f"Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©ØŒ Ù‚Ù… Ø¨Ø²ÙŠØ§Ø±Ø©: {url}"
                            )
                    
                    # If specific FAQ not found, provide general undergraduate info from FAQs
                    url = section.get("url", "")
                    return (
                        "Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¨ÙƒØ§Ù„ÙˆØ±ÙŠÙˆØ³: \n\n"
                        "Ø§Ø³ØªÙ†Ø§Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©ØŒ ØªØ´Ù…Ù„ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¨ÙƒØ§Ù„ÙˆØ±ÙŠÙˆØ³:\n\n"
                        "â€¢ Ø´Ù‡Ø§Ø¯Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© (Ù…Ù† Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø£Ùˆ Ø®Ø§Ø±Ø¬Ù‡Ø§)\n"
                        "â€¢ Ù†ØªØ§Ø¦Ø¬ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©\n"
                        "â€¢ Ù†ØªØ§Ø¦Ø¬ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ­ØµÙŠÙ„ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ\n"
                        "â€¢ Ø§Ø³ØªÙŠÙØ§Ø¡ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©\n\n"
                        f"Ù„Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„Ø© ÙˆØ§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©ØŒ ÙŠØ±Ø¬Ù‰ Ø²ÙŠØ§Ø±Ø©: {url}"
                    )

        # Handle MASTERS requests
        elif entities['level'] == 'masters':
            # Look for Admission Requirements section
            for section in self.data:
                if section.get("title", "") == "Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ ÙÙŠ Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„Ùƒ Ø³Ø¹ÙˆØ¯":
                    # Look for Master's Programs in children
                    for child in section.get("children", []):
                        if child.get("title", "").lower() in ["masterâ€™s programs", "master's programs"]:
                            url = child.get("url", section.get("url", ""))
                            content = child.get("content", "")
                            
                            # Drill down one more level into nested children
                            sub_items = child.get("children", [])
                            if sub_items:
                                content += "\n\nØªÙØ§ØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØ©:\n"
                                for item in sub_items:
                                    title = item.get("title", "")
                                    body = item.get("content", "Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙØ§ØµÙŠÙ„ Ù…ØªØ§Ø­Ø©.")
                                    content += f"\n {title}: {body}â€¢ "
                            
                            return (
                                "Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ù…Ø§Ø¬Ø³ØªÙŠØ±:\n\n"
                                f"{content}\n\n"
                                f"{url} : Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©ØŒ ÙŠØ±Ø¬Ù‰ Ø²ÙŠØ§Ø±Ø©"
                            )

                    
                    # If Master's Programs not found in children, provide general info
                    url = section.get("url", "")
                    content = section.get("content", "")
                    return (
                        "Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ù…Ø§Ø¬Ø³ØªÙŠØ±\n\n"
                        f"{content}\n\n"
                        "ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù‚Ø³Ù… Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ù…Ø§Ø¬Ø³ØªÙŠØ± Ø§Ù„Ù…Ø­Ø¯Ø¯.\n\n"
                        f"{url} : Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©ØŒ ÙŠØ±Ø¬Ù‰ Ø²ÙŠØ§Ø±Ø©"
                    )

        # Handle PHD requests
        elif entities['level'] == 'phd':
            # Look for Admission Requirements section
            for section in self.data:
                if section.get("title", "").lower() == "Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ ÙÙŠ Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„Ùƒ Ø³Ø¹ÙˆØ¯:":
                    # Look for PhD Programs in children
                    for child in section.get("children", []):
                        if child.get("title", "").lower() in ["phd programs"]:
                            url = child.get("url", section.get("url", ""))
                            content = child.get("content", "")
                            
                            # Drill down one more level into nested children
                            sub_items = child.get("children", [])
                            if sub_items:
                                content += "\n\nØªÙØ§ØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØ©:\n"
                                for item in sub_items:
                                    title = item.get("title", "")
                                    body = item.get("content", "Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙØ§ØµÙŠÙ„ Ù…ØªØ§Ø­Ø©.")
                                    content += f"\nâ€¢ **{title}**: {body}"
                            
                            return (
                                "Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ ÙÙŠ Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø§Ù‡:\n\n"
                                f"{content}\n\n"
                                f"Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§Ù…Ù„Ø©ØŒ Ù‚Ù… Ø¨Ø²ÙŠØ§Ø±Ø©: {url}"
                            )
                    
                    # If PhD Programs not found in children, provide general info
                    url = section.get("url", "")
                    content = section.get("content", "")
                    return (
                        "Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ ÙÙŠ Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø§Ù‡:\n\n"
                        f"{content}\n\n"
                        "ÙŠØ±Ø¬Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø© Ù‚Ø³Ù… Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ù„Ù…Ø¹Ø±ÙØ© Ù…Ø¹Ø§ÙŠÙŠØ± Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø§Ù‡ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.\n\n"
                        f"Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§Ù…Ù„Ø©ØŒ Ù‚Ù… Ø¨Ø²ÙŠØ§Ø±Ø©:{url}"
                    )

        # Fallback if level is recognized but no specific info found
        return (
            f"Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ù„Ù€ {entities['level']} ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§. "
            "ÙŠØ±Ø¬Ù‰ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ù…ÙƒØªØ¨ Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ù…Ø¨Ø§Ø´Ø±Ø© Ø£Ùˆ Ø²ÙŠØ§Ø±Ø© Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© ÙˆØ­Ø¯Ø§Ø«Ø©."
        )

    # This function helps get academic calendar from the json file and format it into a table by putting each row on top of another (since it is not in HTML format)
    def handle_academic_calendar(self, query: str = None):
        """Return academic calendar or specific date/event if query is given"""

        # Look for the academic calendar item in self.data
        calendar_item = next(
            (item for item in self.data if item.get("title", "") == 'Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ'),
            None
        )

        if not calendar_item:
            return (
                "Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ"
                "Ù„Ù… Ø£Ø¬Ø¯ Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ ÙÙŠ Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§."
                "ÙŠØ±Ø¬Ù‰ Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø±Ø³Ù…ÙŠ Ù„Ø¹Ù…Ø§Ø¯Ø© Ø§Ù„Ù‚Ø¨ÙˆÙ„ ÙˆØ§Ù„ØªØ³Ø¬ÙŠÙ„ Ù„Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ Ø£Ø­Ø¯Ø« Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª:"
                "https://dar.ksu.edu.sa/ar/CurrentCalendar"
                )

        url = calendar_item.get("url", "")
        table = calendar_item.get("table", {})
        headers = table.get("headers", [])
        rows = table.get("rows", [])

        # Clean duplicate header row if present
        if rows and headers and rows[0] == headers:
            rows = rows[1:]

        # If user asked for a specific event/date
        if query:
            query_lower = query.lower()

            for row in rows:
                row_text = " ".join(row).lower()

                # If query matches part of an event OR part of a date
                if query_lower in row_text:
                    gregorian_date = row[0] if len(row) > 0 else "N/A"
                    hijri_date = row[1] if len(row) > 1 else "N/A"
                    day = row[2] if len(row) > 2 else "N/A"
                    event = row[3] if len(row) > 3 else "N/A"

                    return (
                        f"{event}\n\n"
                        f"- Ù…ÙŠÙ„Ø§Ø¯ÙŠ: {gregorian_date}\n"
                        f"- Ù‡Ø¬Ø±ÙŠ: {hijri_date}\n"
                        f"- Ø§Ù„ÙŠÙˆÙ…: {day}\n\n"
                        f"Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„ÙƒØ§Ù…Ù„: {url}"
                    )

            return f"Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† '{query}'. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„ÙƒØ§Ù…Ù„: {url}"

        # Otherwise, return full table in a neat markdown format
        response = "Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ\n\n"
        response += f"{url}: Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„ÙƒØ§Ù…Ù„\n\n"

        if headers:
            response += "| " + " | ".join(headers) + " |\n"
            response += "|" + " --- |" * len(headers) + "\n"

        for row in rows:
            padded_row = row + [""] * (len(headers) - len(row))
            response += "| " + " | ".join(padded_row[:len(headers)]) + " |\n"

        response += (
            "\nğŸ’¡ Ù†ØµØ§Ø¦Ø­: ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø³Ø¤Ø§Ù„ÙŠ Ø¹Ù† Ø£Ø´ÙŠØ§Ø¡ Ù…Ø«Ù„:\n"
            "'Ù…ØªÙ‰ ØªØ¨Ø¯Ø£ Ø§Ù„Ø§Ù…ØªØ­Ø§Ù†Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©ØŸ' -\n"
            "'Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù…ÙˆØ¹Ø¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„ØªØ³Ø¬ÙŠÙ„ØŸ' -\n"
            "'Ù…ØªÙ‰ Ø¹Ø·Ù„Ø© Ø§Ù„ÙŠÙˆÙ… Ø§Ù„ÙˆØ·Ù†ÙŠØŸ' -"
        )
        return response

    # This function gets the college categories, colleges, departments, their about, contact info and faculty directories
    def handle_degree_programs(self, entities, user_input):
        """
        Simple method to navigate college hierarchy
        Handles: Colleges -> College Categories -> Individual Colleges -> Academic Departments
        """
        
        def find_node(target_title, json_data):
            """Find node by title with fuzzy matching"""
            target_lower = target_title.lower().strip()
            best_match = None
            best_score = 0
            
            # Check if this is a department search
            is_department_search = any(word in target_lower for word in ['Ù‚Ø³Ù…', 'Ø§Ù„Ø£Ù‚Ø³Ø§Ù…'])
            
            # Debug: track all nodes we're checking
            checked_nodes = []
            
            def search_recursive(node, path="root"):
                nonlocal best_match, best_score
                
                if isinstance(node, dict) and 'title' in node:
                    title_lower = node['title'].lower().strip()
                    checked_nodes.append((path, node['title']))
                    
                    # Calculate match score
                    if title_lower == target_lower:
                        score = 100
                    elif target_lower in title_lower:
                        score = 90
                    elif title_lower in target_lower:
                        score = 85
                    else:
                        # Word overlap
                        target_words = set(target_lower.split())
                        title_words = set(title_lower.split())
                        if target_words and title_words:
                            overlap = len(target_words.intersection(title_words))
                            union = len(target_words.union(title_words))
                            score = (overlap / union) * 80
                        else:
                            score = 0
                    
                    if score > best_score:
                        best_score = score
                        best_match = node
                    
                    # Special handling for department searches
                    if is_department_search and 'children' in node:
                        # Look for Academic Departments section
                        for child in node.get('children', []):
                            if isinstance(child, dict) and child.get('section') == 'Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©':
                                # Search within the departments
                                for dept in child.get('children', []):
                                    if isinstance(dept, dict) and 'title' in dept:
                                        dept_title_lower = dept['title'].lower().strip()
                                        checked_nodes.append((f"{path} -> Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© -> {dept['title']}", dept['title']))
                                        
                                        # Calculate department match score
                                        if dept_title_lower == target_lower:
                                            dept_score = 100
                                        elif target_lower in dept_title_lower:
                                            dept_score = 90
                                        elif dept_title_lower in target_lower:
                                            dept_score = 85
                                        else:
                                            # Word overlap for departments
                                            dept_target_words = set(target_lower.split())
                                            dept_title_words = set(dept_title_lower.split())
                                            if dept_target_words and dept_title_words:
                                                dept_overlap = len(dept_target_words.intersection(dept_title_words))
                                                dept_union = len(dept_target_words.union(dept_title_words))
                                                dept_score = (dept_overlap / dept_union) * 80
                                            else:
                                                dept_score = 0
                                        
                                        if dept_score > best_score:
                                            best_score = dept_score
                                            best_match = dept
                    
                    # Search children normally
                    if 'children' in node:
                        for i, child in enumerate(node['children']):
                            search_recursive(child, f"{path} -> children[{i}]")
                
                elif isinstance(node, list):
                    for i, item in enumerate(node):
                        search_recursive(item, f"{path}[{i}]")
            
            search_recursive(json_data)
            
            return best_match if best_score > 30 else None
        
        def get_node_type(node):
            """Determine what type of node this is"""
            if not isinstance(node, dict) or 'title' not in node:
                return 'unknown'
            
            title = node['title'].lower()
            
            # Main "Colleges" node
            if title == 'Ø§Ù„ÙƒÙ„ÙŠØ§Øª' and 'children' in node:
                return 'colleges_root'
            
            # College categories (contains "colleges" in title)
            if 'Ø§Ù„ÙƒÙ„ÙŠØ§Øª' in title and 'children' in node:
                return 'category'
            
            # Individual college (has "About College" section)
            if 'children' in node:
                for child in node.get('children', []):
                    if isinstance(child, dict) and child.get('section') == 'Ø¹Ù† Ø§Ù„ÙƒÙ„ÙŠØ©':
                        return 'college'
            
            # Academic department (has contact_info, faculty_links, faculty_staff_links, or detailed content)
            if any(key in node for key in ['contact_info', 'faculty_links', 'faculty_staff_links']) or \
            (node.get('content') and len(node.get('content', '')) > 100):
                return 'department'
            
            return 'unknown'
        
        # Find the target node
        target_node = find_node(user_input.strip(), self.data)
        
        if not target_node:
            return (" :Ù„Ù… Ø£Ø³ØªØ·Ø¹ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø°Ù„Ùƒ. Ø­Ø§ÙˆÙ„\n"
                    "'Ø§Ù„ÙƒÙ„ÙŠØ§Øª' - Ø§Ù†Ø¸Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª â€¢\n"
                    "'Ø§Ù„ÙƒÙ„ÙŠØ§Øª Ø§Ù„Ø¹Ù„Ù…ÙŠØ©' - Ø§Ù†Ø¸Ø± Ø§Ù„ÙƒÙ„ÙŠØ§Øª ÙÙŠ Ø§Ù„ÙØ¦Ø© â€¢\n"
                    "'ÙƒÙ„ÙŠØ© Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©' - ÙƒÙ„ÙŠØ© Ù…Ø­Ø¯Ø¯Ø© â€¢\n"
                    "'Ù‚Ø³Ù… Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³Ø¨' - Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù‚Ø³Ù… â€¢")
        
        node_type = get_node_type(target_node)
        title = target_node.get('title', '')
        
        # Handle based on node type
        if node_type == 'colleges_root':
            # Show college categories only
            response = "Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„Ùƒ Ø³Ø¹ÙˆØ¯ - Ø§Ù„ÙƒÙ„ÙŠØ§Øª\n\n"
            response += ":ÙØ¦Ø§Øª Ø§Ù„ÙƒÙ„ÙŠØ§Øª\n\n"
            
            total_colleges = 0
            for category in target_node.get('children', []):
                if isinstance(category, dict) and 'title' in category:
                    college_count = len(category.get('children', []))
                    total_colleges += college_count
                    response += f" (ÙƒÙ„ÙŠØ§Øª {college_count} {category['title']}â€¢ )\n"
            
            response += f"\n ÙƒÙ„ÙŠØ§Øª {total_colleges}:Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ \n\n"
            response += "Ø§Ø³Ø£Ù„ Ø¹Ù† Ø£ÙŠ ÙØ¦Ø© Ø£Ø¹Ù„Ø§Ù‡ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§ØµÙŠÙ„"
            return response
        
        elif node_type == 'category':
            # Show colleges in this category
            response = f"{title}\n\n"
            colleges = target_node.get('children', [])
            
            if colleges:
                response += f":Ø§Ù„ÙƒÙ„ÙŠØ§Øª ({len(colleges)} Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹):\n\n"
                for i, college in enumerate(colleges, 1):
                    if isinstance(college, dict) and 'title' in college:
                        response += f"{i}. {college['title']}\n"
                        if college.get('url'):
                            response += f"   {college['url']}\n"
                        response += "\n"
            
            response += "Ø§Ø³Ø£Ù„ Ø¹Ù† Ø£ÙŠ ÙƒÙ„ÙŠØ© Ø£Ø¹Ù„Ø§Ù‡ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§ØµÙŠÙ„!"
            return response
        
        elif node_type == 'college':
            # Show college info + list departments (titles only)
            response = f"{title}\n\n"
            
            if target_node.get('url'):
                response += f"{target_node['url']}: Ù…ÙˆÙ‚Ø¹ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ\n\n"
            
            # Find About College section
            about_content = None
            departments = []
            
            for child in target_node.get('children', []):
                if isinstance(child, dict):
                    if child.get('section') == 'Ø¹Ù† Ø§Ù„ÙƒÙ„ÙŠØ©':
                        about_content = child.get('content', '')
                    elif child.get('section') == 'Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©':
                        departments = child.get('children', [])
            
            if about_content:
                response += f"Ø¹Ù† Ø§Ù„ÙƒÙ„ÙŠØ©:\n{about_content}\n\n"
            
            if departments:
                response += f"Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© ({len(departments)} Ù‚Ø³Ù…):\n\n"
                for i, dept in enumerate(departments, 1):
                    if isinstance(dept, dict) and 'title' in dept:
                        response += f"{i}. {dept['title']}\n"
                
                response += "\nÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø£ÙŠ Ù‚Ø³Ù… Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª!"
            
            return response
        
        elif node_type == 'department':
            # Show full department details
            response = f"{title}\n\n"
            
            if target_node.get('url'):
                response += f"Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ: {target_node['url']}\n\n"
            
            if target_node.get('content'):
                content = target_node['content'].strip()
                if content:
                    response += f"Ø¹Ù† Ø§Ù„Ù‚Ø³Ù…:\n{content}\n\n"
            
            if target_node.get('contact_info'):
                contact = target_node['contact_info'].strip()
                if contact:
                    response += f"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙˆØ§ØµÙ„:\n{contact}\n\n"
            
            # Faculty links
            faculty_links = target_node.get('faculty_links', []) + target_node.get('faculty_staff_links', [])
            if faculty_links:
                response += "Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³ ÙˆØ§Ù„Ù…ÙˆØ¸ÙÙˆÙ†:\n"
                for link in faculty_links:
                    if isinstance(link, dict):
                        title_text = link.get('title', 'Ø±Ø§Ø¨Ø·')
                        url = link.get('url', '#')
                        response += f"â€¢ [{title_text}]({url})\n"
                response += "\n"
            
            return response
        
        else:
            return f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ '{title}' Ù„ÙƒÙ† Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹Ù‡. ÙŠÙØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ø¯ÙŠØ¯ Ø¨Ø´ÙƒÙ„ Ø£Ø¯Ù‚."
       
    # This function finds the housing information (things that I thought were important)
    def handle_housing(self, entities, user_input):
        """Handle housing queries using the correct Housing section"""
        
        # Find the Housing section that actually has data (not the empty one)
        housing_section = self.find_housing_with_data()
        
        if not housing_section:
            return ".Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³ÙƒÙ†"
        
        # If no user type specified, ask for clarification
        if not entities.get('user_type'):
            housing_types = self.get_housing_types(housing_section)
            response = ":ÙŠØ³Ø¹Ø¯Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³ÙƒÙ†! ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªÙˆØ¶ÙŠØ­.\n\n"
            for housing_type in housing_types:
                emoji = "ğŸ“" if "Ø§Ù„Ø·Ù„Ø§Ø¨" in housing_type else "ğŸ‘¨â€ğŸ«"
                response += f"{emoji} {housing_type}\n"
            return response + "\nÙ…Ø§ Ù‡Ùˆ Ù†ÙˆØ¹ Ø§Ù„Ø³ÙƒÙ† Ø§Ù„Ø°ÙŠ Ø£Ù†Øª Ù…Ù‡ØªÙ… Ø¨Ù‡ØŸ"
        
        # Handle faculty housing
        if entities['user_type'].lower() in [
            'Ø¹Ø¶Ùˆ Ù‡ÙŠØ¦Ø© ØªØ¯Ø±ÙŠØ³', 'Ø¹Ø¶ÙˆØ© Ù‡ÙŠØ¦Ø© ØªØ¯Ø±ÙŠØ³', 'Ø£Ø¹Ø¶Ø§Ø¡ Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³', 'Ø¹Ø¶ÙˆØ§Øª Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³',  
            'Ø§Ù„Ù…ÙˆØ¸Ù', 'Ø§Ù„Ù…ÙˆØ¸ÙØ©', 'Ø§Ù„Ù…ÙˆØ¸ÙÙŠÙ†', 'Ø§Ù„Ù…ÙˆØ¸ÙØ§Øª', 'Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³']:
            return self.handle_faculty_housing(housing_section)
        
        # Handle student housing
        elif entities['user_type'] in ['Ø·Ø§Ù„Ø¨', 'Ø·Ø§Ù„Ø¨Ø©', 'Ø·Ù„Ø§Ø¨', 'Ø·Ø§Ù„Ø¨Ø§Øª']:
            return self.handle_student_housing(housing_section)
        
        return ".ÙŠØ±Ø¬Ù‰ ØªØ­Ø¯ÙŠØ¯ Ø³ÙƒÙ† Ø£Ø¹Ø¶Ø§Ø¡ Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³ Ø£Ùˆ Ø§Ù„Ø·Ù„Ø§Ø¨."

    # This addition function gets the housing information
    def find_housing_with_data(self):
        """Find the Housing section that contains actual data (not empty)"""
        def search_recursive(data):
            if isinstance(data, dict):
                title = data.get('title', '')
                if title.lower() == 'housing' and data.get('children'):
                    # Found Housing section with children - this is the one we want
                    return data
                
                # Search in children
                for child in data.get('children', []):
                    result = search_recursive(child)
                    if result:
                        return result
            elif isinstance(data, list):
                for item in data:
                    result = search_recursive(item)
                    if result:
                        return result
            return None
        
        return search_recursive(self.data)

    # This additional function gets housing type whether it is student or faculty housing
    def get_housing_types(self, housing_section):
        """Extract housing types from the housing section"""
        housing_types = []
        for child in housing_section.get('children', []):
            title = child.get('title', '')
            if title and 'Ø§Ø³ÙƒØ§Ù† Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„Ùƒ Ø³Ø¹ÙˆØ¯' in title:
                housing_types.append(title)
        return housing_types

    # This additional function helps navigating Faculty housing sections that I thought were important
    def handle_faculty_housing(self, housing_section):
        """Handle faculty housing navigation"""
        # Find Faculty Housing section
        faculty_housing = None
        for child in housing_section.get('children', []):
            if child.get('title', '') == 'Ø§Ø³ÙƒØ§Ù† Ø£Ø¹Ø¶Ø§Ø¡ Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³':
                faculty_housing = child
                break
        
        if not faculty_housing:
            available_children = [child.get('title', 'No title') for child in housing_section.get('children', [])]
            return f"{', '.join(available_children)}: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø³ÙƒÙ† Ø£Ø¹Ø¶Ø§Ø¡ Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³. Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©"
        
        response = "Ø§Ø³ÙƒØ§Ù† Ø£Ø¹Ø¶Ø§Ø¡ Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³\n\n"
        
        # Display direct children of Faculty Housing
        for child in faculty_housing.get('children', []):
            title = child.get('title', '')
            url = child.get('url', '')
            
            response += f"{title}\n"
            
            if url:
                response += f"({url})[{title}ÙˆØµÙˆÙ„]\n"
            
            # If this child has its own children (like "Related Links"), display them
            if child.get('children'):
                response += self.display_child_links(child)
            
            response += "\n"
        
        return response

    # This additional function helps navigating Student housing sections that I thought were important
    def handle_student_housing(self, housing_section):
        """Handle student housing navigation"""
        student_housing = None
        for child in housing_section.get('children', []):
            if child.get('title', '') == 'Ø§Ø³ÙƒØ§Ù† Ø§Ù„Ø·Ù„Ø§Ø¨':
                student_housing = child
                break
        
        if not student_housing:
            available_children = [child.get('title', 'No title') for child in housing_section.get('children', [])]
            return f"{', '.join(available_children)}: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø³ÙƒÙ† Ø·Ù„Ø§Ø¨ÙŠ. Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©"
        
        response = "Ø§Ø³ÙƒØ§Ù† Ø§Ù„Ø·Ù„Ø§Ø¨\n\n"
        
        for child in student_housing.get('children', []):
            title = child.get('title', '')
            url = child.get('url', '')
            content = child.get('content', '')
            
            response += f"{title}\n"
            
            # Show content if available (like the procedural guide)
            if content:
                # Format the content nicely
                formatted_content = self.format_housing_content(content)
                response += f"{formatted_content}\n"
            
            if url:
                response += f"({url})[{title}ÙˆØµÙˆÙ„]\n"
            
            # Show child links if any (like for Female Student Housing)
            if child.get('children'):
                response += self.display_child_links(child)
            
            response += "\n"
        
        return response

    # Gets child links 
    def display_child_links(self, parent_section):
        """Display children of a section (like Related Links)"""
        if not parent_section.get('children'):
            return ""
        
        response = f"\n:Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©\n"
        
        for child in parent_section.get('children', []):
            child_title = child.get('title', '')
            child_url = child.get('url', '')
            
            if child_title:
                response += f"**{child_title}** â€¢"
                if child_url:
                    response += f"({child_url})[Ø§Ù„ÙˆØµÙˆÙ„ Ù‡Ù†Ø§]-"
                response += "\n"
        
        return response

    # Editing the housing information content for better readability
    def format_housing_content(self, content):
        """Format housing content for better readability"""
        if not content:
            return ""
        
        # Split into lines and format
        lines = content.split('\n')
        formatted_lines = []
        
        for line in lines:
            line = line.strip()
            if line:
                # Make steps and important headings bold
                if any(keyword in line.lower() for keyword in ['Ø§Ù„Ø®Ø·ÙˆØ©','Ø§Ù„Ø´Ø±Ø·','ÙƒÙŠÙÙŠØ© Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…']):
                    formatted_lines.append(f"**{line}**")
                else:
                    formatted_lines.append(line)
        
        return '\n'.join(formatted_lines)
    
    # This is to handle library sections, their libraries and content of the libraries along with Location links
    def handle_library(self, entities, user_input):
        user_input_lower = user_input.lower()

        def contains_title(item, text):
            return item.get("title", "").lower() in text

        def find_matching_node(data, text):
            for node in data:
                if contains_title(node, text):
                    return node
            return None

        def find_matching_child(parent, text):
            for child in parent.get("children", []):
                if contains_title(child, text):
                    return child
            return None

        def format_contact_info(section):
            lines = []
            for table in section.get("tables", []):
                for row in table.get("rows", []):
                    lines.append(" | ".join(row))
            return "\n".join(lines)

        # STEP 1: Start from top-level "Libraries"
        libraries_root = find_matching_node(self.data, "Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª")
        if not libraries_root:
            return "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ø³Ù… 'Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª' ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."

        # STEP 2: If user specifies a category (e.g., Shared libraries)
        selected_category = find_matching_child(libraries_root, user_input_lower)
        if selected_category:
            # STEP 2a: If user already specified a library
            matched_library = find_matching_child(selected_category, user_input_lower)
            if matched_library:
                info, contact, location = "", "", ""

                for section in matched_library.get("children", []):
                    title = section.get("title", "").lower()
                    if "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª" in title:
                        info = section.get("content", "").strip()
                    elif "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„" in title:
                        contact = format_contact_info(section)
                    elif "Ø§Ù„Ù…ÙˆÙ‚Ø¹" in title:
                        location = section.get("url", "")

                response = f"{matched_library['title']}\n\n"
                if info:
                    response += f"\n{info}:Ù…Ø¹Ù„ÙˆÙ…Ø§Øª\n\n"
                if contact:
                    response += f"\n{contact}:Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„\n\n"
                if location:
                    response += f"{location}: Ù…ÙˆÙ‚Ø¹"
                return response.strip()

            # STEP 2b: User just selected the category, list children
            library_titles = [child["title"] for child in selected_category.get("children", [])]
            return f"\n".join(f"- {title}" for title in library_titles) + "**{selected_category['title']}**: ÙˆÙ‡Ù†Ø§ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ØªØ­Øª\n"

        # STEP 3: User mentioned a library directly without saying category
        for category in libraries_root.get("children", []):
            matched_library = find_matching_child(category, user_input_lower)
            if matched_library:
                info, contact, location = "", "", ""
                for section in matched_library.get("children", []):
                    title = section.get("title", "").lower()
                    if "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª" in title:
                        info = section.get("content", "").strip()
                    elif "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„" in title:
                        contact = format_contact_info(section)
                    elif "Ø§Ù„Ù…ÙˆÙ‚Ø¹" in title:
                        location = section.get("url", "")

                response = f"{matched_library['title']}\n\n"
                if info:
                    response += f"\n{info}:Ù…Ø¹Ù„ÙˆÙ…Ø§Øª\n\n"
                if contact:
                    response += f"\n{contact}:Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„\n\n"
                if location:
                    response += f"{location}: Ù…ÙˆÙ‚Ø¹"
                return response.strip()

        # STEP 4: User only said "libraries" â†’ ask to choose category
        category_titles = [cat["title"] for cat in libraries_root.get("children", [])]
        return "\n".join(f"- {title}" for title in category_titles) + "Ù‡Ù„ ØªØ±ØºØ¨ ÙÙŠ Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø¹Ù† Ø¥Ø­Ø¯Ù‰ ÙØ¦Ø§Øª Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©ØŸ\n" 
    
    # This function is to scrape grading system scale and add the PDF
    def handle_grading_system(self):
        """Handle grading system with semantic search"""
        query = "Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚Ø¯ÙŠØ± Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ±Ø§ÙƒÙ…ÙŠ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªÙ‚Ø¯ÙŠØ± Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ù‚Ø±Ø±Ø§Øª"
        relevant_content = self.find_relevant_content_semantic(query, top_k=5)
        response = ":Ù†Ø¸Ø§Ù… Ø§Ù„Ø¯Ø±Ø¬Ø§Øª\n\n"
        
        for content in relevant_content[:3]:
            content_text = content['content']
            response += content_text + "\n\n"
            
            # Extract links from content text
            links = self.extract_links_from_content(content_text)
            if links:
                response += f"({links[0]})[PDF Ø³ÙŠØ§Ø³Ø© Ø§Ù„ØªØµÙ†ÙŠÙ]\n\n"
            
            # Check metadata for additional fields like 'info'
            metadata = content.get('metadata', {})
            if 'info' in metadata and metadata['info']:
                response += f"({metadata['info']})[PDF Ù„Ø§Ø¦Ø­Ø© Ø§Ù„Ø¯Ø±Ø§Ø³Ø© ÙˆØ§Ù„Ø§Ù…ØªØ­Ø§Ù†Ø§Øª]\n\n"
            elif 'url' in metadata and metadata['url']:
                response += f"({metadata['url']})[Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©]\n\n"
        
        if not relevant_content:
            response += "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§ Ø§Ù„Ø­Ø§Ù„ÙŠØ©. ÙŠÙØ±Ø¬Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø£Ùˆ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ø´Ø¤ÙˆÙ† Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©.\n\n"
        
        return response
    
    # This function is to handle the plagiarism with the PDF
    def handle_plagiarism(self):
        """Handle plagiarism queries with semantic search"""
        query = (
            "Ø§Ù„Ø³Ø±Ù‚Ø© Ø§Ù„Ø£Ø¯Ø¨ÙŠØ© Ø§Ù„Ù†Ø²Ø§Ù‡Ø© Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© Ø³ÙŠØ§Ø³Ø© Ø§Ù„ØºØ´ Ø§Ù„Ù…Ø®Ø§Ù„ÙØ§Øª Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© Ø§Ù„Ø§Ù†ØªØ­Ø§Ù„ "
            "Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³ ØºÙŠØ± Ø§Ù„Ù…ÙˆØ«Ù‚ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ø¨Ø§Øª Ø§Ù…ØªØ­Ø§Ù†Ø§Øª ØºÙŠØ± Ù†Ø²ÙŠÙ‡Ø© "
            "ØªØ²ÙŠÙŠÙ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ø§ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª Ø§Ù„Ù…Ø®Ø§Ù„ÙØ§Øª Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ÙŠØ§Øª Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©"
        )
        relevant_content = self.find_relevant_content_semantic(query, top_k=5)
        
        response = "Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ù†Ø²Ø§Ù‡Ø© Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© ÙˆØ§Ù„Ø§Ù†ØªØ­Ø§Ù„\n\n"
        
        for content in relevant_content[:3]:
            response += content['content'] + "\n\n"
            links = self.extract_links_from_content(content['content'])
            if links:
                response += f"({links[0]})[Ø§Ù„Ø³ÙŠØ§Ø³Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©]\n\n"
        
        if not relevant_content:
            response += "Ù„Ø§ ØªØªÙˆÙØ± Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­ÙˆÙ„ Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø§Ù†ØªØ­Ø§Ù„ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§ Ø§Ù„Ø­Ø§Ù„ÙŠØ©. ÙŠÙØ±Ø¬Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø£Ùˆ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ø´Ø¤ÙˆÙ† Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©.\n\n"
        
        return response
    
    def handle_faq(self, entities, user_input):
        """Handle user queries that fall under the 'faq' intent."""
        faqs_section = next((item for item in self.data if item.get('title') == 'Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©'), None)
        if not faqs_section:
            return " Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ø£Ø³Ø¦Ù„Ø© Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ."

        faqs = faqs_section.get('children', [])
        if not faqs:
            return "Ù„Ø§ ØªØªÙˆÙØ± Ø¥Ø¯Ø®Ø§Ù„Ø§Øª Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©."

        # If SentenceTransformer is available, use semantic search
        if self.sentence_model:
            faq_titles = [faq.get('title', '') for faq in faqs if faq.get('title')]
            if not faq_titles:
                return "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ."

            user_emb = self.sentence_model.encode([user_input], convert_to_tensor=True)
            faq_embs = self.sentence_model.encode(faq_titles, convert_to_tensor=True)
            
            import torch
            cos_scores = torch.nn.functional.cosine_similarity(user_emb, faq_embs)
            if len(cos_scores) == 0:
                return " Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù…Ø¹ Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©."

            best_idx = cos_scores.argmax().item()
            best_score = cos_scores[best_idx].item()

            # Threshold for confident match
            if best_score > 0.65:
                return faqs[best_idx].get('answer', "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…ØªØ§Ø­Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø´Ø§Ø¦Ø¹.")
            else:
                return "ÙˆØ¬Ø¯ØªÙ Ø¨Ø¹Ø¶ Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©ØŒ Ù„ÙƒÙ†Ù†ÙŠ Ù„Ø³ØªÙ Ù…ØªØ£ÙƒØ¯Ù‹Ø§ Ø£ÙŠÙ‘Ù‡Ø§ ØªÙ‚ØµØ¯. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ÙƒØŸ"

        # Fallback to simple keyword match
        for faq in faqs:
            title = faq.get('title', '')
            if any(word.lower() in user_input.lower() for word in title.split()):
                return faq.get('answer', "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…ØªØ§Ø­Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø´Ø§Ø¦Ø¹.")

        return "ÙˆØ¬Ø¯ØªÙ Ø¨Ø¹Ø¶ Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©ØŒ Ù„ÙƒÙ†Ù†ÙŠ Ù„Ø³ØªÙ Ù…ØªØ£ÙƒØ¯Ù‹Ø§ Ø£ÙŠÙ‘Ù‡Ø§ ØªÙ‚ØµØ¯. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ÙƒØŸ"

    # This function is to handle the attendance rules with the PDF
    def handle_attendance(self):
        """Handle attendance queries"""
        query = (
            "Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø­Ø¶ÙˆØ± Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø­Ø¶ÙˆØ± Ø­Ø¶ÙˆØ± Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØºÙŠØ§Ø¨ Ø§Ù„ØªØ£Ø®ÙŠØ± Ø§Ù„Ø§Ù†ØµØ±Ø§Ù "
            "Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø­Ø¶ÙˆØ± Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø­Ø¶ÙˆØ±ÙŠ Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø·Ù„Ø§Ø¨ "
            "ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø­Ø¶ÙˆØ± Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø­Ø¶ÙˆØ± Ù†Ø¸Ø§Ù… Ø§Ù„Ø­Ø¶ÙˆØ± ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø­Ø¶ÙˆØ±"
        )
        relevant_content = self.find_relevant_content_semantic(query, top_k=3)
        
        response = "Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø­Ø¶ÙˆØ±\n\n"
        
        if relevant_content:
            for content in relevant_content:
                response += content['content'] + "\n\n"
                links = self.extract_links_from_content(content['content'])
                if links:
                    response += f"({links[0]})[Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø­Ø¶ÙˆØ±]\n\n"
        else:
            response += "ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ ÙˆØ«ÙŠÙ‚Ø© Ù†Ø¸Ø§Ù… Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø­Ø¶ÙˆØ± Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©.\n\n"
        
        # Always check for grading system info field as it contains attendance details
        grading_query = (
            "Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ±Ø§ÙƒÙ…ÙŠ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… "
            "ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø·Ù„Ø§Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ù…ØªØ­Ø§Ù†Ø§Øª Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ… "
            "Ø§Ù„ØªÙ‚Ø¯ÙŠØ±Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ø§Ù„ÙØµÙ„ÙŠØ© Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø·Ù„Ø§Ø¨ "
            "ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ÙƒÙØ§Ø¡Ø© Ø§Ù„ØªØ­ØµÙŠÙ„ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ"
        )
        grading_content = self.find_relevant_content_semantic(grading_query, top_k=5)
        
        for content in grading_content:
            metadata = content.get('metadata', {})
            # Check if this is grading system content and has info field
            if 'info' in metadata and metadata['info']:
                response += f"ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ù„ÙˆØ§Ø¦Ø­ Ø§Ù„Ø¯Ø±Ø§Ø³Ø© ÙˆØ§Ù„Ø§Ù…ØªØ­Ø§Ù†Ø§Øª Ù„Ù…Ø¹Ø±ÙØ© Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø­Ø¶ÙˆØ± Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©:\n"
                response += f"({metadata['info']})[:Ø§Ù„Ø­Ø¶ÙˆØ± Ù…ÙˆØ¬ÙˆØ¯ Ù‡Ù†Ø§]\n\n"
                break
        
        return response
    
    # This is to scrape the research labs links
    def handle_research_labs(self):
        """Handle research labs and facilities - find Research node with Labs child"""
        
        response = "Ù…Ø®ØªØ¨Ø±Ø§Øª ÙˆÙ…Ø±Ø§ÙÙ‚ Ø§Ù„Ø£Ø¨Ø­Ø§Ø«\n\n"
        
        def find_research_with_labs(data):
            """Recursively find Research node that has Labs as a child"""
            if isinstance(data, dict):
                # Check if this is a Research node
                if data.get('title') == 'Ø¨Ø­Ø«':
                    # Check if it has children
                    if 'children' in data:
                        for child in data['children']:
                            if isinstance(child, dict) and child.get('title') == 'Ø§Ù„Ù…Ø®ØªØ¨Ø±Ø§Øª':
                                # Found Research->Labs! Return the Labs children
                                return child.get('children', [])
                
                # Recursively search in children
                if 'children' in data:
                    for child in data['children']:
                        result = find_research_with_labs(child)
                        if result:
                            return result
            
            elif isinstance(data, list):
                # If data is a list, search each item
                for item in data:
                    result = find_research_with_labs(item)
                    if result:
                        return result
            
            return None
        
        try:
            labs_children = find_research_with_labs(self.data)  # You'll need to adjust this
            
            if labs_children:
                response += ":ÙˆÙ‡Ù†Ø§ Ù…Ø®ØªØ¨Ø±Ø§Øª ÙˆÙ…Ø±Ø§ÙÙ‚ Ø§Ù„Ø£Ø¨Ø­Ø§Ø«\n\n"
                for i, lab in enumerate(labs_children, 1):
                    title = lab.get('title', 'Ù…Ø®ØªØ¨Ø± ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')
                    url = lab.get('url', '')
                    
                    # Normalize URL
                    if url and url.startswith('/'):
                        url = f"https://ksu.edu.sa{url}"
                    
                    response += f"{i}. **{title}**\n"
                    if url:
                        response += f"   {url}\n"
                    response += "\n"
            else:
                response += "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¹Ù‚Ø¯Ø© Ø§Ù„Ø¨Ø­Ø« Ù…Ø¹ Ù…Ø®ØªØ¨Ø±Ø§Øª Ø§Ù„Ø·ÙÙ„.\n\n"
                
        except AttributeError:
            # Fallback if we don't have direct access to JSON data
            response += "ØªØ¹Ø°Ø± Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª JSON Ù…Ø¨Ø§Ø´Ø±Ø©Ù‹. ÙŠÙØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªÙˆÙØ± Ø¨ÙŠØ§Ù†Ø§Øª JSON.\n\n"
        
        return response

    # This function adds all the IT helpdesk for student and staff and paths inside. 
    def handle_it_support(self, entities, user_input):
        from difflib import get_close_matches

        # Find IT Helpdesk node
        it_helpdesk = next((item for item in self.data if item.get("title", "").lower() == "it helpdesk"), None)
        if not it_helpdesk:
            return "Sorry, I couldn't find the IT Helpdesk information."

        user_input_lower = user_input.lower()
        user_type = None

        if "student" in user_input_lower:
            user_type = "Student"
        elif "staff" in user_input_lower or "faculty" in user_input_lower or "professor" in user_input_lower:
            user_type = "Staff"

        if not user_type:
            return "â“ Are you a student or staff/faculty? Please specify so I can assist you."

        # Navigate to the relevant section
        section = next((child for child in it_helpdesk.get("children", []) if child.get("title", "").lower() == user_type.lower()), None)
        if not section:
            return f"Sorry, I couldnâ€™t find IT support info for {user_type}."

        # Extract all issues and sub-issues
        def extract_issues_with_hierarchy(node, parent_title=None):
            results = []
            title = node.get("title")
            if title:
                full_title = f"{parent_title} â†’ {title}" if parent_title else title
                results.append((full_title, title))
            for child in node.get("children", []):
                results.extend(extract_issues_with_hierarchy(child, title))
            return results

        all_issues = extract_issues_with_hierarchy(section)
        plain_titles = [t[1].lower() for t in all_issues]

        # Try to match the user input to a known issue
        matched = get_close_matches(user_input.lower(), plain_titles, n=1, cutoff=0.4)

        if matched:
            matched_title = next(full for full, plain in all_issues if plain.lower() == matched[0])
            ksu_code = "KSU1" if user_type == "Staff" else "KSU2"
            return (
                f"ğŸ› ï¸ It looks like you're facing: **{matched_title}**.\n"
                f"Please visit the [IT Helpdesk]({it_helpdesk['url']}), select **{ksu_code}**, and click **'Report an Issue'**."
            )

        # If no match found, show all options (parents and their children)
        options_text = f"I couldn't find an exact match. Here are support topics for {user_type}:\n\n"
        grouped = {}
        for full_title, child_title in all_issues:
            parent = full_title.split("â†’")[0].strip()
            grouped.setdefault(parent, []).append(child_title)

        for parent, children in grouped.items():
            options_text += f"ğŸ”¹ **{parent}**\n"
            for c in children:
                options_text += f"â€ƒâ€ƒâ€¢ {c}\n"

        options_text += "\nğŸ’¬ Please choose one of the topics above or rephrase your issue."

        return options_text
    
    #---------------- THESE THINGS HAVE NOT BEEN SCRAPED YET ---------------------------------------#
    # def handle_fees_tuition(self, user_input):
        """Handle tuition and fees queries"""
        query = "tuition fees cost payment charges"
        relevant_content = self.find_relevant_content_semantic(query, top_k=5)
        
        response = "ğŸ’° **Tuition & Fees Information**\n\n"
        
        for content in relevant_content[:3]:
            response += content['content'] + "\n\n"
            links = self.extract_links_from_content(content['content'])
            if links:
                response += f"ğŸ”— [Fee structure]({links[0]})\n\n"
        
        if not relevant_content:
            response += ("Tuition and fee information is not available in our current database. "
                        "Please contact the finance office or check the student portal for current rates.\n\n")
        
        return response
    
    # def handle_scholarships(self, user_input):
        """Handle scholarship and financial aid queries"""
        query = "scholarship financial aid funding grants assistance"
        relevant_content = self.find_relevant_content_semantic(query, top_k=5)
        
        response = "ğŸ“ **Scholarships & Financial Aid**\n\n"
        
        for content in relevant_content[:3]:
            response += content['content'] + "\n\n"
            links = self.extract_links_from_content(content['content'])
            if links:
                response += f"ğŸ”— [Scholarship portal]({links[0]})\n\n"
        
        if not relevant_content:
            response += ("Scholarship information is not available in our current database. "
                        "Please contact the financial aid office for information about available scholarships and grants.\n\n")
        
        return response
    
    #-----------------------------------------------------------------------------------------#
    
    # General queries
    def handle_general_query_enhanced(self, user_input):
        """Enhanced general query handling with better semantic understanding"""
        # First, try to find relevant content
        relevant_content = self.find_relevant_content_semantic(user_input, top_k=5, threshold=0.2)
        
        if relevant_content:
            response = "ğŸ’¡ **:Ù‡Ø°Ø§ Ù…Ø§ ÙˆØ¬Ø¯ØªÙ‡ Ù„Ùƒ**\n\n"
            
            for i, content in enumerate(relevant_content[:3], 1):
                text = content['content']
                
                # Intelligent summarization
                if len(text) > 250:
                    sentences = text.split('.')
                    summary = sentences[0]
                    if len(summary) < 200 and len(sentences) > 1:
                        summary += '. ' + sentences[1]
                    response += f"**{i}.** {summary}...\n\n"
                else:
                    response += f"**{i}.** {text}\n\n"
                
                # Add links
                links = self.extract_links_from_content(content['content'])
                if links:
                    response += f"({links[0]})[Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª]\n\n"
            
            # Add contextual follow-up suggestions
            response += ":â“ **Ù‡Ù„ ØªØ±ÙŠØ¯ Ø£Ù† ØªØ¹Ø±Ù Ø§Ù„Ù…Ø²ÙŠØ¯ØŸ** ÙŠÙ…ÙƒÙ†Ùƒ Ø£Ù† ØªØ³Ø£Ù„Ù†ÙŠ Ø¹Ù†\n"
            response += "Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ ÙˆØ¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ‚Ø¯ÙŠÙ… â€¢\n"
            response += "Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© ÙˆÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¯ÙˆØ±Ø©â€¢\n"
            response += "Ù…Ø±Ø§ÙÙ‚ Ø§Ù„Ø­Ø±Ù… Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠ ÙˆØ§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø·Ù„Ø§Ø¨ÙŠØ© â€¢\n"
            response += "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ Ù„Ù„Ø£Ù‚Ø³Ø§Ù… â€¢\n"
            
        else:
            response = ("ğŸ¤” Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§. "
                        "ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ:\n\n"
                        "ğŸ“ **Ø§Ù„Ø£Ù…ÙˆØ± Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©:** Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ØŒ Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø§ØªØŒ Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ\n"
                        "ğŸ  **Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠØ©:** Ø§Ù„Ø³ÙƒÙ†ØŒ Ø§Ù„Ù…ÙƒØªØ¨Ø§ØªØŒ Ø§Ù„Ù…Ø·Ø§Ø¹Ù…ØŒ Ø§Ù„ØªØ±ÙÙŠÙ‡\n"
                        "ğŸ’¼ **Ø§Ù„Ø®Ø¯Ù…Ø§Øª:** Ø¯Ø¹Ù… ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªØŒ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ©ØŒ Ø®Ø¯Ù…Ø§Øª Ø§Ù„ØªÙˆØ¸ÙŠÙ\n"
                        "ğŸ“ **Ù„Ù„ØªÙˆØ§ØµÙ„:** Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù‚Ø³Ù…ØŒ Ø¯Ù„ÙŠÙ„ Ø£Ø¹Ø¶Ø§Ø¡ Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³\n"
                        "ğŸ“‹ **Ø§Ù„Ø³ÙŠØ§Ø³Ø§Øª:** Ø§Ù„ØªÙ‚ÙŠÙŠÙ…ØŒ Ø§Ù„Ø­Ø¶ÙˆØ±ØŒ Ø§Ù„Ù†Ø²Ø§Ù‡Ø© Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©\n"
                        "ğŸ”¬ **Ø§Ù„Ø¨Ø­Ø«:** Ø§Ù„Ù…Ø®ØªØ¨Ø±Ø§ØªØŒ Ø§Ù„Ù…Ø±Ø§ÙÙ‚ØŒ Ø§Ù„ÙØ±Øµ\n\n"
                        "Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„Ùƒ Ø£Ùˆ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ù† Ø£Ø­Ø¯ Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ØŸ")
        return response
    
    # This is all the chats handler functions inside
    def chat(self, user_input):
        """Enhanced main chat function with better semantic understanding"""
        # Identify intent using semantic similarity
        intent, confidence = self.identify_intent_semantic(user_input)
        entities = self.extract_entities_enhanced(user_input)
        
        # Update user context
        self.user_context['last_intent'] = intent
        self.user_context['entities'].update(entities)
        
        # Route to appropriate handler based on intent
        if intent == 'Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ ÙÙŠ Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„Ùƒ Ø³Ø¹ÙˆØ¯':
            return self.handle_admission_requirements(entities, user_input)
        elif intent == 'Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ':
            return self.handle_academic_calendar()
        elif intent == 'Ø§Ù„ÙƒÙ„ÙŠØ§Øª':
            return self.handle_degree_programs(entities, user_input)
        #elif intent == 'faculty':
        #    return self.handle_faculty_directory(entities, user_input)
        elif intent == 'Ø§Ø³ÙƒØ§Ù† Ø¬Ø§Ù…Ø¹Ø©':
            return self.handle_housing(entities, user_input)
        elif intent == 'Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª':
            return self.handle_library(entities, user_input)
        elif intent == 'Ù†Ø¸Ø§Ù… Ø§Ù„Ø¯Ø±Ø¬Ø§Øª':
            return self.handle_grading_system()
        elif intent == 'Ø§Ù„Ø§Ù†ØªØ­Ø§Ù„ Ø§Ù„Ø¹Ù„Ù…ÙŠ':
            return self.handle_plagiarism()
        elif intent == 'attendance':
            return self.handle_attendance()
        elif intent == 'Ø¨Ø­Ø«':
            return self.handle_research_labs()
        elif intent == 'it_support':
            return self.handle_it_support(entities, user_input)
        elif intent == 'contact_info':
            return self.handle_contact_info(entities, user_input)
        elif intent == 'Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©':
            return self.handle_faq(entities, user_input)
        # elif intent == 'fees_tuition':
            return self.handle_fees_tuition(user_input)
        # elif intent == 'scholarships':
            return self.handle_scholarships(user_input)
        else:
            return self.handle_general_query_enhanced(user_input)
    
    # Interactive chats
    def run_interactive_chat(self):
        """Run interactive chat session with enhanced experience"""
        print("ğŸ“ Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹! ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø£Ù…ÙˆØ± Ø§Ù„Ù‚Ø¨ÙˆÙ„ØŒ ÙˆØ§Ù„Ø¯Ø±Ø§Ø³Ø§Øª Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©ØŒ ÙˆØ§Ù„Ù…ÙƒØªØ¨Ø§ØªØŒ ÙˆØ§Ù„Ø³ÙƒÙ†ØŒ ÙˆØ£Ø¹Ø¶Ø§Ø¡ Ù‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ³ØŒ ÙˆØ§Ù„Ø±Ø³ÙˆÙ…ØŒ ÙˆØ§Ù„Ø¨Ø­Ø«ØŒ ÙˆØºÙŠØ±Ù‡Ø§. Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø£Ùˆ 'Ø³Ø§Ø¹Ø¯Ù†ÙŠ' Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù…Ø«Ù„Ø©.")
        print("ğŸ’¡ Ù†ØµÙŠØ­Ø©: Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø·Ø¨ÙŠØ¹ÙŠØ© Ù…Ø«Ù„ 'ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„ØªÙ‚Ø¯Ù… Ø¨Ø·Ù„Ø¨ Ø§Ù„Ù‚Ø¨ÙˆÙ„ ÙÙŠ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠØ©ØŸ' Ø£Ùˆ 'Ù…Ø§ Ù‡ÙŠ Ø³Ø§Ø¹Ø§Øª Ø¹Ù…Ù„ Ø§Ù„Ù…ÙƒØªØ¨Ø©ØŸ'")
        print("Ø§ÙƒØªØ¨ 'Ø³Ø§Ø¹Ø¯Ù†ÙŠ' Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù…Ø«Ù„Ø©ØŒ Ø£Ùˆ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø®Ø±ÙˆØ¬.\n")
        
        conversation_count = 0
        
        while True:
            try:
                user_input = input("Ø£Ù†ØªğŸ™‹ :").strip()

                if user_input.lower() in ['hi', 'hello', 'hey', 'good morning', 'good afternoon']:
                    print("ğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: Ù…Ø±Ø­Ø¨Ù‹Ø§! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ ğŸ˜Š\n")
                    continue

                if user_input.lower() in ['exit', 'quit', 'bye', 'goodbye']:
                    print("ğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: Ø´ÙƒØ±Ù‹Ø§ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ùƒ Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø°ÙƒÙŠ! Ù†ØªÙ…Ù†Ù‰ Ù„Ùƒ ÙŠÙˆÙ…Ù‹Ø§ Ø±Ø§Ø¦Ø¹Ù‹Ø§! ğŸ‘‹")
                    break

                if user_input.lower() == 'help':
                    print("ğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: Ø¥Ù„ÙŠÙƒ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ†Ùƒ Ø·Ø±Ø­Ù‡Ø§:")
                    print("â€¢ 'Ù…Ø§ Ù‡ÙŠ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø¨ÙƒØ§Ù„ÙˆØ±ÙŠÙˆØ³ØŸ'")
                    print("â€¢ 'Ø£Ø±Ù†ÙŠ Ø§Ù„ØªÙ‚ÙˆÙŠÙ… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ'")
                    print("â€¢ 'Ø£Ø­ØªØ§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ù‚Ø³Ù… Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³Ø¨'")
                    print("â€¢ 'Ø£ÙŠÙ† Ø£Ø¬Ø¯ Ø³ÙƒÙ† Ø§Ù„Ø·Ù„Ø§Ø¨ØŸ'")
                    print("â€¢ 'Ù…Ø§ Ù‡Ùˆ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ…ØŸ'")
                    print("â€¢ 'Ø£ÙˆØ§Ø¬Ù‡ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„'")
                    print("â€¢ 'Ø£Ø®Ø¨Ø±Ù†ÙŠ Ø¹Ù† ÙØ±Øµ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ù„Ù…ÙŠ'")
                    print("â€¢ 'ÙƒÙ… ØªØ¨Ù„Øº ØªÙƒÙ„ÙØ© Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ©ØŸ'")
                    print("â€¢ 'Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ù†Ø­ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©ØŸ'\n")
                    continue

                if not user_input:
                    print("ğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ! ÙŠØ±Ø¬Ù‰ Ø·Ø±Ø­ Ø£ÙŠ Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©. ğŸ˜Š\n")
                    continue
                
                print("ğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: ", end="")
                response = self.chat(user_input)
                print(f"{response}\n")
                
                conversation_count += 1
                
                # Provide helpful suggestions every few interactions
                if conversation_count % 5 == 0:
                    print("ğŸ’¡ **Ù†ØµÙŠØ­Ø© Ø³Ø±ÙŠØ¹Ø©:** ÙŠÙ…ÙƒÙ†Ùƒ Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ§Ø¨Ø¹Ø© Ø£Ùˆ Ø·Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£ÙƒØ«Ø± ØªØ­Ø¯ÙŠØ¯Ù‹Ø§ ÙÙŠ Ø£ÙŠ ÙˆÙ‚Øª!\n")
                
            except KeyboardInterrupt:
                print("\nğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©! Ø´ÙƒØ±Ù‹Ø§ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ùƒ Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠ! ğŸ‘‹")
                break
            except Exception as e:
                print(f"ğŸ¤– Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: ÙˆØ§Ø¬Ù‡ØªÙ Ø®Ø·Ø£Ù‹ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ. ÙŠÙØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø¬Ø¯Ø¯Ù‹Ø§! ğŸ”§")
                print(f"(Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙÙ†ÙŠØ©: {str(e)})\n")

# Main function to run all the functions
def main():
    """Main function to run the enhanced chatbot"""
    # Initialize chatbot with your JSON file
    json_file_path = "C:\\Nawal\\IT Department\\Practical Training\\Final Chatbot\\university_data_ar.json"  # Replace with your actual JSON file path
    
    print("ğŸš€ Starting University AI Assistant...")
    
    try:
        #chatbot = UniversityChatbot(json_file_path)
        chatbot = asyncio.run(initialize_chatbot(json_file_path))
        chatbot.run_interactive_chat()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Goodbye!")
    except Exception as e:
        print(f"\nâŒ Error initializing chatbot: {str(e)}")
        print("\nSetup Requirements:")
        print("1. Install required packages:")
        print("   pip install sentence-transformers transformers torch nltk spacy scikit-learn")
        print("2. Download spaCy model:")
        print("   python -m spacy download en_core_web_sm")
        print("3. Ensure your JSON file path is correct")
        print("4. Make sure you have sufficient disk space for model downloads")
        print("\nğŸ’¡ Note: The chatbot will work with fallback methods if some models fail to load.")

if __name__ == "__main__":
    main()