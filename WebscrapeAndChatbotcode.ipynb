{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece2587f",
   "metadata": {},
   "source": [
    "# Webscraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb59ee",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dec86b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c61f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter notebook - use: await main()\n",
      "ğŸš€ Starting KSU website scraping...\n",
      "ğŸ“ Output directory: data_backups\n",
      "ğŸ“„ Output file: hierarchy.json\n",
      "ğŸ“‹ Scraping main menu...\n",
      "ğŸ“˜ Scraping major sections concurrently...\n",
      "ğŸ“˜ Scraping Regulations and Policies section...\n",
      "ğŸ“˜ Scraping Admission Requirements section...\n",
      "[âœ…] Found Related Links URL: https://housing.ksu.edu.sa/en/node/83\n",
      "ğŸ”— Visiting FAQ page: https://ksu.edu.sa/en/faqs\n",
      "âœ… Added Regulations and Policies section\n",
      "âœ… Added Admission Requirements section\n",
      "âœ… Added FAQs section\n",
      "âœ… Added Research section\n",
      "âœ… Added Libraries section\n",
      "âœ… Added Academic Calendar section\n",
      "âœ… Added Housing section\n",
      "ğŸ’» Processing IT Helpdesk data...\n",
      "âœ… Added IT Helpdesk section\n",
      "ğŸ« Processing colleges data...\n",
      "ğŸ“š Processing category: Science Colleges\n",
      "ğŸ” Switching to English version: https://ccis.ksu.edu.sa/en\n",
      "ğŸ” Special handling for College of Sciences\n",
      "ğŸ“ Found department: Development of Department\n",
      "ğŸ‘¥ Found faculty/staff link: https://signboards.ksu.edu.sa/Employees/Default.aspx\n",
      "ğŸ“ Found department: Department\n",
      "ğŸ‘¥ Found faculty/staff link: https://signboards.ksu.edu.sa/Employees/Default.aspx\n",
      "ğŸ“ Found department: Statistics & Operations Research Department\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/13349\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/14196\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/13427\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/13357\n",
      "ğŸ“ Found department: Physics & Astronomy Department\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/8040\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/4078\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/13270\n",
      "ğŸ“ Found department: Geology & Geophysics Department\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/10037\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/10034\n",
      "ğŸ“ Found department: Mathematics Department\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/10985\n",
      "ğŸ“ Found department: Chemistry Department\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/4270\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/4270\n",
      "ğŸ“ Found department: Biochemistry Department\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/Biochem\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/Biochem\n",
      "ğŸ“ Found department: Botany & Microbiology Department\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/5863\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/6118\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/6106\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/14106\n",
      "ğŸ“ Found department: Zoology Department\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/1328\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/13942\n",
      "ğŸ‘¥ Found faculty/staff link: https://sciences.ksu.edu.sa/en/node/14224\n",
      "  âœ… Processed college: College of Business Administration\n",
      "  âœ… Processed college: College of Architecture and Planning\n",
      "  âœ… Processed college: College of Computer and Information Sciences\n",
      "  âœ… Processed college: College of Food and Agricultural Sciences\n",
      "  âœ… Processed college: College of Sciences\n",
      "  âœ… Processed college: College of Engineering\n",
      "ğŸ“š Processing category: College of Applied Studies\n",
      "  âœ… Processed college: College of Applied Studies\n",
      "ğŸ“š Processing category: Health Colleges\n",
      "  âœ… Processed college: Prince Sultan Bin Abdulaziz College for Emergency Medical Services\n",
      "  âœ… Processed college: College of Medicine\n",
      "  âœ… Processed college: College of Dentistry\n",
      "  âœ… Processed college: College of Pharmacy\n",
      "  âœ… Processed college: College of Applied Medical Sciences\n",
      "  âœ… Processed college: College of Nursing\n",
      "ğŸ“š Processing category: Humanities Colleges\n",
      "Error fetching https://education.ksu.edu.sa/en/node/1984: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1984')\n",
      "Error fetching https://education.ksu.edu.sa/en/node/1985: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1985')\n",
      "Skipping https://education.ksu.edu.sa/en/node/1985 due to error: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1985')\n",
      "Error fetching https://education.ksu.edu.sa/en/node/1987: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1987')\n",
      "Error fetching https://education.ksu.edu.sa/en/node/1987: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1987')\n",
      "Skipping https://education.ksu.edu.sa/en/node/1987 due to error: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1987')\n",
      "Error fetching https://education.ksu.edu.sa/en/node/1988: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1988')\n",
      "Error fetching https://education.ksu.edu.sa/en/node/1989: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1989')\n",
      "Skipping https://education.ksu.edu.sa/en/node/1989 due to error: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1989')\n",
      "Error fetching https://education.ksu.edu.sa/en/node/1989: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1989')\n",
      "Error fetching https://education.ksu.edu.sa/en/node/1990: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1990')\n",
      "Error fetching https://education.ksu.edu.sa/en/node/1990: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1990')\n",
      "Skipping https://education.ksu.edu.sa/en/node/1990 due to error: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1990')\n",
      "Error fetching https://education.ksu.edu.sa/en/node/1984: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1984')\n",
      "Error fetching https://education.ksu.edu.sa/en/node/1988: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1988')\n",
      "Skipping https://education.ksu.edu.sa/en/node/1984 due to error: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1984')\n",
      "Skipping https://education.ksu.edu.sa/en/node/1988 due to error: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1988')\n",
      "Error fetching https://education.ksu.edu.sa/en/node/1985: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1985')\n",
      "Error fetching https://education.ksu.edu.sa/en/node/1992: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1992')\n",
      "Error fetching https://education.ksu.edu.sa/en/node/1992: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1992')\n",
      "Skipping https://education.ksu.edu.sa/en/node/1992 due to error: 404, message='Not Found', url=URL('https://education.ksu.edu.sa/en/node/1992')\n",
      "Error fetching https://chss.ksu.edu.sa/en/English: 404, message='Not Found', url=URL('https://chss.ksu.edu.sa/en/English')\n",
      "Skipping https://chss.ksu.edu.sa/en/English due to error: 404, message='Not Found', url=URL('https://chss.ksu.edu.sa/en/English')\n",
      "Error fetching https://chss.ksu.edu.sa/en/English: 404, message='Not Found', url=URL('https://chss.ksu.edu.sa/en/English')\n",
      "  âœ… Processed college: College of Law and Political Sciences\n",
      "  âœ… Processed college: College of Language Sciences\n",
      "  âœ… Processed college: College of Tourism and Archeology\n",
      "  âœ… Processed college: College of Sport Sciences and Physical Activity\n",
      "  âœ… Processed college: College of Education\n",
      "  âœ… Processed college: College of Humanities and Social Sciences\n",
      "  âœ… Processed college: College of Arts\n",
      "ğŸ“š Processing category: Colleges of Muzahimya Branch\n",
      "âš ï¸ Fallback (JS-rendered): using Selenium on https://mz.ksu.edu.sa/ar/node/576\n",
      "âš ï¸ Fallback (JS-rendered): using Selenium on https://cacsm.ksu.edu.sa/en\n",
      "âš ï¸ Fallback (JS-rendered): using Selenium on https://cybsmz.ksu.edu.sa/en\n",
      "âš ï¸ Fallback (JS-rendered): using Selenium on https://appliedengineering.ksu.edu.sa/en\n",
      "ğŸ§² JS dept found: Academic programs â†’ https://cacsm.ksu.edu.sa/en/node/2944\n",
      "ğŸ§² JS dept found: Curriculum Plan for the AcademicÂ Program\" â†’ https://cacsm.ksu.edu.sa/en/node/2976\n",
      "ğŸ§² JS dept found: Curriculum Plan for the AcademicÂ Program â†’ https://cacsm.ksu.edu.sa/en/node/2977\n",
      "ğŸ§² JS dept found: Program Supervisors â†’ https://cacsm.ksu.edu.sa/en/node/2983\n",
      "ğŸ§² JS dept found: Academic Programs â†’ https://appliedengineering.ksu.edu.sa/en/node/2996\n",
      "ğŸ§² JS dept found: Department of Applied Electrical Engineering â†’ https://demo33.ksu.edu.sa/en/node/2998\n",
      "ğŸ§² JS dept found: Department of Applied Mechanical Engineering â†’ https://demo33.ksu.edu.sa/en/node/2994\n",
      "Error fetching https://demo33.ksu.edu.sa/en/node/2998: Cannot connect to host demo33.ksu.edu.sa:443 ssl:default [getaddrinfo failed]\n",
      "âŒ Skipping https://demo33.ksu.edu.sa/en/node/2998 due to error: Cannot connect to host demo33.ksu.edu.sa:443 ssl:default [getaddrinfo failed]\n",
      "Error fetching https://demo33.ksu.edu.sa/en/node/2994: Cannot connect to host demo33.ksu.edu.sa:443 ssl:default [getaddrinfo failed]\n",
      "âŒ Skipping https://demo33.ksu.edu.sa/en/node/2994 due to error: Cannot connect to host demo33.ksu.edu.sa:443 ssl:default [getaddrinfo failed]\n",
      "  âœ… Processed college: College of Applied Engineering\n",
      "  âœ… Processed college: College of Applied Business Administration\n",
      "  âœ… Processed college: College of Applied Computer Science\n",
      "  âœ… Processed college: Deanship of Common Year and Basic Sciences in Al-Muzahmiyya\n",
      "ğŸ’¾ Saving data to JSON file...\n",
      "âœ… Data successfully saved to: data_backups\\hierarchy.json\n",
      "ğŸ“Š File size: 295686 bytes\n",
      "ğŸ”„ Backup created: data_backups\\menu_hierarchy_backup_20250804_190836.json\n",
      "ğŸ‰ Scraping completed successfully!\n",
      "ğŸ“Š Total menu items: 13\n",
      "ğŸ”’ Session closed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OUTPUT_DIR = \"data_backups\"\n",
    "OUTPUT_FILE = \"hierarchy.json\"\n",
    "BASE = \"https://ksu.edu.sa\"\n",
    "UA = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "is_arabic = False\n",
    "\n",
    "# Ensure output directory exists\n",
    "def ensure_output_dir():\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        print(f\"ğŸ“ Created directory: {OUTPUT_DIR}\")\n",
    "\n",
    "def save_to_json(data, filename=None):\n",
    "    \"\"\"Save data to JSON file with proper formatting\"\"\"\n",
    "    ensure_output_dir()\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = OUTPUT_FILE\n",
    "    \n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"âœ… Data successfully saved to: {filepath}\")\n",
    "        print(f\"ğŸ“Š File size: {os.path.getsize(filepath)} bytes\")\n",
    "        \n",
    "        # Optional: Create a backup with timestamp\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        backup_filename = f\"menu_hierarchy_backup_{timestamp}.json\"\n",
    "        backup_filepath = os.path.join(OUTPUT_DIR, backup_filename)\n",
    "        \n",
    "        with open(backup_filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"ğŸ”„ Backup created: {backup_filepath}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving to JSON: {e}\")\n",
    "        raise\n",
    "\n",
    "# Session management for connection pooling\n",
    "async def create_session():\n",
    "    connector = aiohttp.TCPConnector(limit=50, limit_per_host=20)\n",
    "    timeout = aiohttp.ClientTimeout(total=30)\n",
    "    return aiohttp.ClientSession(\n",
    "        connector=connector,\n",
    "        timeout=timeout,\n",
    "        headers=UA\n",
    "    )\n",
    "\n",
    "# get_soup finds the base url and joins other urls based on needs. If the pages are in arabic (ar), the method finds an English option and clicks on that or switches to en.\n",
    "async def get_soup(session, path_or_url):\n",
    "    global is_arabic\n",
    "\n",
    "    url = path_or_url if path_or_url.startswith(\"http\") else urljoin(BASE, path_or_url)\n",
    "    \n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            response.raise_for_status()\n",
    "            text = await response.text()\n",
    "            soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "            if not is_arabic:  # Only check for Arabic and redirect if needed\n",
    "                html_tag = soup.find(\"html\")\n",
    "                if html_tag and html_tag.get(\"lang\", \"\").startswith(\"ar\"):\n",
    "                    eng_link = soup.find(\"a\", string=lambda text: text and \"English\" in text)\n",
    "                    if eng_link and eng_link.get(\"href\"):\n",
    "                        eng_url = urljoin(url, eng_link[\"href\"])\n",
    "                        print(f\"ğŸ” Switching to English version: {eng_url}\")\n",
    "                        async with session.get(eng_url) as eng_response:\n",
    "                            eng_response.raise_for_status()\n",
    "                            eng_text = await eng_response.text()\n",
    "                            soup = BeautifulSoup(eng_text, \"html.parser\")\n",
    "                            is_arabic = True\n",
    "\n",
    "            return soup\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Removing all Arabic Unicode blocks: \\u0600 to \\u06FF, plus extended blocks if needed\n",
    "def remove_arabic(text):\n",
    "    arabic_re = re.compile(r'[\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF]+')\n",
    "    return arabic_re.sub('', text)\n",
    "\n",
    "async def find_faculty_links(session, dep_url):\n",
    "    try:\n",
    "        soup = await get_soup(session, dep_url)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    links = []\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        txt = remove_arabic(a.get_text(strip=True)).lower()\n",
    "        if any(kw in txt for kw in [\"faculty\", \"staff\", \"member\", \"academic team\", \"people\", \"employee\", \"employees\"]):\n",
    "            href = a[\"href\"]\n",
    "            if href:\n",
    "                full_url = urljoin(dep_url, href)\n",
    "                if full_url.startswith(\"http\"):\n",
    "                    links.append({\n",
    "                        \"title\": a.get_text(strip=True),\n",
    "                        \"url\": full_url\n",
    "                    })\n",
    "    return links\n",
    "\n",
    "\n",
    "# Finding the contact link for the academic departments \n",
    "def find_contact_link(soup, base_url):\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        text = remove_arabic(a.get_text(strip=True)).lower()\n",
    "        if \"contact\" in text:\n",
    "            href = a[\"href\"]\n",
    "            return urljoin(base_url, href)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Scraping the contact info from the contact links\n",
    "async def extract_contact_info(session, contact_url):\n",
    "    \"\"\"Scrape the Contact Us page and extract phone, email, location, and other info.\"\"\"\n",
    "    try:\n",
    "        soup = await get_soup(session, contact_url)\n",
    "    except Exception:\n",
    "        return \"Contact page could not be loaded.\"\n",
    "\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    phone = \"Not found\"\n",
    "    email = \"Not found\"\n",
    "    location = \"Not found\"\n",
    "    extras = []\n",
    "\n",
    "    for line in lines:\n",
    "        lowered = line.lower()\n",
    "\n",
    "        if email == \"Not found\" and re.search(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", line):\n",
    "            email = re.search(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", line).group(0)\n",
    "            continue\n",
    "\n",
    "        if phone == \"Not found\" and re.search(r\"(\\+?\\d[\\d\\s\\-()]{7,})\", line):\n",
    "            phone = re.search(r\"(\\+?\\d[\\d\\s\\-()]{7,})\", line).group(0)\n",
    "            continue\n",
    "\n",
    "        if location == \"Not found\" and any(x in lowered for x in [\"ksa\", \"riyadh\", \"building\", \"street\", \"road\", \"kingdom\", \"campus\", \"p.o\", \"box\", \"hall\"]):\n",
    "            location = line\n",
    "            continue\n",
    "\n",
    "        if any(x in lowered for x in [\"fax\", \"hours\", \"box\", \"p.o\", \"linkedin\", \"facebook\", \"twitter\", \"instagram\", \"ext\", \"extension\", \"mobile\", \"call\"]):\n",
    "            extras.append(line)\n",
    "\n",
    "    extra_text = \"\\n\".join(extras) if extras else \"None\"\n",
    "    return f\"Phone Number: {phone}, Email: {email}, Location: {location}\\nOthers:\\n{extra_text}\"\n",
    "\n",
    "# Fetching about of the college categories, colleges and their departments\n",
    "async def fetch_about_of(session, url):\n",
    "    \"\"\"\n",
    "    Fetch the \"about\" text from a department (or college) URL,\n",
    "    ignoring any <p> that contains an <img>.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = await get_soup(session, url)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "    # 1) Drupal body block\n",
    "    block = soup.select_one(\"div.field--name-body\")\n",
    "    if block:\n",
    "        paras = []\n",
    "        for p in block.find_all(\"p\"):\n",
    "            if p.find(\"img\"):\n",
    "                continue\n",
    "            text = remove_arabic(p.get_text(\" \", strip=True))\n",
    "            if text:\n",
    "                paras.append(text)\n",
    "        if paras:\n",
    "            return \" \".join(paras)\n",
    "\n",
    "    # 2) Viewsâ€‘body fallback\n",
    "    span_block = soup.select_one(\n",
    "        \"span.views-field.views-field-body span.field-content\"\n",
    "    )\n",
    "    if span_block:\n",
    "        paras = []\n",
    "        for p in span_block.find_all(\"p\"):\n",
    "            if p.find(\"img\"):\n",
    "                continue\n",
    "            text = remove_arabic(p.get_text(\" \", strip=True))\n",
    "            if text:\n",
    "                paras.append(text)\n",
    "        if paras:\n",
    "            return \" \".join(paras)\n",
    "\n",
    "    # 3) Headingâ€‘driven\n",
    "    hdr = soup.find(\n",
    "        lambda t: t.name in (\"h2\",\"h3\",\"h4\")\n",
    "        and any(kw in t.text for kw in (\"About Department\",\"About the Department\",\"About College\"))\n",
    "    )\n",
    "    if hdr:\n",
    "        collected = []\n",
    "        for elem in hdr.find_all_next():\n",
    "            if elem.name in (\"h2\",\"h3\",\"h4\"):\n",
    "                break\n",
    "            if elem.name == \"p\" and not elem.find(\"img\"):\n",
    "                txt = elem.get_text(\" \", strip=True)\n",
    "                if txt:\n",
    "                    collected.append(txt)\n",
    "        if collected:\n",
    "            return \" \".join(collected)\n",
    "\n",
    "    # 4) Ultimate fallback: first pure <p> starting \"The â€¦\"\n",
    "    for p in soup.find_all(\"p\"):\n",
    "        if p.find(\"img\"):\n",
    "            continue\n",
    "        txt = p.get_text(\" \", strip=True)\n",
    "        if txt.startswith(\"The\"):\n",
    "            return txt\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def find_main_menu_ul(soup):\n",
    "    for nav in soup.find_all(\"nav\"):\n",
    "        ul = nav.find(\"ul\")\n",
    "        if ul and len(ul.find_all(\"li\", recursive=False)) >= 5:\n",
    "            return ul\n",
    "    header = soup.find(\"header\") or soup\n",
    "    best, bc = None, 0\n",
    "    for ul in header.find_all(\"ul\"):\n",
    "        cnt = len(ul.find_all(\"li\", recursive=False))\n",
    "        if cnt > bc:\n",
    "            best, bc = ul, cnt\n",
    "    if best and bc >= 5:\n",
    "        return best\n",
    "    raise RuntimeError(\"Main menu <ul> not found\")\n",
    "\n",
    "def recurse_menu(ul):\n",
    "    out = []\n",
    "    for li in ul.find_all(\"li\", recursive=False):\n",
    "        a = li.find(\"a\", recursive=False)\n",
    "        title = a.get_text(strip=True) if a else li.get_text(strip=True)\n",
    "        href  = a[\"href\"] if a and a.has_attr(\"href\") else None\n",
    "        sub = li.find(\"ul\", recursive=False)\n",
    "        out.append({\n",
    "            \"title\":    title,\n",
    "            \"url\":      href,\n",
    "            \"children\": recurse_menu(sub) if sub else []\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# Scraping the menu list\n",
    "async def scrape_menu(session):\n",
    "    soup = await get_soup(session, \"/en/home\")\n",
    "    main_ul = find_main_menu_ul(soup)\n",
    "    return recurse_menu(main_ul)\n",
    "\n",
    "# Scrape the categories from college categories\n",
    "async def scrape_category(session, path):\n",
    "    soup = await get_soup(session, path)\n",
    "    view = soup.find(\"div\", class_=\"view-content\")\n",
    "    items = []\n",
    "    if view:\n",
    "        for card in view.find_all(\"div\", class_=\"views-row\"):\n",
    "            a = card.find(\"a\", href=True)\n",
    "            if not a:\n",
    "                continue\n",
    "            items.append({\n",
    "                \"title\":    a.get_text(strip=True),\n",
    "                \"url\":      a[\"href\"],\n",
    "                \"children\": []\n",
    "            })\n",
    "    return items\n",
    "\n",
    "# Scrape the department links from the colleges (Non-JScript version)\n",
    "def get_departments(soup, base_url):\n",
    "    \"\"\"Try to extract department links either from academic section or fallback menu.\"\"\"\n",
    "    departments = []\n",
    "\n",
    "    # Primary: find \"Academic Departments\" or similar header\n",
    "    hdr = soup.find(lambda t: t.name in [\"h2\", \"h3\", \"h4\"] and any(\n",
    "        kw in t.get_text(strip=True).lower() for kw in [\"academic departments\", \"departments\", \"academic\"]\n",
    "    ))\n",
    "\n",
    "    if hdr:\n",
    "        ul = hdr.find_next_sibling(lambda t: t.name == \"ul\")\n",
    "        if ul:\n",
    "            for li in ul.find_all(\"li\"):\n",
    "                a = li.find(\"a\", href=True)\n",
    "                if a:\n",
    "                    departments.append({\n",
    "                        \"title\": a.get_text(strip=True),\n",
    "                        \"url\": urljoin(base_url, a[\"href\"])\n",
    "                    })\n",
    "\n",
    "    # Fallback: try sidebars, navs, menus\n",
    "    if not departments:\n",
    "        menus = soup.find_all([\"nav\", \"aside\", \"div\"], class_=lambda c: c and \"menu\" in c.lower())\n",
    "        for menu in menus:\n",
    "            for a in menu.find_all(\"a\", href=True):\n",
    "                text = a.get_text(strip=True).lower()\n",
    "                if \"department\" in text:\n",
    "                    departments.append({\n",
    "                        \"title\": a.get_text(strip=True),\n",
    "                        \"url\": urljoin(base_url, a[\"href\"])\n",
    "                    })\n",
    "\n",
    "    return departments\n",
    "\n",
    "# Scraping the departments with JScript sections just like in the Science Departments\n",
    "def extract_js_departments(college_url):\n",
    "    \"\"\"Uses Selenium to extract department links from JS-rendered sections like carousels.\"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--log-level=3\")  # Reduce console logs\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(college_url)\n",
    "    time.sleep(3)  # Wait for JS to load\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    departments = []\n",
    "\n",
    "    # Look for known carousel sections, or any link that looks like a department\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        text = a.get_text(strip=True).lower()\n",
    "        href = a[\"href\"]\n",
    "        if any(kw in text for kw in [\"department\", \"unit\", \"program\", \"academic\"]) and href != \"#\":\n",
    "            full_url = href if href.startswith(\"http\") else urljoin(college_url, href)\n",
    "            departments.append({\n",
    "                \"title\": a.get_text(strip=True),\n",
    "                \"url\": full_url\n",
    "            })\n",
    "\n",
    "    return departments\n",
    "\n",
    "# Scrape the FAQ pages\n",
    "async def scrape_dar_faqs(session):\n",
    "    url = \"https://dar.ksu.edu.sa/en/faqs\"\n",
    "    soup = await get_soup(session, url)\n",
    "\n",
    "    faq_list = []\n",
    "    content_block = soup.select_one(\"div.region-content\") or soup.select_one(\"main\") or soup.select_one(\"div.main-content\")\n",
    "\n",
    "    if not content_block:\n",
    "        return []\n",
    "\n",
    "    current_question = None\n",
    "\n",
    "    for tag in content_block.find_all([\"strong\", \"p\", \"div\"]):\n",
    "        if tag.name == \"strong\":\n",
    "            q_text = tag.get_text(strip=True)\n",
    "            if q_text:\n",
    "                current_question = q_text\n",
    "        elif current_question:\n",
    "            a_text = tag.get_text(strip=True)\n",
    "            if a_text:\n",
    "                faq_list.append({\n",
    "                    \"question\": current_question,\n",
    "                    \"answer\": a_text\n",
    "                })\n",
    "                current_question = None\n",
    "\n",
    "    return faq_list\n",
    "\n",
    "# Remove duplicated FAQs after combining from different sources\n",
    "def deduplicate_faqs(faqs):\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for faq in faqs:\n",
    "        q = faq[\"question\"]\n",
    "        if q not in seen:\n",
    "            deduped.append(faq)\n",
    "            seen.add(q)\n",
    "    return deduped\n",
    "\n",
    "# Scrape the FAQs with JScript in it\n",
    "def scrape_faqs_with_selenium():\n",
    "    base_url = \"https://www.ksu.edu.sa/en\"\n",
    "\n",
    "    # Step 1: Launch headless browser\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(base_url)\n",
    "\n",
    "    # Step 2: Get the FAQs link from the footer\n",
    "    faq_link = None\n",
    "    time.sleep(3)  # Let JS load\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    footer = soup.find(\"footer\")\n",
    "    if footer:\n",
    "        for a in footer.find_all(\"a\", href=True):\n",
    "            if \"faq\" in a[\"href\"].lower():\n",
    "                faq_link = urljoin(base_url, a[\"href\"])\n",
    "                break\n",
    "\n",
    "    if not faq_link:\n",
    "        print(\"âŒ Could not find FAQ link in footer.\")\n",
    "        driver.quit()\n",
    "        return []\n",
    "\n",
    "    # Step 3: Visit FAQs page\n",
    "    print(f\"ğŸ”— Visiting FAQ page: {faq_link}\")\n",
    "    driver.get(faq_link)\n",
    "    time.sleep(3)\n",
    "    fsoup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    # Step 4: Scrape FAQ content\n",
    "    faq_items = fsoup.select(\"div.faq-item\")\n",
    "    if not faq_items:\n",
    "        print(\"âŒ No FAQ items found after rendering.\")\n",
    "        return []\n",
    "\n",
    "    faq_children = []\n",
    "    for item in faq_items:\n",
    "        summary = item.find(\"summary\")\n",
    "        answer_div = item.find(\"div\", class_=\"faq-body\")\n",
    "\n",
    "        if not summary or not answer_div:\n",
    "            continue\n",
    "\n",
    "        question = summary.get_text(strip=True)\n",
    "        answer = answer_div.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        faq_children.append({\n",
    "            \"title\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "\n",
    "    return [{\n",
    "        \"title\": \"FAQs\",\n",
    "        \"url\": faq_link,\n",
    "        \"children\": faq_children\n",
    "    }]\n",
    "\n",
    "# Merge all the FAQs sources\n",
    "async def merge_all_faqs(session):\n",
    "    # Scrape both sources concurrently\n",
    "    ksu_task = asyncio.create_task(asyncio.to_thread(scrape_faqs_with_selenium))\n",
    "    dar_task = asyncio.create_task(scrape_dar_faqs(session))\n",
    "    \n",
    "    ksu_faq_sections, dar_faq_items = await asyncio.gather(ksu_task, dar_task)\n",
    "\n",
    "    # Ensure we have base KSU FAQs section\n",
    "    if not ksu_faq_sections:\n",
    "        print(\"âŒ Could not load KSU FAQs.\")\n",
    "        return []\n",
    "\n",
    "    # Extract the first (and only) section\n",
    "    ksu_faq_section = ksu_faq_sections[0]\n",
    "\n",
    "    # Convert DAR items into same structure as KSU's children\n",
    "    dar_children = [{\n",
    "        \"title\": item[\"question\"],\n",
    "        \"answer\": item[\"answer\"],\n",
    "        \"source\": \"https://dar.ksu.edu.sa/en/faqs\"\n",
    "    } for item in dar_faq_items]\n",
    "\n",
    "    # Optional: Deduplicate all FAQs (by question text)\n",
    "    combined_children = ksu_faq_section[\"children\"] + dar_children\n",
    "    deduped_children = deduplicate_faqs([\n",
    "        {\"question\": c[\"title\"], \"answer\": c[\"answer\"]} for c in combined_children\n",
    "    ])\n",
    "\n",
    "    # Re-wrap into children format\n",
    "    final_children = [{\n",
    "        \"title\": item[\"question\"],\n",
    "        \"answer\": item[\"answer\"]\n",
    "    } for item in deduped_children]\n",
    "\n",
    "    # Update and return unified section\n",
    "    return [{\n",
    "        \"title\": \"FAQs\",\n",
    "        \"url\": ksu_faq_section[\"url\"],\n",
    "        \"children\": final_children\n",
    "    }]\n",
    "\n",
    "# Scrape plagiarism\n",
    "async def scrape_plagiarism_content(session):\n",
    "    url = \"https://chss.ksu.edu.sa/en/plagiarism-en\"\n",
    "    soup = await get_soup(session, url)\n",
    "    content_block = soup.select_one(\"div.region-content\") or soup.select_one(\"main\") or soup.select_one(\"div.main-content\")\n",
    "\n",
    "    if not content_block:\n",
    "        return \"\"\n",
    "\n",
    "    text_elements = content_block.find_all([\"p\", \"h2\", \"h3\", \"li\"])\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in text_elements if p.get_text(strip=True))\n",
    "    return content\n",
    "\n",
    "# Scrape the grading system table\n",
    "async def scrape_grading_system_table(session):\n",
    "    url = \"https://dar.ksu.edu.sa/en/node/815\"\n",
    "    soup = await get_soup(session, url)\n",
    "    \n",
    "    # Find the first table in the content region\n",
    "    table = soup.find(\"table\")\n",
    "    if not table:\n",
    "        return \"\"\n",
    "\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        cols = [td.get_text(strip=True) for td in tr.find_all([\"td\", \"th\"])]\n",
    "        rows.append(\" | \".join(cols))\n",
    "\n",
    "    # Format as markdown-style table\n",
    "    if not rows:\n",
    "        return \"\"\n",
    "\n",
    "    header = rows[0]\n",
    "    separator = \" | \".join([\"---\"] * len(header.split(\" | \")))\n",
    "    body = \"\\n\".join(rows[1:])\n",
    "    return f\"**KSU Grading System Table:**\\n\\n{header}\\n{separator}\\n{body}\"\n",
    "\n",
    "# Scrape the policies\n",
    "async def scrape_regulations_and_policies(session):\n",
    "    print(\"ğŸ“˜ Scraping Regulations and Policies section...\")\n",
    "    base_url = \"https://ksu.edu.sa/en/policies\"\n",
    "    soup = await get_soup(session, base_url)\n",
    "\n",
    "    # Use the main content area (not header or footer)\n",
    "    content_block = soup.select_one(\"div.region-content\") or soup.select_one(\"main\") or soup.select_one(\"div.main-content\")\n",
    "\n",
    "    # Extract base page content\n",
    "    page_content = \"\"\n",
    "    if content_block:\n",
    "        main_text_elements = content_block.find_all([\"p\", \"h2\", \"h3\", \"li\"])\n",
    "        page_content = \"\\n\".join(p.get_text(strip=True) for p in main_text_elements if p.get_text(strip=True))\n",
    "\n",
    "    # Extract child links in main content only\n",
    "    children = []\n",
    "    seen_urls = set()\n",
    "\n",
    "    if content_block:\n",
    "        for a in content_block.select(\"a[href]\"):\n",
    "            title = a.get_text(strip=True)\n",
    "            href = a[\"href\"].strip()\n",
    "\n",
    "            if not title or \"javascript\" in href.lower() or href.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            full_url = urljoin(base_url, href)\n",
    "            if full_url in seen_urls:\n",
    "                continue\n",
    "            seen_urls.add(full_url)\n",
    "\n",
    "            children.append({\n",
    "                \"title\": title,\n",
    "                \"url\": full_url\n",
    "            })\n",
    "\n",
    "    # Get plagiarism and grading content concurrently\n",
    "    plagiarism_task = asyncio.create_task(scrape_plagiarism_content(session))\n",
    "    grading_task = asyncio.create_task(scrape_grading_system_table(session))\n",
    "    \n",
    "    plagiarism_content, grading_content = await asyncio.gather(plagiarism_task, grading_task)\n",
    "\n",
    "    #  Manually add extra children (outside the loop) like group plagiarism, grading systems and policies together under regulations and policies section\n",
    "    children.append({\n",
    "        \"title\": \"Plagiarism\",\n",
    "        \"url\": \"https://chss.ksu.edu.sa/en/plagiarism-en\",\n",
    "        \"content\": plagiarism_content\n",
    "    })\n",
    "\n",
    "    children.append({\n",
    "        \"title\": \"Grading System\",\n",
    "        \"url\": \"https://dar.ksu.edu.sa/en/node/815\",\n",
    "        \"content\": grading_content,\n",
    "        \"info\": \"https://engineering.ksu.edu.sa/sites/engineering.ksu.edu.sa/files/imce_images/regulations_of_study_and_examinations_of_ksu.pdf\"\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        \"title\": \"Regulations and Policies\",\n",
    "        \"url\": base_url,\n",
    "        \"content\": page_content,\n",
    "        \"children\": children\n",
    "    }\n",
    "\n",
    "# Scrape the admission requirements of masters and phd students \n",
    "async def scrape_admission_requirements(session):\n",
    "    print(\"ğŸ“˜ Scraping Admission Requirements section...\")\n",
    "    url = \"https://graduatestudies.ksu.edu.sa/en/node/859\"\n",
    "    soup = await get_soup(session, url)\n",
    "\n",
    "    table = soup.find(\"table\")\n",
    "    children = []\n",
    "\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            cols = row.find_all([\"td\", \"th\"])\n",
    "            if len(cols) >= 2:\n",
    "                title = cols[0].get_text(strip=True)\n",
    "                content = cols[1].get_text(separator=\"\\n\", strip=True)\n",
    "                if title and content:\n",
    "                    children.append({\n",
    "                        \"title\": title,\n",
    "                        \"url\": url,\n",
    "                        \"content\": content\n",
    "                    })\n",
    "\n",
    "    return {\n",
    "        \"title\": \"Admission Requirements\",\n",
    "        \"url\": url,\n",
    "        \"content\": \"Graduate admission criteria as listed by the Deanship of Graduate Studies.\",\n",
    "        \"children\": children\n",
    "    }\n",
    "\n",
    "# Scrape research institures \n",
    "async def scrape_research_institutes(session):\n",
    "    url = \"https://ksu.edu.sa/en/node/3106\"\n",
    "    soup = await get_soup(session, url)\n",
    "    content_block = soup.select_one(\"div.region-content\") or soup.select_one(\"main\") or soup.select_one(\"div.main-content\")\n",
    "    if not content_block:\n",
    "        return []\n",
    "\n",
    "    institutes = []\n",
    "    text = content_block.get_text(separator=\"\\n\").strip()\n",
    "    # the page lists institute names in plain textâ€”best-effort split lines:\n",
    "    for line in text.split(\"\\n\"):\n",
    "        name = line.strip()\n",
    "        if name and not name.lower().startswith(\"do you like\"):\n",
    "            institutes.append({\"title\": name})\n",
    "    return institutes\n",
    "\n",
    "# Scrape libraries sections and the libraries in each section and then finally scrape the content inside\n",
    "async def scrape_library_section(session):\n",
    "    base_url = \"https://library.ksu.edu.sa\"\n",
    "    page_url = f\"{base_url}/en\"\n",
    "    \n",
    "    async with session.get(page_url) as response:\n",
    "        response.raise_for_status()\n",
    "        text = await response.text()\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "    # Arabic â†’ English mapping\n",
    "    title_map = {\n",
    "        \"Ù…ÙƒØªØ¨Ø§Øª Ù…Ø´ØªØ±ÙƒØ©\": \"Shared libraries\",\n",
    "        \"Men libraries\": \"Men libraries\",\n",
    "        \"Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø·Ø§Ù„Ø¨Ø§Øª\": \"Female libraries\"\n",
    "    }\n",
    "\n",
    "    # Find \"Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\" tab\n",
    "    menu_items = soup.select(\"li.menu-item--expanded > a\")\n",
    "    libraries_link = None\n",
    "    for a in menu_items:\n",
    "        if \"Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\" in a.get_text(strip=True):\n",
    "            libraries_link = a.find_parent(\"li\")\n",
    "            break\n",
    "\n",
    "    if not libraries_link:\n",
    "        print(\"âŒ Could not find Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª menu.\")\n",
    "        return None\n",
    "\n",
    "    libraries_section = {\n",
    "        \"title\": \"Libraries\",\n",
    "        \"url\": page_url,\n",
    "        \"children\": []\n",
    "    }\n",
    "\n",
    "    # Process each library type concurrently\n",
    "    async def process_library_type(a):\n",
    "        arabic_title = a.get_text(strip=True)\n",
    "        url = a[\"href\"]\n",
    "        full_url = urljoin(base_url, url)\n",
    "        english_title = title_map.get(arabic_title, arabic_title)\n",
    "\n",
    "        # Scrape children links inside the page's main content\n",
    "        async with session.get(full_url) as response:\n",
    "            response.raise_for_status()\n",
    "            page_text = await response.text()\n",
    "            page_soup = BeautifulSoup(page_text, \"html.parser\")\n",
    "            \n",
    "        main_content = (\n",
    "            page_soup.select_one(\"main\") or\n",
    "            page_soup.select_one(\"div.region-content\") or\n",
    "            page_soup.select_one(\"div.main-content\") or\n",
    "            page_soup.body\n",
    "        )\n",
    "\n",
    "        link_children = []\n",
    "        if main_content:\n",
    "            # Process child pages concurrently\n",
    "            async def process_child_link(link):\n",
    "                text = link.get_text(strip=True)\n",
    "                href = link[\"href\"]\n",
    "\n",
    "                if not text or href.startswith(\"#\") or \"mailto:\" in href:\n",
    "                    return None\n",
    "\n",
    "                child_url = urljoin(full_url, href)\n",
    "\n",
    "                try:\n",
    "                    async with session.get(child_url) as child_resp:\n",
    "                        child_resp.raise_for_status()\n",
    "                        child_text = await child_resp.text()\n",
    "                        child_soup = BeautifulSoup(child_text, \"html.parser\")\n",
    "                        \n",
    "                    child_main = (\n",
    "                        child_soup.select_one(\"main\") or\n",
    "                        child_soup.select_one(\"div.region-content\") or\n",
    "                        child_soup.select_one(\"div.main-content\") or\n",
    "                        child_soup.body\n",
    "                    )\n",
    "\n",
    "                    # Extract paragraphs as \"Information\"\n",
    "                    paragraphs = [p.get_text(separator=\"\\n\", strip=True) for p in child_main.find_all(\"p\")]\n",
    "                    info_section = {\n",
    "                        \"title\": \"Information\",\n",
    "                        \"content\": \"\\n\\n\".join(paragraphs)\n",
    "                    } if paragraphs else None\n",
    "\n",
    "                    # Extract tables as \"Contact info\" - filter out empty tables\n",
    "                    tables = []\n",
    "                    for table in child_main.find_all(\"table\"):\n",
    "                        rows = []\n",
    "                        for row in table.find_all(\"tr\"):\n",
    "                            cols = [cell.get_text(strip=True) for cell in row.find_all([\"td\", \"th\"])]\n",
    "                            # Only add row if it has non-empty content\n",
    "                            if cols and any(col.strip() for col in cols):\n",
    "                                rows.append(cols)\n",
    "\n",
    "                        headers = rows[0] if rows and table.find(\"th\") else []\n",
    "                        data_rows = rows[1:] if headers else rows\n",
    "\n",
    "                        # Only add table if it has meaningful content\n",
    "                        if rows and any(any(cell.strip() for cell in row) for row in rows):\n",
    "                            tables.append({\n",
    "                                \"headers\": headers,\n",
    "                                \"rows\": data_rows\n",
    "                            })\n",
    "\n",
    "                    contact_info_section = {\n",
    "                        \"title\": \"Contact info\",\n",
    "                        \"tables\": tables\n",
    "                    } if tables else None\n",
    "\n",
    "                    # Extract location link\n",
    "                    location_section = None\n",
    "                    for a_tag in child_main.find_all(\"a\", href=True):\n",
    "                        if \"click here\" in a_tag.get_text(strip=True).lower():\n",
    "                            location_section = {\n",
    "                                \"title\": \"Location\",\n",
    "                                \"url\": urljoin(child_url, a_tag[\"href\"])\n",
    "                            }\n",
    "                            break\n",
    "\n",
    "                    child_sections = []\n",
    "                    if info_section:\n",
    "                        child_sections.append(info_section)\n",
    "                    if contact_info_section:\n",
    "                        child_sections.append(contact_info_section)\n",
    "                    if location_section:\n",
    "                        child_sections.append(location_section)\n",
    "\n",
    "                    return {\n",
    "                        \"title\": text,\n",
    "                        \"url\": child_url,\n",
    "                        \"children\": child_sections\n",
    "                    }\n",
    "\n",
    "                except Exception as e:\n",
    "                    return None\n",
    "\n",
    "            # Process all child links concurrently\n",
    "            child_tasks = [process_child_link(link) for link in main_content.find_all(\"a\", href=True)]\n",
    "            child_results = await asyncio.gather(*child_tasks, return_exceptions=True)\n",
    "            \n",
    "            # Filter out None results and exceptions\n",
    "            link_children = [result for result in child_results if result is not None and not isinstance(result, Exception)]\n",
    "\n",
    "        return {\n",
    "            \"title\": english_title,\n",
    "            \"url\": full_url,\n",
    "            \"children\": link_children\n",
    "        }\n",
    "\n",
    "    # Process all library types concurrently\n",
    "    library_tasks = [process_library_type(a) for a in libraries_link.select(\"ul.menu a\")]\n",
    "    library_results = await asyncio.gather(*library_tasks, return_exceptions=True)\n",
    "    \n",
    "    # Filter out exceptions\n",
    "    libraries_section[\"children\"] = [result for result in library_results if not isinstance(result, Exception)]\n",
    "\n",
    "    return libraries_section\n",
    "\n",
    "# Scrape the academic calendar table and contents\n",
    "async def scrape_academic_calendar(session):\n",
    "    url = \"https://dar.ksu.edu.sa/en/CurrentCalendar\"\n",
    "    \n",
    "    async with session.get(url) as response:\n",
    "        response.raise_for_status()\n",
    "        text = await response.text()\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "    # Attempt to locate a <table> first\n",
    "    tbl = soup.find(\"table\")\n",
    "    if tbl:\n",
    "        headers = [th.get_text(strip=True) for th in tbl.select(\"thead th\")] if tbl.find(\"thead\") else []\n",
    "        rows = [\n",
    "            [cell.get_text(strip=True) for cell in row.find_all([\"td\",\"th\"])]\n",
    "            for row in tbl.find_all(\"tr\")\n",
    "        ]\n",
    "        table_content = {\n",
    "            \"headers\": headers,\n",
    "            \"rows\": rows\n",
    "        }\n",
    "    else:\n",
    "        # Fallback: parse freeâ€‘text to keyâ€‘value rows\n",
    "        text_content = soup.get_text(separator=\"\\n\")\n",
    "        lines = [ln.strip() for ln in text_content.split(\"\\n\") if ln.strip()]\n",
    "        # Remove footer notices\n",
    "        lines = [ln for ln in lines if not ln.lower().startswith(\"last updated\")]\n",
    "        table_content = {\n",
    "            \"text_rows\": lines\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"title\": \"Academic Calendar\",\n",
    "        \"url\": url,\n",
    "        \"table\": table_content\n",
    "    }\n",
    "\n",
    "# Scrape housing sections for both faculties and students\n",
    "async def scrape_housing_section(session):\n",
    "    housing_url = f\"{BASE}/en/housing\"\n",
    "    soup = await get_soup(session, housing_url)\n",
    "\n",
    "    async def extract_links_from_tab(tab_id):\n",
    "        tab_div = soup.select_one(tab_id)\n",
    "        if not tab_div:\n",
    "            return []\n",
    "\n",
    "        children = []\n",
    "        for link in tab_div.find_all(\"a\", href=True):\n",
    "            title = link.get_text(strip=True)\n",
    "            href = link[\"href\"]\n",
    "            if not title or href.startswith(\"#\") or \"mailto:\" in href:\n",
    "                continue\n",
    "\n",
    "            full_url = urljoin(BASE, href)\n",
    "            children.append({\n",
    "                \"title\": title,\n",
    "                \"url\": full_url,\n",
    "                \"children\": []\n",
    "            })\n",
    "        return children\n",
    "\n",
    "    # Extract faculty and student links concurrently\n",
    "    faculty_task = extract_links_from_tab(\"#nav-faculty\")\n",
    "    student_task = extract_links_from_tab(\"#nav-students\")\n",
    "    \n",
    "    faculty_links, student_links = await asyncio.gather(faculty_task, student_task)\n",
    "\n",
    "    # Add Procedural Guide for Registration in Student Housing\n",
    "    procedural_url = \"https://sa.ksu.edu.sa/en/node/1013\"\n",
    "    procedural_soup = await get_soup(session, procedural_url)\n",
    "    article = procedural_soup.select_one(\"article\")\n",
    "    procedural_text = article.get_text(separator=\"\\n\", strip=True) if article else \"\"\n",
    "\n",
    "    student_links.append({\n",
    "        \"title\": \"Procedural Guide for Registration in Student Housing\",\n",
    "        \"url\": procedural_url,\n",
    "        \"content\": procedural_text,\n",
    "        \"children\": []\n",
    "    })\n",
    "\n",
    "    student_base_url = \"https://sa.ksu.edu.sa/en/node/1007\"\n",
    "    student_soup = await get_soup(session, student_base_url)\n",
    "\n",
    "    # Find the parent <li> that links to /en/node/6649\n",
    "    female_menu_li = student_soup.select_one('li.menu-item--expanded > a[href=\"/en/node/6649\"]')\n",
    "    if female_menu_li:\n",
    "        parent_li = female_menu_li.find_parent(\"li\")\n",
    "        submenu = parent_li.find(\"ul\", class_=\"menu sub-menu\")\n",
    "\n",
    "        female_children = []\n",
    "\n",
    "        if submenu:\n",
    "            for a in submenu.find_all(\"a\", href=True):\n",
    "                title = a.get_text(strip=True)\n",
    "                full_url = urljoin(student_base_url, a[\"href\"])\n",
    "\n",
    "                female_children.append({\n",
    "                    \"title\": title,\n",
    "                    \"url\": full_url,\n",
    "                    \"children\": []\n",
    "                })\n",
    "\n",
    "        student_links.append({\n",
    "            \"title\": \"Registration in Female Student Housing\",\n",
    "            \"url\": urljoin(student_base_url, \"/en/node/6649\"),\n",
    "            \"children\": female_children\n",
    "        })\n",
    "\n",
    "    # Scrape \"RELATED LINKS\" from the English housing site\n",
    "    faculty_housing_url = \"https://housing.ksu.edu.sa/en/\"\n",
    "    faculty_soup = await get_soup(session, faculty_housing_url)\n",
    "\n",
    "    related_links_url = \"\"\n",
    "    related_children = []\n",
    "\n",
    "    # Find <a> tag that says \"Related links\" (case-insensitive)\n",
    "    for a in faculty_soup.find_all(\"a\", href=True):\n",
    "        text = a.get_text(strip=True).lower()\n",
    "        if \"related links\" in text:\n",
    "            related_links_url = urljoin(faculty_housing_url, a[\"href\"])\n",
    "            print(f\"[âœ…] Found Related Links URL: {related_links_url}\")\n",
    "            break\n",
    "\n",
    "    # Now fetch the Related Links page if found\n",
    "    if related_links_url:\n",
    "        related_links_section = await get_soup(session, related_links_url)\n",
    "\n",
    "        content_area = related_links_section.select_one(\"article\")\n",
    "\n",
    "        if content_area:\n",
    "            links = []\n",
    "            for p in content_area.find_all(\"p\"):\n",
    "                a = p.find(\"a\", href=True)\n",
    "                if not a:\n",
    "                    continue\n",
    "                href = a[\"href\"]\n",
    "                strong = a.find(\"strong\")\n",
    "                title = strong.get_text(strip=True) if strong else a.get_text(strip=True)\n",
    "                if not title or href.startswith(\"#\") or \"mailto:\" in href:\n",
    "                    continue\n",
    "                full_url = urljoin(related_links_url, href)\n",
    "                links.append({\n",
    "                    \"title\": title,\n",
    "                    \"url\": full_url,\n",
    "                })\n",
    "\n",
    "            # Deduplicate links by (title + url)\n",
    "            seen = set()\n",
    "            deduped_links = []\n",
    "            for link in links:\n",
    "                key = (link[\"title\"], link[\"url\"])\n",
    "                if key not in seen:\n",
    "                    deduped_links.append(link)\n",
    "                    seen.add(key)\n",
    "\n",
    "            if deduped_links:\n",
    "                related_children.append({\n",
    "                    \"title\": \"Related Links\",\n",
    "                    \"url\": related_links_url,\n",
    "                    \"children\": deduped_links\n",
    "                })\n",
    "\n",
    "        # Add \"Related Links\" as a sub-section under Faculty Housing\n",
    "        faculty_links.extend(related_children)\n",
    "\n",
    "    return {\n",
    "        \"title\": \"Housing\",\n",
    "        \"url\": housing_url,\n",
    "        \"children\": [\n",
    "            {\n",
    "                \"title\": \"Faculty Housing\",\n",
    "                \"children\": faculty_links\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"Student Housing\",\n",
    "                \"children\": student_links\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Adding the scraped excel sheet from the IT Helpdesk, categorizing them as either students or staff and adding them to the hierarchy\n",
    "def build_it_helpdesk_tree(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Clean and drop invalid rows\n",
    "    df.dropna(subset=[\"Audience\", \"Category\", \"Subcategory\"], inplace=True)\n",
    "\n",
    "    # Normalize strings\n",
    "    df[\"Audience\"] = df[\"Audience\"].str.strip().str.title()\n",
    "    df[\"Category\"] = df[\"Category\"].str.strip().str.title()\n",
    "    df[\"Subcategory\"] = df[\"Subcategory\"].str.strip().str.title()\n",
    "\n",
    "    # Remove exact duplicate rows\n",
    "    df.drop_duplicates(subset=[\"Audience\", \"Category\", \"Subcategory\"], inplace=True)\n",
    "\n",
    "    # Build tree structure: Audience -> Category -> Subcategory\n",
    "    audiences = {}\n",
    "    for _, row in df.iterrows():\n",
    "        aud = row[\"Audience\"]\n",
    "        cat = row[\"Category\"]\n",
    "        sub = row[\"Subcategory\"]\n",
    "\n",
    "        if aud not in audiences:\n",
    "            audiences[aud] = {}\n",
    "\n",
    "        if cat not in audiences[aud]:\n",
    "            audiences[aud][cat] = set()\n",
    "\n",
    "        audiences[aud][cat].add(sub)\n",
    "\n",
    "    # Construct final JSON\n",
    "    audience_nodes = []\n",
    "    for aud, cats in audiences.items():\n",
    "        category_nodes = []\n",
    "        for cat, subcats in cats.items():\n",
    "            sub_nodes = [{\"title\": s, \"children\": []} for s in sorted(subcats)]\n",
    "            category_nodes.append({\n",
    "                \"title\": f\"Categories: {cat}\",\n",
    "                \"children\": sub_nodes\n",
    "            })\n",
    "        audience_nodes.append({\n",
    "            \"title\": aud,\n",
    "            \"children\": category_nodes\n",
    "        })\n",
    "\n",
    "    tree = {\n",
    "        \"title\": \"IT Helpdesk\",\n",
    "        \"url\": \"https://its.ksu.edu.sa/\",\n",
    "        \"children\": audience_nodes\n",
    "    }\n",
    "\n",
    "    return tree\n",
    "\n",
    "# Scrape the college details, about colleges, academic departments, url, content, faculty links and their contact info for both non-JScript and JScript\n",
    "async def scrape_college_details(session, path):\n",
    "    \"\"\"\n",
    "    Given a college URL, return its six modules:\n",
    "      â€“ About College (with your existing fallbacks)\n",
    "      â€“ Academic Departments (with children: title, url, content, faculty_links, contact_info)\n",
    "      â€“ News, Eventsâ€¦, Service, Important links (unchanged)\n",
    "    \"\"\"\n",
    "    college_base_url = path if path.startswith(\"http\") else urljoin(BASE, path)\n",
    "    soup = await get_soup(session, path)\n",
    "    modules = []\n",
    "\n",
    "    # â€” About College â€”\n",
    "    about = \"\"\n",
    "    block = soup.select_one(\"div.field--name-body\")\n",
    "    if block:\n",
    "        ps = block.find_all(\"p\")\n",
    "        about = \" \".join(p.get_text(\" \", strip=True) for p in ps if p.get_text(strip=True))\n",
    "\n",
    "    if not about:\n",
    "        span_block = soup.select_one(\"span.views-field.views-field-body span.field-content\")\n",
    "        if span_block:\n",
    "            about = span_block.get_text(\" \", strip=True)\n",
    "\n",
    "    if not about:\n",
    "        hdr = soup.find(lambda t: t.name in [\"h2\", \"h3\", \"h4\"] and \"About College\" in t.text)\n",
    "        if hdr:\n",
    "            p = hdr.find_next_sibling(\"p\")\n",
    "            if p:\n",
    "                about = p.get_text(\" \", strip=True)\n",
    "\n",
    "    if not about:\n",
    "        for p in soup.find_all(\"p\"):\n",
    "            t = p.get_text(\" \", strip=True)\n",
    "            if t.startswith(\"The College of\"):\n",
    "                about = t\n",
    "                break\n",
    "\n",
    "    modules.append({\"section\": \"About College\", \"content\": about})\n",
    "\n",
    "    # â€” Academic Departments â€”\n",
    "    dept_links = []\n",
    "    seen_urls = set()\n",
    "    keywords = [\"academic departments\", \"departments\", \"department\", \"academic\"]\n",
    "    hdr = soup.find(lambda t: t.name in [\"h2\", \"h3\", \"h4\"] and any(\n",
    "        kw in t.get_text(strip=True).lower() for kw in keywords)\n",
    "    )\n",
    "\n",
    "    if \"sciences.ksu.edu.sa\" in path:\n",
    "        print(\"ğŸ” Special handling for College of Sciences\")\n",
    "\n",
    "        soup = await get_soup(session, path)\n",
    "        base = path if path.endswith(\"/\") else path + \"/\"\n",
    "        departments = []\n",
    "\n",
    "        # Step 1: From main page, find departments from dropdown menu\n",
    "        for a in soup.select(\"li.menu-item a[href]\"):\n",
    "            text = a.get_text(strip=True).lower()\n",
    "            if \"department\" in text and \"/en/\" in a[\"href\"]:\n",
    "                dept_url = urljoin(base, a[\"href\"])\n",
    "                dept_title = a.get_text(strip=True)\n",
    "                print(f\"ğŸ“ Found department: {dept_title}\")\n",
    "\n",
    "                try:\n",
    "                    dept_soup = await get_soup(session, dept_url)\n",
    "                    about = await fetch_about_of(session, dept_url)\n",
    "                    contact_link = find_contact_link(dept_soup, dept_url)\n",
    "                    contact_info = await extract_contact_info(session, contact_link) if contact_link else \"Contact page not found.\"\n",
    "\n",
    "                    # Step 2: Inside department page, find EDUCATION dropdown menu\n",
    "                    edu_links = []\n",
    "                    for edu_a in dept_soup.select(\"li.menu-item a[href]\"):\n",
    "                        edu_text = edu_a.get_text(strip=True).lower()\n",
    "                        if any(x in edu_text for x in [\"faculty\", \"staff\", \"employee\"]):\n",
    "                            edu_url = urljoin(dept_url, edu_a[\"href\"])\n",
    "                            edu_links.append({\n",
    "                                \"title\": edu_a.get_text(strip=True),\n",
    "                                \"url\": edu_url\n",
    "                            })\n",
    "                            print(f\"ğŸ‘¥ Found faculty/staff link: {edu_url}\")\n",
    "\n",
    "                    departments.append({\n",
    "                        \"title\": dept_title,\n",
    "                        \"url\": dept_url,\n",
    "                        \"content\": about,\n",
    "                        \"contact_info\": contact_info,\n",
    "                        \"faculty_staff_links\": edu_links\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Error processing {dept_url}: {e}\")\n",
    "\n",
    "        modules = []\n",
    "        if departments:\n",
    "            modules.append({\n",
    "                \"section\": \"Academic Departments\",\n",
    "                \"children\": departments\n",
    "            })\n",
    "\n",
    "        return modules\n",
    "\n",
    "    grid = hdr.find_next(\"div\", class_=\"views-view-grid\") if hdr else None\n",
    "\n",
    "    if grid:\n",
    "        # Process departments concurrently\n",
    "        async def process_department(a):\n",
    "            title = a.get_text(strip=True)\n",
    "            href = a[\"href\"].strip()\n",
    "            if not title:\n",
    "                return None\n",
    "            if href.startswith(\"/ar/\") and \"/en/\" not in href:\n",
    "                href = href.replace(\"/ar/\", \"/en/\")\n",
    "            href = href if href.startswith(\"http\") else urljoin(college_base_url, href)\n",
    "            if href in seen_urls:\n",
    "                return None\n",
    "            seen_urls.add(href)\n",
    "\n",
    "            try:\n",
    "                content_task = fetch_about_of(session, href)\n",
    "                dep_soup_task = get_soup(session, href)\n",
    "                \n",
    "                content, dep_soup = await asyncio.gather(content_task, dep_soup_task)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {href} due to error: {e}\")\n",
    "                return None\n",
    "\n",
    "            contact_link = find_contact_link(dep_soup, href)\n",
    "            \n",
    "            # Run contact extraction and faculty links concurrently\n",
    "            contact_task = extract_contact_info(session, contact_link) if contact_link else asyncio.create_task(asyncio.sleep(0, result=\"Contact page not found.\"))\n",
    "            faculty_task = find_faculty_links(session, href)\n",
    "            \n",
    "            contact_info, faculty_links = await asyncio.gather(contact_task, faculty_task)\n",
    "\n",
    "            return {\n",
    "                \"title\":   title,\n",
    "                \"url\":     href,\n",
    "                \"content\": content,\n",
    "                \"faculty_links\": faculty_links,\n",
    "                \"contact_info\": contact_info\n",
    "            }\n",
    "\n",
    "        # Process all departments concurrently\n",
    "        dept_tasks = [process_department(a) for a in grid.select(\".portfolio-content a[href]\")]\n",
    "        dept_results = await asyncio.gather(*dept_tasks, return_exceptions=True)\n",
    "        \n",
    "        # Filter out None results and exceptions\n",
    "        dept_links = [result for result in dept_results if result is not None and not isinstance(result, Exception)]\n",
    "    else:\n",
    "        print(f\"âš ï¸ Fallback (JS-rendered): using Selenium on {college_base_url}\")\n",
    "        try:\n",
    "            js_departments = await asyncio.to_thread(extract_js_departments, college_base_url)\n",
    "            \n",
    "            async def process_js_department(dep):\n",
    "                title = dep[\"title\"]\n",
    "                href = dep[\"url\"]\n",
    "                if href in seen_urls:\n",
    "                    return None\n",
    "                seen_urls.add(href)\n",
    "                print(f\"ğŸ§² JS dept found: {title} â†’ {href}\")\n",
    "                \n",
    "                try:\n",
    "                    dep_soup = await get_soup(session, href)\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Skipping {href} due to error: {e}\")\n",
    "                    return None\n",
    "                    \n",
    "                content = await fetch_about_of(session, href)\n",
    "                contact_link = find_contact_link(dep_soup, href)\n",
    "                \n",
    "                contact_task = extract_contact_info(session, contact_link) if contact_link else asyncio.create_task(asyncio.sleep(0, result=\"Contact page not found.\"))\n",
    "                faculty_task = find_faculty_links(session, href)\n",
    "                \n",
    "                contact_info, faculty_links = await asyncio.gather(contact_task, faculty_task)\n",
    "                \n",
    "                return {\n",
    "                    \"title\":   title,\n",
    "                    \"url\":     href,\n",
    "                    \"content\": content,\n",
    "                    \"faculty_links\": faculty_links,\n",
    "                    \"contact_info\": contact_info\n",
    "                }\n",
    "\n",
    "            js_dept_tasks = [process_js_department(dep) for dep in js_departments]\n",
    "            js_dept_results = await asyncio.gather(*js_dept_tasks, return_exceptions=True)\n",
    "            \n",
    "            dept_links = [result for result in js_dept_results if result is not None and not isinstance(result, Exception)]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ JS Fallback failed: {e}\")\n",
    "\n",
    "    modules.append({\n",
    "        \"section\":  \"Academic Departments\",\n",
    "        \"children\": dept_links\n",
    "    })\n",
    "\n",
    "    # â€” Remaining: News, Eventsâ€¦, Service, Important links â€”\n",
    "    for name in [\"Service\"]:\n",
    "        items = []\n",
    "        hdr2 = soup.find(lambda t: t.name in [\"h2\", \"h3\", \"h4\"] and name in t.text)\n",
    "        if hdr2:\n",
    "            view2 = hdr2.find_next_sibling(\"div\", class_=\"view-content\")\n",
    "            if view2:\n",
    "                for a in view2.find_all(\"a\", href=True):\n",
    "                    items.append({\n",
    "                        \"title\": a.get_text(strip=True),\n",
    "                        \"url\":   a[\"href\"].strip()\n",
    "                    })\n",
    "        modules.append({\"section\": name, \"items\": items})\n",
    "\n",
    "    return modules\n",
    "\n",
    "# The main for all the methods above and then output it into a json file\n",
    "async def main():\n",
    "    session = await create_session()\n",
    "    \n",
    "    try:\n",
    "        print(\"Starting KSU website scraping...\")\n",
    "        print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "        print(f\"Output file: {OUTPUT_FILE}\")\n",
    "        \n",
    "        # 1) scrape menu\n",
    "        print(\"Scraping main menu...\")\n",
    "        menu = await scrape_menu(session)\n",
    "\n",
    "        # Create tasks for major sections that can run concurrently\n",
    "        print(\"Scraping major sections concurrently...\")\n",
    "        regulations_task = scrape_regulations_and_policies(session)\n",
    "        admission_task = scrape_admission_requirements(session)\n",
    "        faq_task = merge_all_faqs(session)\n",
    "        research_task = scrape_research_institutes(session)\n",
    "        library_task = scrape_library_section(session)\n",
    "        calendar_task = scrape_academic_calendar(session)\n",
    "        housing_task = scrape_housing_section(session)\n",
    "        \n",
    "        # Wait for all major sections to complete\n",
    "        (regulations_section, admission_req_section, faq_section, \n",
    "         research_institutes, library_section, academic_calendar_section, \n",
    "         housing_section) = await asyncio.gather(\n",
    "            regulations_task, admission_task, faq_task, research_task,\n",
    "            library_task, calendar_task, housing_task,\n",
    "            return_exceptions=True\n",
    "        )\n",
    "\n",
    "        # Add sections to menu\n",
    "        if not isinstance(regulations_section, Exception) and regulations_section[\"children\"]:\n",
    "            menu.append(regulations_section)\n",
    "            print(\"Added Regulations and Policies section\")\n",
    "        else:\n",
    "            print(\"No policies found under Regulations and Policies.\")\n",
    "\n",
    "        if not isinstance(admission_req_section, Exception) and admission_req_section[\"children\"]:\n",
    "            menu.append(admission_req_section)\n",
    "            print(\"Added Admission Requirements section\")\n",
    "        else:\n",
    "            print(\"No admissions found.\")\n",
    "\n",
    "        if not isinstance(faq_section, Exception) and faq_section:\n",
    "            menu.extend(faq_section)\n",
    "            print(\"âœ… Added FAQs section\")\n",
    "        else:\n",
    "            print(\"No FAQs found.\")\n",
    "\n",
    "        if not isinstance(research_institutes, Exception):\n",
    "            labs_children = [\n",
    "                {\"title\": inst[\"title\"], \"url\": \"https://ksu.edu.sa/en/node/3106\", \"content\": \"\"}\n",
    "                for inst in research_institutes\n",
    "            ]\n",
    "            # manually append Central Research Lab\n",
    "            labs_children.append({\n",
    "                \"title\": \"Central Research Lab\",\n",
    "                \"url\": \"https://crl.ksu.edu.sa/en\",\n",
    "                \"content\": \"\"\n",
    "            })\n",
    "\n",
    "            research_section = {\n",
    "                \"title\": \"Research\",\n",
    "                \"url\": \"https://ksu.edu.sa/en/node/3106\",\n",
    "                \"content\": \"Research institutes at King Saud University.\",\n",
    "                \"children\": [\n",
    "                    {\n",
    "                        \"title\": \"Labs\",\n",
    "                        \"url\": \"https://ksu.edu.sa/en/node/3106\",\n",
    "                        \"children\": labs_children\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            menu.append(research_section)\n",
    "            print(\"Added Research section\")\n",
    "\n",
    "        # Add other sections\n",
    "        if not isinstance(library_section, Exception) and library_section:\n",
    "            menu.append(library_section)\n",
    "            print(\"Added Libraries section\")\n",
    "        else:\n",
    "            print(\"Library section failed.\")\n",
    "\n",
    "        if not isinstance(academic_calendar_section, Exception) and academic_calendar_section:\n",
    "            menu.append(academic_calendar_section)\n",
    "            print(\"Added Academic Calendar section\")\n",
    "\n",
    "        if not isinstance(housing_section, Exception) and housing_section:\n",
    "            menu.append(housing_section)\n",
    "            print(\"Added Housing section\")\n",
    "\n",
    "        # IT Helpdesk (synchronous operation)\n",
    "        print(\"Processing IT Helpdesk data...\")\n",
    "        csv_path = \"IT_Helpdesk_cleaned.csv\"\n",
    "        try:\n",
    "            it_helpdesk_section = build_it_helpdesk_tree(csv_path)\n",
    "            if it_helpdesk_section:\n",
    "                menu.append(it_helpdesk_section)\n",
    "                print(\"Added IT Helpdesk section\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"CSV file '{csv_path}' not found. Skipping IT Helpdesk section.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing IT Helpdesk: {e}\")\n",
    "\n",
    "        # 2) drill into Study at KSU â†’ Colleges\n",
    "        print(\"Processing colleges data...\")\n",
    "        try:\n",
    "            study = next(m for m in menu if m[\"title\"].lower() == \"study at ksu\")\n",
    "            colleges_node = next(c for c in study[\"children\"] if c[\"title\"].lower() == \"colleges\")\n",
    "\n",
    "            # 3) build each category â†’ colleges â†’ details\n",
    "            for cat in colleges_node[\"children\"]:\n",
    "                print(f\"Processing category: {cat['title']}\")\n",
    "                cat[\"children\"] = await scrape_category(session, cat[\"url\"])\n",
    "                \n",
    "                # Process all colleges in this category concurrently\n",
    "                college_tasks = [scrape_college_details(session, coll[\"url\"]) for coll in cat[\"children\"]]\n",
    "                college_results = await asyncio.gather(*college_tasks, return_exceptions=True)\n",
    "                \n",
    "                # Assign results to colleges\n",
    "                for i, result in enumerate(college_results):\n",
    "                    if not isinstance(result, Exception):\n",
    "                        cat[\"children\"][i][\"children\"] = result\n",
    "                        print(f\"Processed college: {cat['children'][i]['title']}\")\n",
    "                    else:\n",
    "                        print(f\"Error processing college {cat['children'][i]['title']}: {result}\")\n",
    "                        cat[\"children\"][i][\"children\"] = []\n",
    "        except StopIteration:\n",
    "            print(\"Could not find 'Study at KSU' or 'Colleges' section in menu\")\n",
    "\n",
    "        # 4) Save to JSON file\n",
    "        print(\"Saving data to JSON file...\")\n",
    "        save_to_json(menu)\n",
    "        \n",
    "        print(\"Scraping completed successfully!\")\n",
    "        print(f\"Total menu items: {len(menu)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Critical error in main function: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        await session.close()\n",
    "        print(\"Session closed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if we're in a Jupyter notebook or already have an event loop\n",
    "    try:\n",
    "        # Try to get the current event loop\n",
    "        loop = asyncio.get_running_loop()\n",
    "        # If we get here, we're in a notebook - use await instead\n",
    "        print(\"Running in Jupyter notebook - use: await main()\")\n",
    "    except RuntimeError:\n",
    "        # No event loop running - safe to use asyncio.run()\n",
    "        asyncio.run(main())\n",
    "\n",
    "# For Jupyter notebooks, uncomment and run this instead:\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afef693c",
   "metadata": {},
   "source": [
    "# MAIN CHATBOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b8780",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b12b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7085d31e",
   "metadata": {},
   "source": [
    "### Creating a university chatbot that answers university data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5421620f",
   "metadata": {},
   "source": [
    "NLTK tokenizers used:\n",
    "- punkt\n",
    "- stopwords\n",
    "\n",
    "Sentence Transformers used for semantic similarity:\n",
    "- all-MiniLM-L6-v2\n",
    "- all-mpnet-base-v2\n",
    "- paraphrase-MiniLM-L3-v2\n",
    "\n",
    "Models used\n",
    "- spaCy for NER recognition\n",
    "- Local LLM for text generation: distilgpt2\n",
    "\n",
    "There is no need to download separately. Just run the code below and it will download everything needed on the run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319dceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting University AI Assistant...\n",
      "âœ… Loaded university data from C:\\Nawal\\IT Department\\Practical Training\\Final Chatbot\\data_backups\\menu_hierarchy.json\n",
      "ğŸ” Creating embeddings for 1330 text segments...\n",
      "âœ… Semantic corpus ready\n",
      "ğŸ“ Welcome! I can help with admissions, academics, libraries, housing, faculty, fees, research, and more. Type your question or 'help' for examples.\n",
      "ğŸ’¡ Tip: Ask natural questions like 'How do I apply for undergraduate admission?' or 'What are the library hours?'\n",
      "Type 'help' for examples, or 'exit' to quit.\n",
      "\n",
      "ğŸ¤– Assistant: DEBUG: Searching for: 'Colleges'\n",
      "DEBUG: Is department search: False\n",
      "DEBUG: Best match found: Colleges\n",
      "DEBUG: Best score: 100\n",
      "DEBUG: Total nodes checked: 350\n",
      "DEBUG: First 50 nodes checked:\n",
      "  1. About (at root[0])\n",
      "  2. President's Message (at root[0] -> children[0])\n",
      "  3. Leadership (at root[0] -> children[1])\n",
      "  4. King Saud University Board of Directors (at root[0] -> children[1] -> children[0])\n",
      "  5. University Administration (at root[0] -> children[1] -> children[1])\n",
      "  6. Vice President & Deanships (at root[0] -> children[1] -> children[2])\n",
      "  7. Deans and Deanship Supervisors (at root[0] -> children[1] -> children[3])\n",
      "  8. Managements and Offices (at root[0] -> children[1] -> children[4])\n",
      "  9. History (at root[0] -> children[2])\n",
      "  10. Strategy and Values (at root[0] -> children[3])\n",
      "  11. Organizational Structure (at root[0] -> children[4])\n",
      "  12. Investments (at root[0] -> children[5])\n",
      "  13. Community (at root[0] -> children[6])\n",
      "  14. Global Collaboration (at root[0] -> children[7])\n",
      "  15. Research (at root[1])\n",
      "  16. Research Achievements in 2024 (at root[1] -> children[0])\n",
      "  17. Deanships (at root[1] -> children[1])\n",
      "  18. Research Institutes (at root[1] -> children[2])\n",
      "  19. Research Centers (at root[1] -> children[3])\n",
      "  20. Research Departments and Units (at root[1] -> children[4])\n",
      "  21. Research Programs (at root[1] -> children[5])\n",
      "  22. Research Initiatives (at root[1] -> children[6])\n",
      "  23. Study at KSU (at root[2])\n",
      "  24. Colleges (at root[2] -> children[0])\n",
      "  25. Science Colleges (at root[2] -> children[0] -> children[0])\n",
      "  26. College of Business Administration (at root[2] -> children[0] -> children[0] -> children[0])\n",
      "  27. College of Architecture and Planning (at root[2] -> children[0] -> children[0] -> children[1])\n",
      "  28. College of Computer and Information Sciences (at root[2] -> children[0] -> children[0] -> children[2])\n",
      "  29. College of Food and Agricultural Sciences (at root[2] -> children[0] -> children[0] -> children[3])\n",
      "  30. College of Sciences (at root[2] -> children[0] -> children[0] -> children[4])\n",
      "  31. College of Engineering (at root[2] -> children[0] -> children[0] -> children[5])\n",
      "  32. College of Applied Studies (at root[2] -> children[0] -> children[1])\n",
      "  33. College of Applied Studies (at root[2] -> children[0] -> children[1] -> children[0])\n",
      "  34. Health Colleges (at root[2] -> children[0] -> children[2])\n",
      "  35. Prince Sultan Bin Abdulaziz College for Emergency Medical Services (at root[2] -> children[0] -> children[2] -> children[0])\n",
      "  36. College of Medicine (at root[2] -> children[0] -> children[2] -> children[1])\n",
      "  37. College of Dentistry (at root[2] -> children[0] -> children[2] -> children[2])\n",
      "  38. College of Pharmacy (at root[2] -> children[0] -> children[2] -> children[3])\n",
      "  39. College of Applied Medical Sciences (at root[2] -> children[0] -> children[2] -> children[4])\n",
      "  40. College of Nursing (at root[2] -> children[0] -> children[2] -> children[5])\n",
      "  41. Humanities Colleges (at root[2] -> children[0] -> children[3])\n",
      "  42. College of Law and Political Sciences (at root[2] -> children[0] -> children[3] -> children[0])\n",
      "  43. College of Language Sciences (at root[2] -> children[0] -> children[3] -> children[1])\n",
      "  44. College of Tourism and Archeology (at root[2] -> children[0] -> children[3] -> children[2])\n",
      "  45. College of Sport Sciences and Physical Activity (at root[2] -> children[0] -> children[3] -> children[3])\n",
      "  46. College of Education (at root[2] -> children[0] -> children[3] -> children[4])\n",
      "  47. College of Humanities and Social Sciences (at root[2] -> children[0] -> children[3] -> children[5])\n",
      "  48. College of Arts (at root[2] -> children[0] -> children[3] -> children[6])\n",
      "  49. Colleges of Muzahimya Branch (at root[2] -> children[0] -> children[4])\n",
      "  50. College of Applied Engineering (at root[2] -> children[0] -> children[4] -> children[0])\n",
      "  ... and 300 more\n",
      "\n",
      "DEBUG: Looking for department-like nodes:\n",
      "  1. Research Departments and Units (at root[1] -> children[4])\n",
      "  2. Request To Transfer Ownership Of Department Mail Or Mailing Group (at root[12] -> children[0] -> children[6] -> children[7])\n",
      "ğŸ›ï¸ **King Saud University - Colleges**\n",
      "\n",
      "ğŸ“š **College Categories:**\n",
      "\n",
      "â€¢ **Science Colleges** (6 colleges)\n",
      "â€¢ **College of Applied Studies** (1 colleges)\n",
      "â€¢ **Health Colleges** (6 colleges)\n",
      "â€¢ **Humanities Colleges** (7 colleges)\n",
      "â€¢ **Colleges of Muzahimya Branch** (4 colleges)\n",
      "\n",
      "ğŸ“ **Total: 24 colleges**\n",
      "\n",
      "ğŸ’¡ Ask about any category above for details!\n",
      "\n",
      "ğŸ¤– Assistant: I'm here to help! Please ask me something about the university. ğŸ˜Š\n",
      "\n",
      "ğŸ¤– Assistant: Thank you for using the University AI Assistant! Have a wonderful day! ğŸ‘‹\n"
     ]
    }
   ],
   "source": [
    "class UniversityChatbot():\n",
    "\n",
    "    # Initializing the university chatbot with the data and intent\n",
    "    def __init__(self, json_file_path):\n",
    "        \"\"\"Initialize the chatbot with university data\"\"\"\n",
    "        \n",
    "        # Download required NLTK data\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "        \n",
    "        try:\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "        except LookupError:\n",
    "            nltk.download('stopwords')\n",
    "        \n",
    "        # Load spaCy model\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            print(\"âš ï¸  spaCy English model not found. Install with: python -m spacy download en_core_web_sm\")\n",
    "            self.nlp = None\n",
    "        \n",
    "        # Initialize Sentence Transformer for semantic similarity\n",
    "        try:\n",
    "            self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Error loading sentence transformer: {e}\")\n",
    "            try:\n",
    "                self.sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "            except:\n",
    "                try:\n",
    "                    self.sentence_model = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n",
    "                except:\n",
    "                    print(\"âŒ Could not load any sentence transformer model\")\n",
    "                    self.sentence_model = None\n",
    "        \n",
    "        # Initialize local LLM for better response generation (optional)\n",
    "        try:\n",
    "            self.generator = pipeline(\"text-generation\", \n",
    "                                    model=\"distilgpt2\", \n",
    "                                    tokenizer=\"distilgpt2\",\n",
    "                                    max_length=100,\n",
    "                                    temperature=0.7,\n",
    "                                    do_sample=True,\n",
    "                                    pad_token_id=50256)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Could not load language model: {e}\")\n",
    "            self.generator = None\n",
    "        \n",
    "        # Load university data\n",
    "        self.data = self.load_data(json_file_path)\n",
    "        \n",
    "        # Prepare semantic corpus\n",
    "        self.prepare_semantic_corpus()\n",
    "        \n",
    "        # Enhanced intent patterns with semantic variations\n",
    "        self.intent_patterns = {\n",
    "            'admission_requirements': [\n",
    "                'admission requirements', 'how to apply', 'application process', 'entrance requirements',\n",
    "                'eligibility criteria', 'prerequisites for admission', 'admission criteria',\n",
    "                'what do I need to apply', 'application requirements', 'how can I get admitted',\n",
    "                'admission guidelines', 'entry requirements', 'application procedure'\n",
    "            ],\n",
    "            'academic_calendar': [\n",
    "                'academic calendar', 'semester schedule', 'academic year dates', 'term dates',\n",
    "                'when does semester start', 'exam schedule', 'registration dates',\n",
    "                'academic timeline', 'semester timeline', 'course schedule', 'class schedule',\n",
    "                'important dates', 'academic deadlines', 'holiday schedule'\n",
    "            ],\n",
    "            'degree_programs': [\n",
    "                'degree programs', 'available majors', 'courses offered', 'study programs',\n",
    "                'what can I study', 'academic programs', 'curriculum information',\n",
    "                'undergraduate programs', 'graduate programs', 'fields of study',\n",
    "                'departments and colleges', 'course catalog', 'program information', \"majors\", \n",
    "                'bachelors program', 'masters program', 'phd programs'\n",
    "            ],\n",
    "            'faculty': [\n",
    "                'faculty directory', 'professor information', 'faculty members', 'teaching staff',\n",
    "                'instructor details', 'faculty contacts', 'who teaches what',\n",
    "                'professor contacts', 'faculty profiles', 'academic staff',\n",
    "                'department faculty', 'faculty list'\n",
    "            ],\n",
    "            'contact_info': [\n",
    "                'contact information', 'phone numbers', 'email addresses', 'office locations',\n",
    "                'how to reach', 'contact details', 'department contacts',\n",
    "                'office hours', 'where to find', 'contact directory'\n",
    "            ],\n",
    "            'housing': [\n",
    "                'housing information', 'dormitory details', 'campus accommodation', 'residence halls',\n",
    "                'where to live', 'student housing', 'residential facilities',\n",
    "                'accommodation options', 'campus living', 'housing services'\n",
    "            ],\n",
    "            'library': [\n",
    "                'library information', 'library services', 'study resources', 'library hours',\n",
    "                'book collection', 'research materials', 'library facilities',\n",
    "                'study spaces', 'library locations', 'library catalog'\n",
    "            ],\n",
    "            'grading': [\n",
    "                'grading system', 'grade scale', 'how grades work', 'GPA calculation',\n",
    "                'academic evaluation', 'marking scheme', 'grade distribution',\n",
    "                'transcript information', 'academic performance'\n",
    "            ],\n",
    "            'plagiarism': [\n",
    "                'plagiarism policy', 'academic integrity', 'cheating policy', 'academic dishonesty',\n",
    "                'citation requirements', 'academic misconduct', 'integrity guidelines'\n",
    "            ],\n",
    "            'attendance': [\n",
    "                'attendance policy', 'class attendance', 'attendance requirements',\n",
    "                'absence policy', 'attendance rules', 'missing classes'\n",
    "            ],\n",
    "            'research': [\n",
    "                'research opportunities', 'research labs', 'research facilities', 'research centers',\n",
    "                'laboratory information', 'research programs', 'research projects'\n",
    "            ],\n",
    "            'it_support': [\n",
    "                'IT support', 'technical help', 'computer problems', 'system issues',\n",
    "                'help desk', 'technology support', 'login problems', 'password issues',\n",
    "                'wifi problems', 'software help', 'hardware issues'\n",
    "            ],\n",
    "            'fees_tuition': [\n",
    "                'tuition fees', 'cost of education', 'fee structure', 'payment information',\n",
    "                'how much does it cost', 'financial information', 'fee payment'\n",
    "            ],\n",
    "            'scholarships': [\n",
    "                'scholarship information', 'financial aid', 'funding opportunities',\n",
    "                'scholarship applications', 'financial assistance', 'grants available'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Create intent embeddings for semantic matching\n",
    "        if self.sentence_model:\n",
    "            self.create_intent_embeddings()\n",
    "        \n",
    "        # User context to track conversation state\n",
    "        self.user_context = {'last_intent': None, 'entities': {}}\n",
    "        self.conversation_state = None\n",
    "        self.user_type = None\n",
    "        \n",
    "    # Loading the data    \n",
    "    def load_data(self, json_file_path):\n",
    "        \"\"\"Load the university data from JSON file\"\"\"\n",
    "        try:\n",
    "            with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                print(f\"âœ… Loaded university data from {json_file_path}\")\n",
    "                return data\n",
    "        except FileNotFoundError:\n",
    "            print(f\"âŒ Error: Could not find {json_file_path}\")\n",
    "            print(\"Creating sample data structure...\")\n",
    "            return self.create_sample_data()\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"âŒ Error: Invalid JSON format in {json_file_path}\")\n",
    "            return {}\n",
    "    \n",
    "    # Add the semantic embeddings onto the text corpus\n",
    "    def prepare_semantic_corpus(self):\n",
    "        \"\"\"Prepare text corpus with semantic embeddings\"\"\"\n",
    "        self.corpus = []\n",
    "        self.corpus_metadata = []\n",
    "        self.corpus_embeddings = None\n",
    "        \n",
    "        def extract_text_recursive(data, path=\"\", parent_key=\"\", parent_obj=None):\n",
    "            if isinstance(data, dict):\n",
    "                for key, value in data.items():\n",
    "                    current_path = f\"{path}/{key}\" if path else key\n",
    "                    extract_text_recursive(value, current_path, key, data)\n",
    "            elif isinstance(data, list):\n",
    "                for i, item in enumerate(data):\n",
    "                    current_path = f\"{path}[{i}]\"\n",
    "                    extract_text_recursive(item, current_path, parent_key, data)\n",
    "            elif isinstance(data, str) and len(data.strip()) > 10:\n",
    "                # Extract additional fields from parent object\n",
    "                additional_fields = {}\n",
    "                if parent_obj and isinstance(parent_obj, dict):\n",
    "                    # Look for common additional fields like 'info', 'url', 'link', etc.\n",
    "                    for field in ['info', 'url', 'link', 'source', 'reference']:\n",
    "                        if field in parent_obj and parent_obj[field]:\n",
    "                            additional_fields[field] = parent_obj[field]\n",
    "                \n",
    "                self.corpus.append(data.strip())\n",
    "                self.corpus_metadata.append({\n",
    "                    'path': path,\n",
    "                    'parent_key': parent_key,\n",
    "                    'content': data.strip(),\n",
    "                    **additional_fields  # Include additional fields from parent object\n",
    "                })\n",
    "        \n",
    "        extract_text_recursive(self.data)\n",
    "        \n",
    "        if self.corpus and self.sentence_model:\n",
    "            print(f\"ğŸ” Creating embeddings for {len(self.corpus)} text segments...\")\n",
    "            self.corpus_embeddings = self.sentence_model.encode(self.corpus, convert_to_tensor=True)\n",
    "            print(\"âœ… Semantic corpus ready\")\n",
    "    \n",
    "    # Creating and adding embeddings for intent patterns and for better semantic matching\n",
    "    def create_intent_embeddings(self):\n",
    "        \"\"\"Create embeddings for intent patterns for better semantic matching\"\"\"\n",
    "        self.intent_embeddings = {}\n",
    "        \n",
    "        for intent, patterns in self.intent_patterns.items():\n",
    "            # Create embeddings for all patterns of this intent\n",
    "            pattern_embeddings = self.sentence_model.encode(patterns, convert_to_tensor=True)\n",
    "            # Use mean embedding as the intent representation\n",
    "            self.intent_embeddings[intent] = torch.mean(pattern_embeddings, dim=0)\n",
    "    \n",
    "    # Identifying user intent using semantic similarity\n",
    "    def identify_intent_semantic(self, user_input):\n",
    "        \"\"\"Identify user intent using semantic similarity\"\"\"\n",
    "        if not self.sentence_model or not hasattr(self, 'intent_embeddings'):\n",
    "            return self.identify_intent_fallback(user_input)\n",
    "        \n",
    "        user_embedding = self.sentence_model.encode([user_input], convert_to_tensor=True)\n",
    "        \n",
    "        best_intent = 'general'\n",
    "        best_score = 0.0\n",
    "        \n",
    "        for intent, intent_embedding in self.intent_embeddings.items():\n",
    "            # Calculate cosine similarity\n",
    "            similarity = torch.cosine_similarity(user_embedding, intent_embedding.unsqueeze(0))\n",
    "            score = similarity.item()\n",
    "            \n",
    "            if score > best_score and score > 0.3:  # Threshold for intent confidence\n",
    "                best_score = score\n",
    "                best_intent = intent\n",
    "        \n",
    "        return best_intent, best_score\n",
    "    \n",
    "    # Falling back using keyword matching \n",
    "    def identify_intent_fallback(self, user_input):\n",
    "        \"\"\"Fallback intent identification using keyword matching\"\"\"\n",
    "        user_input_lower = user_input.lower()\n",
    "        intent_scores = defaultdict(float)\n",
    "        \n",
    "        # Keyword matching\n",
    "        for intent, patterns in self.intent_patterns.items():\n",
    "            for pattern in patterns:\n",
    "                if pattern.lower() in user_input_lower:\n",
    "                    intent_scores[intent] += 1\n",
    "        \n",
    "        if intent_scores:\n",
    "            best_intent = max(intent_scores.items(), key=lambda x: x[1])[0]\n",
    "            best_score = intent_scores[best_intent] / len(self.intent_patterns[best_intent])\n",
    "            return best_intent, best_score\n",
    "        \n",
    "        return 'general', 0.0\n",
    "    \n",
    "    # Extradting entities and adding similar names to recognize them \n",
    "    def extract_entities_enhanced(self, user_input):\n",
    "        \"\"\"Enhanced entity extraction with better NLP\"\"\"\n",
    "        entities = {\n",
    "            'college': None,\n",
    "            'department': None,\n",
    "            'level': None,\n",
    "            'user_type': None,\n",
    "            'specific_info': []\n",
    "        }\n",
    "        \n",
    "        user_input_lower = user_input.lower()\n",
    "        \n",
    "        # Enhanced education level detection\n",
    "        level_patterns = {\n",
    "            'undergraduate': ['undergraduate', 'undergrad', 'bachelor', 'bachelors', 'bsc', 'ba', 'first degree'],\n",
    "            'masters': ['master', 'masters', 'graduate', 'msc', 'ma', 'postgraduate', 'masters degree'],\n",
    "            'phd': ['phd', 'doctorate', 'doctoral', 'ph.d', 'doctor of philosophy']\n",
    "        }\n",
    "        \n",
    "        for level, patterns in level_patterns.items():\n",
    "            if any(pattern in user_input_lower for pattern in patterns):\n",
    "                entities['level'] = level\n",
    "                break\n",
    "        \n",
    "        # Enhanced user type detection\n",
    "        user_type_patterns = {\n",
    "            'student': ['student', 'students', 'pupil', 'learner', 'studying'],\n",
    "            'staff': ['staff', 'employee', 'worker', 'administration'],\n",
    "            'faculty': ['faculty', 'professor', 'teacher', 'instructor', 'lecturer', 'academic staff']\n",
    "        }\n",
    "        \n",
    "        for user_type, patterns in user_type_patterns.items():\n",
    "            if any(pattern in user_input_lower for pattern in patterns):\n",
    "                entities['user_type'] = user_type\n",
    "                break\n",
    "        \n",
    "        # Use spaCy for named entity recognition\n",
    "        if self.nlp:\n",
    "            doc = self.nlp(user_input)\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == \"ORG\":\n",
    "                    entities['college'] = ent.text\n",
    "                elif ent.label_ in [\"PERSON\", \"GPE\"]:\n",
    "                    entities['specific_info'].append(ent.text)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    # Find most relevant content using semantic similarity which includes fallback check, encoding query, comparing similarities, best matching and filtering based on the similarity threshold\n",
    "    def find_relevant_content_semantic(self, query, top_k=5, threshold=0.25):\n",
    "        \"\"\"Find most relevant content using semantic similarity\"\"\"\n",
    "        if not self.sentence_model or self.corpus_embeddings is None or len(self.corpus) == 0:\n",
    "            return self.find_relevant_content_fallback(query, top_k)\n",
    "        \n",
    "        query_embedding = self.sentence_model.encode([query], convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        similarities = torch.cosine_similarity(query_embedding, self.corpus_embeddings)\n",
    "        \n",
    "        # Get top-k most similar documents\n",
    "        top_indices = torch.topk(similarities, min(top_k, len(similarities))).indices\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            score = similarities[idx].item()\n",
    "            if score > threshold:\n",
    "                results.append({\n",
    "                    'content': self.corpus[idx],\n",
    "                    'metadata': self.corpus_metadata[idx],\n",
    "                    'similarity': score\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # Fallback method for the previous one \n",
    "    def find_relevant_content_fallback(self, query, top_k=5):\n",
    "        \"\"\"Fallback content search using simple text matching\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        results = []\n",
    "        \n",
    "        for i, content in enumerate(self.corpus):\n",
    "            content_lower = content.lower()\n",
    "            # Simple relevance scoring based on keyword overlap\n",
    "            query_words = set(query_lower.split())\n",
    "            content_words = set(content_lower.split())\n",
    "            overlap = len(query_words.intersection(content_words))\n",
    "            \n",
    "            if overlap > 0:\n",
    "                score = overlap / len(query_words)\n",
    "                results.append({\n",
    "                    'content': content,\n",
    "                    'metadata': self.corpus_metadata[i],\n",
    "                    'similarity': score\n",
    "                })\n",
    "        \n",
    "        # Sort by similarity and return top k\n",
    "        results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        return results[:top_k]\n",
    "    \n",
    "    # Extracting links from content\n",
    "    def extract_links_from_content(self, content):\n",
    "        \"\"\"Extract URLs from content\"\"\"\n",
    "        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "        links = re.findall(url_pattern, str(content))\n",
    "        return links\n",
    "    \n",
    "    def format_table_from_html(self, html_content):\n",
    "        \"\"\"Convert HTML table to readable format\"\"\"\n",
    "        if '<table' in str(html_content).lower():\n",
    "            return f\"ğŸ“Š **Table Data:**\\n{str(html_content)}\\n\"\n",
    "        return str(html_content)\n",
    "    \n",
    "    # Help getter function to search for departments and colleges\n",
    "    def get_colleges_and_departments(self):\n",
    "        \"\"\"Extract list of colleges and departments from data\"\"\"\n",
    "        colleges = {}\n",
    "        \n",
    "        def search_recursive(data, path=\"\"):\n",
    "            if isinstance(data, dict):\n",
    "                for key, value in data.items():\n",
    "                    current_path = f\"{path}/{key}\" if path else key\n",
    "                    \n",
    "                    # Check if this looks like a college or department\n",
    "                    if any(term in key.lower() for term in ['college', 'school', 'faculty']):\n",
    "                        colleges[key] = []\n",
    "                        \n",
    "                        # Look for departments within this college\n",
    "                        if isinstance(value, dict):\n",
    "                            for sub_key in value.keys():\n",
    "                                if any(term in sub_key.lower() for term in ['department', 'dept', 'program']):\n",
    "                                    colleges[key].append(sub_key)\n",
    "                    \n",
    "                    search_recursive(value, current_path)\n",
    "        \n",
    "        search_recursive(self.data)\n",
    "        \n",
    "        # If no colleges found, try semantic search\n",
    "        if not colleges and self.sentence_model:\n",
    "            college_query = \"college school department faculty academic division\"\n",
    "            relevant_content = self.find_relevant_content_semantic(college_query, top_k=10)\n",
    "            \n",
    "            for content in relevant_content:\n",
    "                path = content['metadata']['path']\n",
    "                if any(term in path.lower() for term in ['college', 'school', 'department']):\n",
    "                    parts = path.split('/')\n",
    "                    for part in parts:\n",
    "                        if any(term in part.lower() for term in ['college', 'school']):\n",
    "                            if part not in colleges:\n",
    "                                colleges[part] = []\n",
    "        \n",
    "        return colleges\n",
    "\n",
    "    # This function helps get admission requirements of undergraduate (found in FAQs), masters and PhD from links found in Admission Requirements section from the json file\n",
    "    def handle_admission_requirements(self, entities, user_input):\n",
    "        \"\"\"Admission requirements handler with JSON navigation and follow-up support\"\"\"\n",
    "\n",
    "        if not hasattr(self, 'conversation_state'):\n",
    "            self.conversation_state = None\n",
    "\n",
    "        # Handle follow-up input if waiting for level\n",
    "        if self.conversation_state == \"awaiting_admission_level\":\n",
    "            # Re-extract entities from the follow-up input\n",
    "            follow_up_entities = self.extract_entities_enhanced(user_input)\n",
    "            if follow_up_entities['level']:\n",
    "                entities['level'] = follow_up_entities['level']\n",
    "            \n",
    "            self.conversation_state = None\n",
    "\n",
    "        # If still no level detected, prompt user\n",
    "        if not entities['level']:\n",
    "            self.conversation_state = \"awaiting_admission_level\"\n",
    "            return (\n",
    "                \"Hi! I'd be happy to help you with admission requirements. ğŸ˜Š\\n\\n\"\n",
    "                \"Could you please specify which level you're interested in?\\n\"\n",
    "                \"ğŸ“ **Undergraduate** (Bachelor's degree)\\n\"\n",
    "                \"ğŸ“š **Masters** (Graduate degree)\\n\"\n",
    "                \"ğŸ”¬ **PhD** (Doctoral degree)\\n\\n\"\n",
    "                \"Just reply with one of those!\"\n",
    "            )\n",
    "\n",
    "        self.conversation_state = None  # Clear state\n",
    "\n",
    "        # Handle UNDERGRADUATE requests\n",
    "        if entities['level'] == 'undergraduate':\n",
    "            # Look for FAQs section\n",
    "            for section in self.data:\n",
    "                if section.get(\"title\", \"\").lower() == \"faqs\":\n",
    "                    # Look for the specific FAQ about high school certificate requirements\n",
    "                    for faq in section.get(\"children\", []):\n",
    "                        title = faq.get(\"title\", \"\").lower()\n",
    "                        if \"are students who obtained their high school certificate\" in title:\n",
    "                            answer = faq.get(\"answer\", \"No answer provided.\")\n",
    "                            url = section.get(\"url\", \"\")\n",
    "                            \n",
    "                            return (\n",
    "                                \"ğŸ“ **Undergraduate Admission Requirements**\\n\\n\"\n",
    "                                f\"**Answer:** {answer}\\n\\n\"\n",
    "                                f\"ğŸ”— For more detailed information, visit: {url}\"\n",
    "                            )\n",
    "                    \n",
    "                    # If specific FAQ not found, provide general undergraduate info from FAQs\n",
    "                    url = section.get(\"url\", \"\")\n",
    "                    return (\n",
    "                        \"ğŸ“ **Undergraduate Admission Requirements**\\n\\n\"\n",
    "                        \"Based on the available information, undergraduate admission requirements include:\\n\\n\"\n",
    "                        \"â€¢ High school certificate (from within or outside the Kingdom)\\n\"\n",
    "                        \"â€¢ General Aptitude Test scores\\n\"\n",
    "                        \"â€¢ Academic Achievement Test scores\\n\"\n",
    "                        \"â€¢ Meeting specific program requirements\\n\\n\"\n",
    "                        f\"ğŸ”— For complete details and FAQs, visit: {url}\"\n",
    "                    )\n",
    "\n",
    "        # Handle MASTERS requests\n",
    "        elif entities['level'] == 'masters':\n",
    "            # Look for Admission Requirements section\n",
    "            for section in self.data:\n",
    "                if section.get(\"title\", \"\").lower() == \"admission requirements\":\n",
    "                    # Look for Master's Programs in children\n",
    "                    for child in section.get(\"children\", []):\n",
    "                        if child.get(\"title\", \"\").lower() in [\"masterâ€™s programs\", \"master's programs\"]:\n",
    "                            url = child.get(\"url\", section.get(\"url\", \"\"))\n",
    "                            content = child.get(\"content\", \"\")\n",
    "                            \n",
    "                            # Drill down one more level into nested children\n",
    "                            sub_items = child.get(\"children\", [])\n",
    "                            if sub_items:\n",
    "                                content += \"\\n\\nAdditional Details:\\n\"\n",
    "                                for item in sub_items:\n",
    "                                    title = item.get(\"title\", \"\")\n",
    "                                    body = item.get(\"content\", \"No details available.\")\n",
    "                                    content += f\"\\nâ€¢ **{title}**: {body}\"\n",
    "                            \n",
    "                            return (\n",
    "                                \"ğŸ“š **Master's Programs Admission Requirements**\\n\\n\"\n",
    "                                f\"{content}\\n\\n\"\n",
    "                                f\"ğŸ”— For complete information, visit: {url}\"\n",
    "                            )\n",
    "                    \n",
    "                    # If Master's Programs not found in children, provide general info\n",
    "                    url = section.get(\"url\", \"\")\n",
    "                    content = section.get(\"content\", \"\")\n",
    "                    return (\n",
    "                        \"ğŸ“š **Master's Programs Admission Requirements**\\n\\n\"\n",
    "                        f\"{content}\\n\\n\"\n",
    "                        \"Please check the admission requirements section for specific master's program criteria.\\n\\n\"\n",
    "                        f\"ğŸ”— For complete information, visit: {url}\"\n",
    "                    )\n",
    "\n",
    "        # Handle PHD requests\n",
    "        elif entities['level'] == 'phd':\n",
    "            # Look for Admission Requirements section\n",
    "            for section in self.data:\n",
    "                if section.get(\"title\", \"\").lower() == \"admission requirements\":\n",
    "                    # Look for PhD Programs in children\n",
    "                    for child in section.get(\"children\", []):\n",
    "                        if child.get(\"title\", \"\").lower() in [\"phd programs\"]:\n",
    "                            url = child.get(\"url\", section.get(\"url\", \"\"))\n",
    "                            content = child.get(\"content\", \"\")\n",
    "                            \n",
    "                            # Drill down one more level into nested children\n",
    "                            sub_items = child.get(\"children\", [])\n",
    "                            if sub_items:\n",
    "                                content += \"\\n\\nAdditional Details:\\n\"\n",
    "                                for item in sub_items:\n",
    "                                    title = item.get(\"title\", \"\")\n",
    "                                    body = item.get(\"content\", \"No details available.\")\n",
    "                                    content += f\"\\nâ€¢ **{title}**: {body}\"\n",
    "                            \n",
    "                            return (\n",
    "                                \"ğŸ“š **PhD's Programs Admission Requirements**\\n\\n\"\n",
    "                                f\"{content}\\n\\n\"\n",
    "                                f\"ğŸ”— For complete information, visit: {url}\"\n",
    "                            )\n",
    "                    \n",
    "                    # If PhD Programs not found in children, provide general info\n",
    "                    url = section.get(\"url\", \"\")\n",
    "                    content = section.get(\"content\", \"\")\n",
    "                    return (\n",
    "                        \"ğŸ”¬ **PhD Programs Admission Requirements**\\n\\n\"\n",
    "                        f\"{content}\\n\\n\"\n",
    "                        \"Please check the admission requirements section for specific PhD program criteria.\\n\\n\"\n",
    "                        f\"ğŸ”— For complete information, visit: {url}\"\n",
    "                    )\n",
    "\n",
    "        # Fallback if level is recognized but no specific info found\n",
    "        return (\n",
    "            f\"ğŸ¤” I couldn't find specific {entities['level']} admission requirements in our database. \"\n",
    "            \"Please try contacting the admissions office directly or visit the university website for the most accurate and up-to-date information.\"\n",
    "        )\n",
    "\n",
    "    # This function helps get academic calendar from the json file and format it into a table by putting each row on top of another (since it is not in HTML format)\n",
    "    def handle_academic_calendar(self):\n",
    "        \"\"\"Return the academic calendar from structured JSON with table formatting\"\"\"\n",
    "\n",
    "        # Look for the academic calendar item in self.data\n",
    "        calendar_item = next(\n",
    "            (item for item in self.data if item.get(\"title\", \"\").lower() == \"academic calendar\"),\n",
    "            None\n",
    "        )\n",
    "\n",
    "        if not calendar_item:\n",
    "            return (\n",
    "                \"ğŸ“… **Academic Calendar**\\n\\n\"\n",
    "                \"I couldn't find the academic calendar in our current data. \"\n",
    "                \"Please visit the registrarâ€™s official site for updated info.\\n\\n\"\n",
    "                \"ğŸ”— https://dar.ksu.edu.sa/en/CurrentCalendar\"\n",
    "            )\n",
    "\n",
    "        url = calendar_item.get(\"url\", \"\")\n",
    "        table = calendar_item.get(\"table\", {})\n",
    "\n",
    "        headers = table.get(\"headers\", [])\n",
    "        rows = table.get(\"rows\", [])\n",
    "\n",
    "        # Clean up any duplicate header row in the rows\n",
    "        if rows and headers and rows[0] == headers:\n",
    "            rows = rows[1:]\n",
    "\n",
    "        # Start formatting\n",
    "        response = \"ğŸ“… **Academic Calendar**\\n\\n\"\n",
    "        response += f\"ğŸ”— [View Full Calendar]({url})\\n\\n\"\n",
    "\n",
    "        # Format headers\n",
    "        if headers and all(isinstance(h, str) for h in headers):\n",
    "            response += \"| \" + \" | \".join(headers) + \" |\\n\"\n",
    "            response += \"|\" + \" --- |\" * len(headers) + \"\\n\"\n",
    "\n",
    "        # Format rows\n",
    "        for row in rows:\n",
    "            # Ensure each row has exactly 4 items\n",
    "            padded_row = row + [\"\"] * (len(headers) - len(row))\n",
    "            response += \"| \" + \" | \".join(padded_row[:len(headers)]) + \" |\\n\"\n",
    "\n",
    "        return response + \"\\nğŸ—“ï¸ **Need specific dates?** You can ask about registration deadlines, exam schedules, or semester start dates!\"\n",
    "      \n",
    "    # This function gets the college categories, colleges, departments, their about, contact info and faculty directories\n",
    "    def handle_degree_programs(self, entities, user_input):\n",
    "        \"\"\"\n",
    "        Simple method to navigate college hierarchy\n",
    "        Handles: Colleges -> College Categories -> Individual Colleges -> Academic Departments\n",
    "        \"\"\"\n",
    "        \n",
    "        def find_node(target_title, json_data):\n",
    "            \"\"\"Find node by title with fuzzy matching\"\"\"\n",
    "            target_lower = target_title.lower().strip()\n",
    "            best_match = None\n",
    "            best_score = 0\n",
    "            \n",
    "            # Check if this is a department search\n",
    "            is_department_search = any(word in target_lower for word in ['department', 'dept'])\n",
    "            \n",
    "            # Debug: track all nodes we're checking\n",
    "            checked_nodes = []\n",
    "            \n",
    "            def search_recursive(node, path=\"root\"):\n",
    "                nonlocal best_match, best_score\n",
    "                \n",
    "                if isinstance(node, dict) and 'title' in node:\n",
    "                    title_lower = node['title'].lower().strip()\n",
    "                    checked_nodes.append((path, node['title']))\n",
    "                    \n",
    "                    # Calculate match score\n",
    "                    if title_lower == target_lower:\n",
    "                        score = 100\n",
    "                    elif target_lower in title_lower:\n",
    "                        score = 90\n",
    "                    elif title_lower in target_lower:\n",
    "                        score = 85\n",
    "                    else:\n",
    "                        # Word overlap\n",
    "                        target_words = set(target_lower.split())\n",
    "                        title_words = set(title_lower.split())\n",
    "                        if target_words and title_words:\n",
    "                            overlap = len(target_words.intersection(title_words))\n",
    "                            union = len(target_words.union(title_words))\n",
    "                            score = (overlap / union) * 80\n",
    "                        else:\n",
    "                            score = 0\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_match = node\n",
    "                    \n",
    "                    # Special handling for department searches\n",
    "                    if is_department_search and 'children' in node:\n",
    "                        # Look for Academic Departments section\n",
    "                        for child in node.get('children', []):\n",
    "                            if isinstance(child, dict) and child.get('section') == 'Academic Departments':\n",
    "                                # Search within the departments\n",
    "                                for dept in child.get('children', []):\n",
    "                                    if isinstance(dept, dict) and 'title' in dept:\n",
    "                                        dept_title_lower = dept['title'].lower().strip()\n",
    "                                        checked_nodes.append((f\"{path} -> Academic Departments -> {dept['title']}\", dept['title']))\n",
    "                                        \n",
    "                                        # Calculate department match score\n",
    "                                        if dept_title_lower == target_lower:\n",
    "                                            dept_score = 100\n",
    "                                        elif target_lower in dept_title_lower:\n",
    "                                            dept_score = 90\n",
    "                                        elif dept_title_lower in target_lower:\n",
    "                                            dept_score = 85\n",
    "                                        else:\n",
    "                                            # Word overlap for departments\n",
    "                                            dept_target_words = set(target_lower.split())\n",
    "                                            dept_title_words = set(dept_title_lower.split())\n",
    "                                            if dept_target_words and dept_title_words:\n",
    "                                                dept_overlap = len(dept_target_words.intersection(dept_title_words))\n",
    "                                                dept_union = len(dept_target_words.union(dept_title_words))\n",
    "                                                dept_score = (dept_overlap / dept_union) * 80\n",
    "                                            else:\n",
    "                                                dept_score = 0\n",
    "                                        \n",
    "                                        if dept_score > best_score:\n",
    "                                            best_score = dept_score\n",
    "                                            best_match = dept\n",
    "                    \n",
    "                    # Search children normally\n",
    "                    if 'children' in node:\n",
    "                        for i, child in enumerate(node['children']):\n",
    "                            search_recursive(child, f\"{path} -> children[{i}]\")\n",
    "                \n",
    "                elif isinstance(node, list):\n",
    "                    for i, item in enumerate(node):\n",
    "                        search_recursive(item, f\"{path}[{i}]\")\n",
    "            \n",
    "            search_recursive(json_data)\n",
    "            \n",
    "            return best_match if best_score > 30 else None\n",
    "        \n",
    "        def get_node_type(node):\n",
    "            \"\"\"Determine what type of node this is\"\"\"\n",
    "            if not isinstance(node, dict) or 'title' not in node:\n",
    "                return 'unknown'\n",
    "            \n",
    "            title = node['title'].lower()\n",
    "            \n",
    "            # Main \"Colleges\" node\n",
    "            if title == 'colleges' and 'children' in node:\n",
    "                return 'colleges_root'\n",
    "            \n",
    "            # College categories (contains \"colleges\" in title)\n",
    "            if 'colleges' in title and 'children' in node:\n",
    "                return 'category'\n",
    "            \n",
    "            # Individual college (has \"About College\" section)\n",
    "            if 'children' in node:\n",
    "                for child in node.get('children', []):\n",
    "                    if isinstance(child, dict) and child.get('section') == 'About College':\n",
    "                        return 'college'\n",
    "            \n",
    "            # Academic department (has contact_info, faculty_links, faculty_staff_links, or detailed content)\n",
    "            if any(key in node for key in ['contact_info', 'faculty_links', 'faculty_staff_links']) or \\\n",
    "            (node.get('content') and len(node.get('content', '')) > 100):\n",
    "                return 'department'\n",
    "            \n",
    "            return 'unknown'\n",
    "        \n",
    "        # Find the target node\n",
    "        target_node = find_node(user_input.strip(), self.data)\n",
    "        \n",
    "        if not target_node:\n",
    "            return (\"ğŸ¤– I couldn't find that. Try:\\n\"\n",
    "                    \"â€¢ 'Colleges' - see all categories\\n\"\n",
    "                    \"â€¢ 'Science Colleges' - see colleges in category\\n\"\n",
    "                    \"â€¢ 'College of Engineering' - specific college\\n\"\n",
    "                    \"â€¢ 'Computer Science Department' - department info\")\n",
    "        \n",
    "        node_type = get_node_type(target_node)\n",
    "        title = target_node.get('title', '')\n",
    "        \n",
    "        # Handle based on node type\n",
    "        if node_type == 'colleges_root':\n",
    "            # Show college categories only\n",
    "            response = \"ğŸ›ï¸ **King Saud University - Colleges**\\n\\n\"\n",
    "            response += \"ğŸ“š **College Categories:**\\n\\n\"\n",
    "            \n",
    "            total_colleges = 0\n",
    "            for category in target_node.get('children', []):\n",
    "                if isinstance(category, dict) and 'title' in category:\n",
    "                    college_count = len(category.get('children', []))\n",
    "                    total_colleges += college_count\n",
    "                    response += f\"â€¢ **{category['title']}** ({college_count} colleges)\\n\"\n",
    "            \n",
    "            response += f\"\\nğŸ“ **Total: {total_colleges} colleges**\\n\\n\"\n",
    "            response += \"ğŸ’¡ Ask about any category above for details!\"\n",
    "            return response\n",
    "        \n",
    "        elif node_type == 'category':\n",
    "            # Show colleges in this category\n",
    "            response = f\"ğŸ›ï¸ **{title}**\\n\\n\"\n",
    "            colleges = target_node.get('children', [])\n",
    "            \n",
    "            if colleges:\n",
    "                response += f\"**Colleges ({len(colleges)} total):**\\n\\n\"\n",
    "                for i, college in enumerate(colleges, 1):\n",
    "                    if isinstance(college, dict) and 'title' in college:\n",
    "                        response += f\"{i}. **{college['title']}**\\n\"\n",
    "                        if college.get('url'):\n",
    "                            response += f\"   ğŸ”— {college['url']}\\n\"\n",
    "                        response += \"\\n\"\n",
    "            \n",
    "            response += \"ğŸ’¡ Ask about any college above for details!\"\n",
    "            return response\n",
    "        \n",
    "        elif node_type == 'college':\n",
    "            # Show college info + list departments (titles only)\n",
    "            response = f\"ğŸ“ **{title}**\\n\\n\"\n",
    "            \n",
    "            if target_node.get('url'):\n",
    "                response += f\"ğŸ”— **Website:** {target_node['url']}\\n\\n\"\n",
    "            \n",
    "            # Find About College section\n",
    "            about_content = None\n",
    "            departments = []\n",
    "            \n",
    "            for child in target_node.get('children', []):\n",
    "                if isinstance(child, dict):\n",
    "                    if child.get('section') == 'About College':\n",
    "                        about_content = child.get('content', '')\n",
    "                    elif child.get('section') == 'Academic Departments':\n",
    "                        departments = child.get('children', [])\n",
    "            \n",
    "            if about_content:\n",
    "                response += f\"**About the College:**\\n{about_content}\\n\\n\"\n",
    "            \n",
    "            if departments:\n",
    "                response += f\"**Academic Departments ({len(departments)} total):**\\n\\n\"\n",
    "                for i, dept in enumerate(departments, 1):\n",
    "                    if isinstance(dept, dict) and 'title' in dept:\n",
    "                        response += f\"{i}. **{dept['title']}**\\n\"\n",
    "                \n",
    "                response += \"\\nğŸ’¡ Ask about any department for detailed info!\"\n",
    "            \n",
    "            return response\n",
    "        \n",
    "        elif node_type == 'department':\n",
    "            # Show full department details\n",
    "            response = f\"ğŸ“ **{title}**\\n\\n\"\n",
    "            \n",
    "            if target_node.get('url'):\n",
    "                response += f\"ğŸ”— **Website:** {target_node['url']}\\n\\n\"\n",
    "            \n",
    "            if target_node.get('content'):\n",
    "                content = target_node['content'].strip()\n",
    "                if content:\n",
    "                    response += f\"**About the Department:**\\n{content}\\n\\n\"\n",
    "            \n",
    "            if target_node.get('contact_info'):\n",
    "                contact = target_node['contact_info'].strip()\n",
    "                if contact:\n",
    "                    response += f\"ğŸ“ **Contact Information:**\\n{contact}\\n\\n\"\n",
    "            \n",
    "            # Faculty links\n",
    "            faculty_links = target_node.get('faculty_links', []) + target_node.get('faculty_staff_links', [])\n",
    "            if faculty_links:\n",
    "                response += \"ğŸ‘¨â€ğŸ« **Faculty & Staff:**\\n\"\n",
    "                for link in faculty_links:\n",
    "                    if isinstance(link, dict):\n",
    "                        title_text = link.get('title', 'Faculty Link')\n",
    "                        url = link.get('url', '#')\n",
    "                        response += f\"â€¢ [{title_text}]({url})\\n\"\n",
    "                response += \"\\n\"\n",
    "            \n",
    "            return response\n",
    "        \n",
    "        else:\n",
    "            return f\"â„¹ï¸ Found '{title}' but couldn't determine its type. Please be more specific.\"\n",
    "       \n",
    "    # This function finds the housing information (things that I thought were important)\n",
    "    def handle_housing(self, entities, user_input):\n",
    "        \"\"\"Handle housing queries using the correct Housing section\"\"\"\n",
    "        \n",
    "        # Find the Housing section that actually has data (not the empty one)\n",
    "        housing_section = self.find_housing_with_data()\n",
    "        \n",
    "        if not housing_section:\n",
    "            return \"ğŸ  Housing information not found.\"\n",
    "        \n",
    "        # If no user type specified, ask for clarification\n",
    "        if not entities.get('user_type'):\n",
    "            housing_types = self.get_housing_types(housing_section)\n",
    "            response = \"ğŸ  I'd be happy to help with housing information! Please specify:\\n\\n\"\n",
    "            for housing_type in housing_types:\n",
    "                emoji = \"ğŸ“\" if \"student\" in housing_type.lower() else \"ğŸ‘¨â€ğŸ«\"\n",
    "                response += f\"{emoji} **{housing_type}**\\n\"\n",
    "            return response + \"\\nWhich type of housing are you interested in?\"\n",
    "        \n",
    "        # Handle faculty housing\n",
    "        if entities['user_type'].lower() in ['faculty', 'staff', 'employee']:\n",
    "            return self.handle_faculty_housing(housing_section)\n",
    "        \n",
    "        # Handle student housing\n",
    "        elif entities['user_type'].lower() in ['student']:\n",
    "            return self.handle_student_housing(housing_section)\n",
    "        \n",
    "        return \"ğŸ  Please specify faculty or student housing.\"\n",
    "\n",
    "    # This addition function gets the housing information\n",
    "    def find_housing_with_data(self):\n",
    "        \"\"\"Find the Housing section that contains actual data (not empty)\"\"\"\n",
    "        def search_recursive(data):\n",
    "            if isinstance(data, dict):\n",
    "                title = data.get('title', '')\n",
    "                if title.lower() == 'housing' and data.get('children'):\n",
    "                    # Found Housing section with children - this is the one we want\n",
    "                    return data\n",
    "                \n",
    "                # Search in children\n",
    "                for child in data.get('children', []):\n",
    "                    result = search_recursive(child)\n",
    "                    if result:\n",
    "                        return result\n",
    "            elif isinstance(data, list):\n",
    "                for item in data:\n",
    "                    result = search_recursive(item)\n",
    "                    if result:\n",
    "                        return result\n",
    "            return None\n",
    "        \n",
    "        return search_recursive(self.data)\n",
    "\n",
    "    # This additional function gets housing type whether it is student or faculty housing\n",
    "    def get_housing_types(self, housing_section):\n",
    "        \"\"\"Extract housing types from the housing section\"\"\"\n",
    "        housing_types = []\n",
    "        for child in housing_section.get('children', []):\n",
    "            title = child.get('title', '')\n",
    "            if title and 'housing' in title.lower():\n",
    "                housing_types.append(title)\n",
    "        return housing_types\n",
    "\n",
    "    # This additional function helps navigating Faculty housing sections that I thought were important\n",
    "    def handle_faculty_housing(self, housing_section):\n",
    "        \"\"\"Handle faculty housing navigation\"\"\"\n",
    "        # Find Faculty Housing section\n",
    "        faculty_housing = None\n",
    "        for child in housing_section.get('children', []):\n",
    "            if child.get('title', '').lower() == 'faculty housing':\n",
    "                faculty_housing = child\n",
    "                break\n",
    "        \n",
    "        if not faculty_housing:\n",
    "            available_children = [child.get('title', 'No title') for child in housing_section.get('children', [])]\n",
    "            return f\"ğŸ  Faculty housing not found. Available options: {', '.join(available_children)}\"\n",
    "        \n",
    "        response = \"ğŸ  **Faculty Housing**\\n\\n\"\n",
    "        \n",
    "        # Display direct children of Faculty Housing\n",
    "        for child in faculty_housing.get('children', []):\n",
    "            title = child.get('title', '')\n",
    "            url = child.get('url', '')\n",
    "            \n",
    "            response += f\"ğŸ“‹ **{title}**\\n\"\n",
    "            \n",
    "            if url:\n",
    "                response += f\"ğŸ”— [Access {title}]({url})\\n\"\n",
    "            \n",
    "            # If this child has its own children (like \"Related Links\"), display them\n",
    "            if child.get('children'):\n",
    "                response += self.display_child_links(child)\n",
    "            \n",
    "            response += \"\\n\"\n",
    "        \n",
    "        return response\n",
    "\n",
    "    # This additional function helps navigating Student housing sections that I thought were important\n",
    "    def handle_student_housing(self, housing_section):\n",
    "        \"\"\"Handle student housing navigation\"\"\"\n",
    "        student_housing = None\n",
    "        for child in housing_section.get('children', []):\n",
    "            if child.get('title', '').lower() == 'student housing':\n",
    "                student_housing = child\n",
    "                break\n",
    "        \n",
    "        if not student_housing:\n",
    "            available_children = [child.get('title', 'No title') for child in housing_section.get('children', [])]\n",
    "            return f\"ğŸ  Student housing not found. Available options: {', '.join(available_children)}\"\n",
    "        \n",
    "        response = \"ğŸ  **Student Housing**\\n\\n\"\n",
    "        \n",
    "        for child in student_housing.get('children', []):\n",
    "            title = child.get('title', '')\n",
    "            url = child.get('url', '')\n",
    "            content = child.get('content', '')\n",
    "            \n",
    "            response += f\"ğŸ“‹ **{title}**\\n\"\n",
    "            \n",
    "            # Show content if available (like the procedural guide)\n",
    "            if content:\n",
    "                # Format the content nicely\n",
    "                formatted_content = self.format_housing_content(content)\n",
    "                response += f\"{formatted_content}\\n\"\n",
    "            \n",
    "            if url:\n",
    "                response += f\"ğŸ”— [Access {title}]({url})\\n\"\n",
    "            \n",
    "            # Show child links if any (like for Female Student Housing)\n",
    "            if child.get('children'):\n",
    "                response += self.display_child_links(child)\n",
    "            \n",
    "            response += \"\\n\"\n",
    "        \n",
    "        return response\n",
    "\n",
    "    # Gets child links \n",
    "    def display_child_links(self, parent_section):\n",
    "        \"\"\"Display children of a section (like Related Links)\"\"\"\n",
    "        if not parent_section.get('children'):\n",
    "            return \"\"\n",
    "        \n",
    "        response = f\"\\n**Available options:**\\n\"\n",
    "        \n",
    "        for child in parent_section.get('children', []):\n",
    "            child_title = child.get('title', '')\n",
    "            child_url = child.get('url', '')\n",
    "            \n",
    "            if child_title:\n",
    "                response += f\"â€¢ **{child_title}**\"\n",
    "                if child_url:\n",
    "                    response += f\" - [Access here]({child_url})\"\n",
    "                response += \"\\n\"\n",
    "        \n",
    "        return response\n",
    "\n",
    "    # Editing the housing information content for better readability\n",
    "    def format_housing_content(self, content):\n",
    "        \"\"\"Format housing content for better readability\"\"\"\n",
    "        if not content:\n",
    "            return \"\"\n",
    "        \n",
    "        # Split into lines and format\n",
    "        lines = content.split('\\n')\n",
    "        formatted_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                # Make steps and important headings bold\n",
    "                if any(keyword in line.lower() for keyword in ['step ', 'condition', 'how to apply']):\n",
    "                    formatted_lines.append(f\"**{line}**\")\n",
    "                else:\n",
    "                    formatted_lines.append(line)\n",
    "        \n",
    "        return '\\n'.join(formatted_lines)\n",
    "    \n",
    "    # This is to handle library sections, their libraries and content of the libraries along with Location links\n",
    "    def handle_library(self, entities, user_input):\n",
    "        user_input_lower = user_input.lower()\n",
    "\n",
    "        def contains_title(item, text):\n",
    "            return item.get(\"title\", \"\").lower() in text\n",
    "\n",
    "        def find_matching_node(data, text):\n",
    "            for node in data:\n",
    "                if contains_title(node, text):\n",
    "                    return node\n",
    "            return None\n",
    "\n",
    "        def find_matching_child(parent, text):\n",
    "            for child in parent.get(\"children\", []):\n",
    "                if contains_title(child, text):\n",
    "                    return child\n",
    "            return None\n",
    "\n",
    "        def format_contact_info(section):\n",
    "            lines = []\n",
    "            for table in section.get(\"tables\", []):\n",
    "                for row in table.get(\"rows\", []):\n",
    "                    lines.append(\" | \".join(row))\n",
    "            return \"\\n\".join(lines)\n",
    "\n",
    "        # STEP 1: Start from top-level \"Libraries\"\n",
    "        libraries_root = find_matching_node(self.data, \"libraries\")\n",
    "        if not libraries_root:\n",
    "            return \"âŒ Could not find 'Libraries' section in data.\"\n",
    "\n",
    "        # STEP 2: If user specifies a category (e.g., Shared libraries)\n",
    "        selected_category = find_matching_child(libraries_root, user_input_lower)\n",
    "        if selected_category:\n",
    "            # STEP 2a: If user already specified a library\n",
    "            matched_library = find_matching_child(selected_category, user_input_lower)\n",
    "            if matched_library:\n",
    "                info, contact, location = \"\", \"\", \"\"\n",
    "\n",
    "                for section in matched_library.get(\"children\", []):\n",
    "                    title = section.get(\"title\", \"\").lower()\n",
    "                    if \"information\" in title:\n",
    "                        info = section.get(\"content\", \"\").strip()\n",
    "                    elif \"contact\" in title:\n",
    "                        contact = format_contact_info(section)\n",
    "                    elif \"location\" in title:\n",
    "                        location = section.get(\"url\", \"\")\n",
    "\n",
    "                response = f\"ğŸ“š **{matched_library['title']}**\\n\\n\"\n",
    "                if info:\n",
    "                    response += f\"**Information:**\\n{info}\\n\\n\"\n",
    "                if contact:\n",
    "                    response += f\"**Contact Info:**\\n{contact}\\n\\n\"\n",
    "                if location:\n",
    "                    response += f\"**Location:** {location}\"\n",
    "                return response.strip()\n",
    "\n",
    "            # STEP 2b: User just selected the category, list children\n",
    "            library_titles = [child[\"title\"] for child in selected_category.get(\"children\", [])]\n",
    "            return f\"Here are the libraries under **{selected_category['title']}**:\\n\" + \"\\n\".join(f\"- {title}\" for title in library_titles)\n",
    "\n",
    "        # STEP 3: User mentioned a library directly without saying category\n",
    "        for category in libraries_root.get(\"children\", []):\n",
    "            matched_library = find_matching_child(category, user_input_lower)\n",
    "            if matched_library:\n",
    "                info, contact, location = \"\", \"\", \"\"\n",
    "                for section in matched_library.get(\"children\", []):\n",
    "                    title = section.get(\"title\", \"\").lower()\n",
    "                    if \"information\" in title:\n",
    "                        info = section.get(\"content\", \"\").strip()\n",
    "                    elif \"contact\" in title:\n",
    "                        contact = format_contact_info(section)\n",
    "                    elif \"location\" in title:\n",
    "                        location = section.get(\"url\", \"\")\n",
    "\n",
    "                response = f\"ğŸ“š **{matched_library['title']}**\\n\\n\"\n",
    "                if info:\n",
    "                    response += f\"**Information:**\\n{info}\\n\\n\"\n",
    "                if contact:\n",
    "                    response += f\"**Contact Info:**\\n{contact}\\n\\n\"\n",
    "                if location:\n",
    "                    response += f\"**Location:** {location}\"\n",
    "                return response.strip()\n",
    "\n",
    "        # STEP 4: User only said \"libraries\" â†’ ask to choose category\n",
    "        category_titles = [cat[\"title\"] for cat in libraries_root.get(\"children\", [])]\n",
    "        return \"ğŸ¤– Would you like to know about one of the following library categories?\\n\" + \"\\n\".join(f\"- {title}\" for title in category_titles)\n",
    "    \n",
    "    # This function is to scrape grading system scale and add the PDF\n",
    "    def handle_grading_system(self):\n",
    "        \"\"\"Handle grading system with semantic search\"\"\"\n",
    "        query = \"grading system grade scale GPA evaluation assessment\"\n",
    "        relevant_content = self.find_relevant_content_semantic(query, top_k=5)\n",
    "        response = \"ğŸ“Š **Grading System**\\n\\n\"\n",
    "        \n",
    "        for content in relevant_content[:3]:\n",
    "            content_text = content['content']\n",
    "            response += content_text + \"\\n\\n\"\n",
    "            \n",
    "            # Extract links from content text\n",
    "            links = self.extract_links_from_content(content_text)\n",
    "            if links:\n",
    "                response += f\"ğŸ”— [Grading policy PDF]({links[0]})\\n\\n\"\n",
    "            \n",
    "            # Check metadata for additional fields like 'info'\n",
    "            metadata = content.get('metadata', {})\n",
    "            if 'info' in metadata and metadata['info']:\n",
    "                response += f\"ğŸ”— [Study and Examinations Regulations PDF]({metadata['info']})\\n\\n\"\n",
    "            elif 'url' in metadata and metadata['url']:\n",
    "                response += f\"ğŸ”— [Additional information]({metadata['url']})\\n\\n\"\n",
    "        \n",
    "        if not relevant_content:\n",
    "            response += \"Grading system information is not available in our current database. Please check the student handbook or contact academic affairs.\\n\\n\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    # This function is to handle the plagiarism with the PDF\n",
    "    def handle_plagiarism(self):\n",
    "        \"\"\"Handle plagiarism queries with semantic search\"\"\"\n",
    "        query = \"plagiarism academic integrity policy cheating misconduct\"\n",
    "        relevant_content = self.find_relevant_content_semantic(query, top_k=5)\n",
    "        \n",
    "        response = \"âš ï¸ **Academic Integrity & Plagiarism Policy**\\n\\n\"\n",
    "        \n",
    "        for content in relevant_content[:3]:\n",
    "            response += content['content'] + \"\\n\\n\"\n",
    "            links = self.extract_links_from_content(content['content'])\n",
    "            if links:\n",
    "                response += f\"ğŸ”— [Full policy]({links[0]})\\n\\n\"\n",
    "        \n",
    "        if not relevant_content:\n",
    "            response += \"Plagiarism policy information is not available in our current database. Please check the student handbook or contact academic affairs.\\n\\n\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    # This function is to handle the attendance rules with the PDF\n",
    "    def handle_attendance(self):\n",
    "        \"\"\"Handle attendance queries\"\"\"\n",
    "        query = \"attendance policy class attendance requirements\"\n",
    "        relevant_content = self.find_relevant_content_semantic(query, top_k=3)\n",
    "        \n",
    "        response = \"ğŸ“‹ **Attendance Policy**\\n\\n\"\n",
    "        \n",
    "        if relevant_content:\n",
    "            for content in relevant_content:\n",
    "                response += content['content'] + \"\\n\\n\"\n",
    "                links = self.extract_links_from_content(content['content'])\n",
    "                if links:\n",
    "                    response += f\"ğŸ”— [Attendance policy]({links[0]})\\n\\n\"\n",
    "        else:\n",
    "            response += \"Please refer to the grading system document for detailed attendance requirements.\\n\\n\"\n",
    "        \n",
    "        # Always check for grading system info field as it contains attendance details\n",
    "        grading_query = \"grading system grade scale GPA evaluation assessment\"\n",
    "        grading_content = self.find_relevant_content_semantic(grading_query, top_k=5)\n",
    "        \n",
    "        for content in grading_content:\n",
    "            metadata = content.get('metadata', {})\n",
    "            # Check if this is grading system content and has info field\n",
    "            if 'info' in metadata and metadata['info']:\n",
    "                response += f\"ğŸ“– Please refer to the Study and Examinations Regulations for detailed attendance policy:\\n\"\n",
    "                response += f\"ğŸ”— [Attendance found here: ]({metadata['info']})\\n\\n\"\n",
    "                break\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    # This is to scrape the research labs links\n",
    "    def handle_research_labs(self):\n",
    "        \"\"\"Handle research labs and facilities - find Research node with Labs child\"\"\"\n",
    "        \n",
    "        response = \"ğŸ”¬ **Research Labs & Facilities**\\n\\n\"\n",
    "        \n",
    "        def find_research_with_labs(data):\n",
    "            \"\"\"Recursively find Research node that has Labs as a child\"\"\"\n",
    "            if isinstance(data, dict):\n",
    "                # Check if this is a Research node\n",
    "                if data.get('title') == 'Research':\n",
    "                    # Check if it has children\n",
    "                    if 'children' in data:\n",
    "                        for child in data['children']:\n",
    "                            if isinstance(child, dict) and child.get('title') == 'Labs':\n",
    "                                # Found Research->Labs! Return the Labs children\n",
    "                                return child.get('children', [])\n",
    "                \n",
    "                # Recursively search in children\n",
    "                if 'children' in data:\n",
    "                    for child in data['children']:\n",
    "                        result = find_research_with_labs(child)\n",
    "                        if result:\n",
    "                            return result\n",
    "            \n",
    "            elif isinstance(data, list):\n",
    "                # If data is a list, search each item\n",
    "                for item in data:\n",
    "                    result = find_research_with_labs(item)\n",
    "                    if result:\n",
    "                        return result\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            labs_children = find_research_with_labs(self.data)  # You'll need to adjust this\n",
    "            \n",
    "            if labs_children:\n",
    "                response += \"Here are the research labs and facilities:\\n\\n\"\n",
    "                for i, lab in enumerate(labs_children, 1):\n",
    "                    title = lab.get('title', 'Unknown Lab')\n",
    "                    url = lab.get('url', '')\n",
    "                    \n",
    "                    # Normalize URL\n",
    "                    if url and url.startswith('/'):\n",
    "                        url = f\"https://ksu.edu.sa{url}\"\n",
    "                    \n",
    "                    response += f\"{i}. **{title}**\\n\"\n",
    "                    if url:\n",
    "                        response += f\"   ğŸ”— {url}\\n\"\n",
    "                    response += \"\\n\"\n",
    "            else:\n",
    "                response += \"Could not find Research node with Labs child.\\n\\n\"\n",
    "                \n",
    "        except AttributeError:\n",
    "            # Fallback if we don't have direct access to JSON data\n",
    "            response += \"Unable to access JSON data directly. Please ensure the JSON data is available.\\n\\n\"\n",
    "        \n",
    "        return response\n",
    "\n",
    "    # This function adds all the IT helpdesk for student and staff and paths inside. \n",
    "    def handle_it_support(self, entities, user_input):\n",
    "        from difflib import get_close_matches\n",
    "\n",
    "        # Find IT Helpdesk node\n",
    "        it_helpdesk = next((item for item in self.data if item.get(\"title\", \"\").lower() == \"it helpdesk\"), None)\n",
    "        if not it_helpdesk:\n",
    "            return \"âš ï¸ Sorry, I couldn't find the IT Helpdesk information.\"\n",
    "\n",
    "        user_input_lower = user_input.lower()\n",
    "        user_type = None\n",
    "\n",
    "        if \"student\" in user_input_lower:\n",
    "            user_type = \"Student\"\n",
    "        elif \"staff\" in user_input_lower or \"faculty\" in user_input_lower or \"professor\" in user_input_lower:\n",
    "            user_type = \"Staff\"\n",
    "\n",
    "        if not user_type:\n",
    "            return \"â“ Are you a student or staff/faculty? Please specify so I can assist you.\"\n",
    "\n",
    "        # Navigate to the relevant section\n",
    "        section = next((child for child in it_helpdesk.get(\"children\", []) if child.get(\"title\", \"\").lower() == user_type.lower()), None)\n",
    "        if not section:\n",
    "            return f\"âš ï¸ Sorry, I couldnâ€™t find IT support info for {user_type}.\"\n",
    "\n",
    "        # Extract all issues and sub-issues\n",
    "        def extract_issues_with_hierarchy(node, parent_title=None):\n",
    "            results = []\n",
    "            title = node.get(\"title\")\n",
    "            if title:\n",
    "                full_title = f\"{parent_title} â†’ {title}\" if parent_title else title\n",
    "                results.append((full_title, title))\n",
    "            for child in node.get(\"children\", []):\n",
    "                results.extend(extract_issues_with_hierarchy(child, title))\n",
    "            return results\n",
    "\n",
    "        all_issues = extract_issues_with_hierarchy(section)\n",
    "        plain_titles = [t[1].lower() for t in all_issues]\n",
    "\n",
    "        # Try to match the user input to a known issue\n",
    "        matched = get_close_matches(user_input.lower(), plain_titles, n=1, cutoff=0.4)\n",
    "\n",
    "        if matched:\n",
    "            matched_title = next(full for full, plain in all_issues if plain.lower() == matched[0])\n",
    "            ksu_code = \"KSU1\" if user_type == \"Staff\" else \"KSU2\"\n",
    "            return (\n",
    "                f\"ğŸ› ï¸ It looks like you're facing: **{matched_title}**.\\n\"\n",
    "                f\"Please visit the [IT Helpdesk]({it_helpdesk['url']}), select **{ksu_code}**, and click **'Report an Issue'**.\"\n",
    "            )\n",
    "\n",
    "        # If no match found, show all options (parents and their children)\n",
    "        options_text = f\"ğŸ“‹ I couldn't find an exact match. Here are support topics for {user_type}:\\n\\n\"\n",
    "        grouped = {}\n",
    "        for full_title, child_title in all_issues:\n",
    "            parent = full_title.split(\"â†’\")[0].strip()\n",
    "            grouped.setdefault(parent, []).append(child_title)\n",
    "\n",
    "        for parent, children in grouped.items():\n",
    "            options_text += f\"ğŸ”¹ **{parent}**\\n\"\n",
    "            for c in children:\n",
    "                options_text += f\"â€ƒâ€ƒâ€¢ {c}\\n\"\n",
    "\n",
    "        options_text += \"\\nğŸ’¬ Please choose one of the topics above or rephrase your issue.\"\n",
    "\n",
    "        return options_text\n",
    "    \n",
    "\n",
    "    #---------------- THESE THINGS HAVE NOT BEEN SCRAPED YET ---------------------------------------#\n",
    "    def handle_fees_tuition(self, user_input):\n",
    "        \"\"\"Handle tuition and fees queries\"\"\"\n",
    "        query = \"tuition fees cost payment financial charges\"\n",
    "        relevant_content = self.find_relevant_content_semantic(query, top_k=5)\n",
    "        \n",
    "        response = \"ğŸ’° **Tuition & Fees Information**\\n\\n\"\n",
    "        \n",
    "        for content in relevant_content[:3]:\n",
    "            response += content['content'] + \"\\n\\n\"\n",
    "            links = self.extract_links_from_content(content['content'])\n",
    "            if links:\n",
    "                response += f\"ğŸ”— [Fee structure]({links[0]})\\n\\n\"\n",
    "        \n",
    "        if not relevant_content:\n",
    "            response += (\"Tuition and fee information is not available in our current database. \"\n",
    "                        \"Please contact the finance office or check the student portal for current rates.\\n\\n\")\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def handle_scholarships(self, user_input):\n",
    "        \"\"\"Handle scholarship and financial aid queries\"\"\"\n",
    "        query = \"scholarship financial aid funding grants assistance\"\n",
    "        relevant_content = self.find_relevant_content_semantic(query, top_k=5)\n",
    "        \n",
    "        response = \"ğŸ“ **Scholarships & Financial Aid**\\n\\n\"\n",
    "        \n",
    "        for content in relevant_content[:3]:\n",
    "            response += content['content'] + \"\\n\\n\"\n",
    "            links = self.extract_links_from_content(content['content'])\n",
    "            if links:\n",
    "                response += f\"ğŸ”— [Scholarship portal]({links[0]})\\n\\n\"\n",
    "        \n",
    "        if not relevant_content:\n",
    "            response += (\"Scholarship information is not available in our current database. \"\n",
    "                        \"Please contact the financial aid office for information about available scholarships and grants.\\n\\n\")\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------#\n",
    "    \n",
    "    # General queries\n",
    "    def handle_general_query_enhanced(self, user_input):\n",
    "        \"\"\"Enhanced general query handling with better semantic understanding\"\"\"\n",
    "        # First, try to find relevant content\n",
    "        relevant_content = self.find_relevant_content_semantic(user_input, top_k=5, threshold=0.2)\n",
    "        \n",
    "        if relevant_content:\n",
    "            response = \"ğŸ’¡ **Here's what I found for you:**\\n\\n\"\n",
    "            \n",
    "            for i, content in enumerate(relevant_content[:3], 1):\n",
    "                text = content['content']\n",
    "                \n",
    "                # Intelligent summarization\n",
    "                if len(text) > 250:\n",
    "                    sentences = text.split('.')\n",
    "                    summary = sentences[0]\n",
    "                    if len(summary) < 200 and len(sentences) > 1:\n",
    "                        summary += '. ' + sentences[1]\n",
    "                    response += f\"**{i}.** {summary}...\\n\\n\"\n",
    "                else:\n",
    "                    response += f\"**{i}.** {text}\\n\\n\"\n",
    "                \n",
    "                # Add links\n",
    "                links = self.extract_links_from_content(content['content'])\n",
    "                if links:\n",
    "                    response += f\"   ğŸ”— [More information]({links[0]})\\n\\n\"\n",
    "            \n",
    "            # Add contextual follow-up suggestions\n",
    "            response += \"â“ **Want to know more?** You can ask me about:\\n\"\n",
    "            response += \"â€¢ Admission requirements and application process\\n\"\n",
    "            response += \"â€¢ Academic programs and course information\\n\"\n",
    "            response += \"â€¢ Campus facilities and student services\\n\"\n",
    "            response += \"â€¢ Contact information for departments\\n\"\n",
    "            \n",
    "        else:\n",
    "            response = (\"ğŸ¤” I couldn't find specific information about that in our database. \"\n",
    "                       \"However, I can help you with:\\n\\n\"\n",
    "                       \"ğŸ“ **Academics:** Admission requirements, degree programs, academic calendar\\n\"\n",
    "                       \"ğŸ  **Campus Life:** Housing, libraries, dining, recreation\\n\"\n",
    "                       \"ğŸ’¼ **Services:** IT support, financial aid, career services\\n\"\n",
    "                       \"ğŸ“ **Contact:** Department information, faculty directories\\n\"\n",
    "                       \"ğŸ“‹ **Policies:** Grading, attendance, academic integrity\\n\"\n",
    "                       \"ğŸ”¬ **Research:** Labs, facilities, opportunities\\n\\n\"\n",
    "                       \"Could you try rephrasing your question or ask about one of these topics?\")\n",
    "        \n",
    "        return response\n",
    "    \n",
    "\n",
    "    # This is all the chats handler functions inside\n",
    "    def chat(self, user_input):\n",
    "        \"\"\"Enhanced main chat function with better semantic understanding\"\"\"\n",
    "        # Identify intent using semantic similarity\n",
    "        intent, confidence = self.identify_intent_semantic(user_input)\n",
    "        entities = self.extract_entities_enhanced(user_input)\n",
    "        \n",
    "        # Update user context\n",
    "        self.user_context['last_intent'] = intent\n",
    "        self.user_context['entities'].update(entities)\n",
    "        \n",
    "        # Route to appropriate handler based on intent\n",
    "        if intent == 'admission_requirements':\n",
    "            return self.handle_admission_requirements(entities, user_input)\n",
    "        elif intent == 'academic_calendar':\n",
    "            return self.handle_academic_calendar()\n",
    "        elif intent == 'degree_programs':\n",
    "            return self.handle_degree_programs(entities, user_input)\n",
    "        elif intent == 'faculty':\n",
    "            return self.handle_faculty_directory(entities, user_input)\n",
    "        elif intent == 'housing':\n",
    "            return self.handle_housing(entities, user_input)\n",
    "        elif intent == 'library':\n",
    "            return self.handle_library(entities, user_input)\n",
    "        elif intent == 'grading':\n",
    "            return self.handle_grading_system()\n",
    "        elif intent == 'plagiarism':\n",
    "            return self.handle_plagiarism()\n",
    "        elif intent == 'attendance':\n",
    "            return self.handle_attendance()\n",
    "        elif intent == 'research':\n",
    "            return self.handle_research_labs()\n",
    "        elif intent == 'it_support':\n",
    "            return self.handle_it_support(entities, user_input)\n",
    "        elif intent == 'contact_info':\n",
    "            return self.handle_contact_info(entities, user_input)\n",
    "        elif intent == 'fees_tuition':\n",
    "            return self.handle_fees_tuition(user_input)\n",
    "        elif intent == 'scholarships':\n",
    "            return self.handle_scholarships(user_input)\n",
    "        else:\n",
    "            return self.handle_general_query_enhanced(user_input)\n",
    "    \n",
    "    # Interactive chats\n",
    "    def run_interactive_chat(self):\n",
    "        \"\"\"Run interactive chat session with enhanced experience\"\"\"\n",
    "        print(\"ğŸ“ Welcome! I can help with admissions, academics, libraries, housing, faculty, fees, research, and more. Type your question or 'help' for examples.\")\n",
    "        print(\"ğŸ’¡ Tip: Ask natural questions like 'How do I apply for undergraduate admission?' or 'What are the library hours?'\")\n",
    "        print(\"Type 'help' for examples, or 'exit' to quit.\\n\")\n",
    "        \n",
    "        conversation_count = 0\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"ğŸ™‹ You: \").strip()\n",
    "                \n",
    "                if user_input.lower() in ['exit', 'quit', 'bye', 'goodbye']:\n",
    "                    print(\"ğŸ¤– Assistant: Thank you for using the University AI Assistant! Have a wonderful day! ğŸ‘‹\")\n",
    "                    break\n",
    "                \n",
    "                if user_input.lower() == 'help':\n",
    "                    print(\"ğŸ¤– Assistant: Here are some example questions you can ask:\")\n",
    "                    print(\"â€¢ 'What are the admission requirements for undergraduate programs?'\")\n",
    "                    print(\"â€¢ 'Show me the academic calendar'\")\n",
    "                    print(\"â€¢ 'I need information about computer science department'\")\n",
    "                    print(\"â€¢ 'Where can I find student housing?'\")\n",
    "                    print(\"â€¢ 'What's the grading system?'\")\n",
    "                    print(\"â€¢ 'I'm having trouble with my login'\")\n",
    "                    print(\"â€¢ 'Tell me about research opportunities'\")\n",
    "                    print(\"â€¢ 'How much does tuition cost?'\")\n",
    "                    print(\"â€¢ 'What scholarships are available?'\\n\")\n",
    "                    continue\n",
    "                \n",
    "                if not user_input:\n",
    "                    print(\"ğŸ¤– Assistant: I'm here to help! Please ask me something about the university. ğŸ˜Š\\n\")\n",
    "                    continue\n",
    "                \n",
    "                print(\"ğŸ¤– Assistant: \", end=\"\")\n",
    "                response = self.chat(user_input)\n",
    "                print(f\"{response}\\n\")\n",
    "                \n",
    "                conversation_count += 1\n",
    "                \n",
    "                # Provide helpful suggestions every few interactions\n",
    "                if conversation_count % 5 == 0:\n",
    "                    print(\"ğŸ’¡ **Quick tip:** You can ask follow-up questions or request more specific information anytime!\\n\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nğŸ¤– Assistant: Goodbye! Thanks for using the University AI Assistant! ğŸ‘‹\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"ğŸ¤– Assistant: I encountered an error processing your request. Please try again! ğŸ”§\")\n",
    "                print(f\"(Technical details: {str(e)})\\n\")\n",
    "\n",
    "\n",
    "# Main function to run all the functions\n",
    "def main():\n",
    "    \"\"\"Main function to run the enhanced chatbot\"\"\"\n",
    "    # Initialize chatbot with your JSON file\n",
    "    json_file_path = \"C:\\\\Nawal\\\\IT Department\\\\Practical Training\\\\Final Chatbot\\\\data_backups\\\\menu_hierarchy.json\"  # Replace with your actual JSON file path\n",
    "    \n",
    "    print(\"ğŸš€ Starting University AI Assistant...\")\n",
    "    \n",
    "    try:\n",
    "        chatbot = UniversityChatbot(json_file_path)\n",
    "        chatbot.run_interactive_chat()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nğŸ‘‹ Goodbye!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error initializing chatbot: {str(e)}\")\n",
    "        print(\"\\nğŸ”§ **Setup Requirements:**\")\n",
    "        print(\"1. Install required packages:\")\n",
    "        print(\"   pip install sentence-transformers transformers torch nltk spacy scikit-learn\")\n",
    "        print(\"2. Download spaCy model:\")\n",
    "        print(\"   python -m spacy download en_core_web_sm\")\n",
    "        print(\"3. Ensure your JSON file path is correct\")\n",
    "        print(\"4. Make sure you have sufficient disk space for model downloads\")\n",
    "        print(\"\\nğŸ’¡ **Note:** The chatbot will work with fallback methods if some models fail to load.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f109b20",
   "metadata": {},
   "source": [
    "### Enhancement's needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8686fa8",
   "metadata": {},
   "source": [
    "- Need to add a bubble chat functionality to enhance performance and transparency to users\n",
    "- Adding AI Agents to scrape PDFs and several other items\n",
    "- Scraping from KSU account (from X) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db6532",
   "metadata": {},
   "source": [
    "### Fixing needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ba464",
   "metadata": {},
   "source": [
    "- Some degree programs need extra functions as they are being mistakened as intent of others such as Finance Departments not taken as Colleges\n",
    "- IT Helpdesk has some issues as well\n",
    "- I need to add Campus locations to webscrape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
